[
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## Export and retrieve a job run\n\n`\nGET/api/2.1/jobs/runs/export`\n\nExport and retrieve the job run task.\n\n### Query parameters\n\n[`run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun#run_id) requiredint64\n\nExample`run_id=455644833`\n\nThe canonical identifier for the run. This field is required.\n\n[`views_to_export`](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun#views_to_export) string\n\nEnum: `CODE | DASHBOARDS | ALL`\n\nDefault`\"CODE\"`\n\nWhich views to export (CODE, DASHBOARDS, or ALL). Defaults to CODE.\n\n### Responses\n\n**200** Request completed successfully.\n\nRequest completed successfully.\n\n[`views`](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun#views) Array of object\n\nThe exported content in HTML format (one for every view item). To extract the HTML notebook from the JSON response, download and run this [Python script](https://docs.databricks.com/en/_static/examples/extract.py).\n\nArray \\[\\\n\\\n[`content`](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun#views-content) string\\\n\\\nContent of the view.\\\n\\\n[`name`](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun#views-name) string\\\n\\\nName of the view item. In the case of code view, it would be the notebook’s name. In the case of dashboard view, it would be the dashboard’s name.\\\n\\\n[`type`](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun#views-type) string\\\n\\\nEnum: `NOTEBOOK | DASHBOARD`\\\n\\\nType of the view item.\\\n\\\n\\]\n\nThis method might return the following HTTP codes: 400, 401, 403, 429, 500\n\nError responses are returned in the following format:\n\n```\n{\n  \"error_code\": \"Error code\",\n  \"message\": \"Human-readable error message.\"\n}\n```\n\n# Possible error codes:\n\nHTTP code\n\nerror\\_code\n\nDescription\n\n400\n\nINVALID\\_PARAMETER\\_VALUE\n\nSupplied value for a parameter was invalid.\n\n401\n\nUNAUTHORIZED\n\nThe request does not have valid authentication credentials for the operation.\n\n403\n\nPERMISSION\\_DENIED\n\nCaller does not have permission to execute the specified operation.\n\n429\n\nREQUEST\\_LIMIT\\_EXCEEDED\n\nRequest is rejected due to throttling.\n\n500\n\nINTERNAL\\_SERVER\\_ERROR\n\nInternal error.\n\n# Response samples\n\n200\n\n```\n{\n  \"views\": [\\\n    {\\\n      \"content\": \"string\",\\\n      \"name\": \"string\",\\\n      \"type\": \"NOTEBOOK\"\\\n    }\\\n  ]\n}\n```"
  },
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## Create a new job\n\n`\nPOST/api/2.1/jobs/create`\n\nCreate a new job.\n\n### Request body\n\n[`name`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#name) string\n\n<= 4096 characters\n\nDefault`\"Untitled\"`\n\nExample`\"A multitask job\"`\n\nAn optional name for the job. The maximum length is 4096 bytes in UTF-8 encoding.\n\n[`description`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#description) string\n\n<= 27700 characters\n\nExample`\"This job contain multiple tasks that are required to produce the weekly shark sightings report.\"`\n\nAn optional description for the job. The maximum length is 27700 characters in UTF-8 encoding.\n\n[`email_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#email_notifications) object\n\nDefault`{}`\n\nAn optional set of email addresses that is notified when runs of this job begin or complete as well as when this job is deleted.\n\n[`on_start`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#email_notifications-on_start) Array of string\n\nA list of email addresses to be notified when a run begins. If not specified on job creation, reset, or update, the list is empty, and notifications are not sent.\n\n[`on_success`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#email_notifications-on_success) Array of string\n\nA list of email addresses to be notified when a run successfully completes. A run is considered to have completed successfully if it ends with a `TERMINATED``life_cycle_state` and a `SUCCESS` result\\_state. If not specified on job creation, reset, or update, the list is empty, and notifications are not sent.\n\n[`on_failure`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#email_notifications-on_failure) Array of string\n\nA list of email addresses to be notified when a run unsuccessfully completes. A run is considered to have completed unsuccessfully if it ends with an `INTERNAL_ERROR``life_cycle_state` or a `FAILED`, or `TIMED_OUT` result\\_state. If this is not specified on job creation, reset, or update the list is empty, and notifications are not sent.\n\n[`on_duration_warning_threshold_exceeded`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#email_notifications-on_duration_warning_threshold_exceeded) Array of string\n\nA list of email addresses to be notified when the duration of a run exceeds the threshold specified for the `RUN_DURATION_SECONDS` metric in the `health` field. If no rule for the `RUN_DURATION_SECONDS` metric is specified in the `health` field for the job, notifications are not sent.\n\n[`on_streaming_backlog_exceeded`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#email_notifications-on_streaming_backlog_exceeded) Array of string\n\nPublic preview\n\nA list of email addresses to notify when any streaming backlog thresholds are exceeded for any stream.\nStreaming backlog thresholds can be set in the `health` field using the following metrics: `STREAMING_BACKLOG_BYTES`, `STREAMING_BACKLOG_RECORDS`, `STREAMING_BACKLOG_SECONDS`, or `STREAMING_BACKLOG_FILES`.\nAlerting is based on the 10-minute average of these metrics. If the issue persists, notifications are resent every 30 minutes.\n\n[`no_alert_for_skipped_runs`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#email_notifications-no_alert_for_skipped_runs) boolean\n\nDeprecated\n\nDefault`false`\n\nIf true, do not send email to recipients specified in `on_failure` if the run is skipped.\nThis field is `deprecated`. Please use the `notification_settings.no_alert_for_skipped_runs` field.\n\n[`webhook_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#webhook_notifications) object\n\nDefault`{}`\n\nA collection of system notification IDs to notify when runs of this job begin or complete.\n\n[`on_start`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#webhook_notifications-on_start) Array of object\n\nAn optional list of system notification IDs to call when the run starts. A maximum of 3 destinations can be specified for the `on_start` property.\n\n[`on_success`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#webhook_notifications-on_success) Array of object\n\nAn optional list of system notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified for the `on_success` property.\n\n[`on_failure`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#webhook_notifications-on_failure) Array of object\n\nAn optional list of system notification IDs to call when the run fails. A maximum of 3 destinations can be specified for the `on_failure` property.\n\n[`on_duration_warning_threshold_exceeded`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#webhook_notifications-on_duration_warning_threshold_exceeded) Array of object\n\nAn optional list of system notification IDs to call when the duration of a run exceeds the threshold specified for the `RUN_DURATION_SECONDS` metric in the `health` field. A maximum of 3 destinations can be specified for the `on_duration_warning_threshold_exceeded` property.\n\n[`on_streaming_backlog_exceeded`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#webhook_notifications-on_streaming_backlog_exceeded) Array of object\n\nPublic preview\n\nAn optional list of system notification IDs to call when any streaming backlog thresholds are exceeded for any stream.\nStreaming backlog thresholds can be set in the `health` field using the following metrics: `STREAMING_BACKLOG_BYTES`, `STREAMING_BACKLOG_RECORDS`, `STREAMING_BACKLOG_SECONDS`, or `STREAMING_BACKLOG_FILES`.\nAlerting is based on the 10-minute average of these metrics. If the issue persists, notifications are resent every 30 minutes.\nA maximum of 3 destinations can be specified for the `on_streaming_backlog_exceeded` property.\n\n[`notification_settings`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#notification_settings) object\n\nDefault`{}`\n\nOptional notification settings that are used when sending notifications to each of the `email_notifications` and `webhook_notifications` for this job.\n\n[`no_alert_for_skipped_runs`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#notification_settings-no_alert_for_skipped_runs) boolean\n\nDefault`false`\n\nIf true, do not send notifications to recipients specified in `on_failure` if the run is skipped.\n\n[`no_alert_for_canceled_runs`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#notification_settings-no_alert_for_canceled_runs) boolean\n\nDefault`false`\n\nIf true, do not send notifications to recipients specified in `on_failure` if the run is canceled.\n\n[`alert_on_last_attempt`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#notification_settings-alert_on_last_attempt) boolean\n\nDefault`false`\n\nIf true, do not send notifications to recipients specified in `on_start` for the retried runs and do not send notifications to recipients specified in `on_failure` until the last retry of the run.\n\n[`timeout_seconds`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#timeout_seconds) int32\n\nDefault`0`\n\nExample`86400`\n\nAn optional timeout applied to each run of this job. A value of `0` means no timeout.\n\n[`health`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#health) object\n\nAn optional set of health rules that can be defined for this job.\n\n[`rules`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#health-rules) Array of object\n\n[`max_retries`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#max_retries) int32\n\nDefault`0`\n\nExample`10`\n\nAn optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with the `FAILED` result\\_state or `INTERNAL_ERROR``life_cycle_state`. The value `-1` means to retry indefinitely and the value `0` means to never retry.\n\n[`min_retry_interval_millis`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#min_retry_interval_millis) int32\n\nExample`2000`\n\nAn optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n\n[`retry_on_timeout`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#retry_on_timeout) boolean\n\nDefault`false`\n\nExample`true`\n\nAn optional policy to specify whether to retry a job when it times out. The default behavior\nis to not retry on timeout.\n\n[`disable_auto_optimization`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#disable_auto_optimization) boolean\n\nDefault`false`\n\nExample`true`\n\nAn option to disable auto optimization in serverless\n\n[`schedule`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#schedule) object\n\nAn optional periodic schedule for this job. The default behavior is that the job only runs when triggered by clicking “Run Now” in the Jobs UI or sending an API request to `runNow`.\n\n[`quartz_cron_expression`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#schedule-quartz_cron_expression) requiredstring\n\nExample`\"20 30 * * * ?\"`\n\nA Cron expression using Quartz syntax that describes the schedule for a job. See [Cron Trigger](http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html) for details. This field is required.\n\n[`timezone_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#schedule-timezone_id) requiredstring\n\nExample`\"Europe/London\"`\n\nA Java timezone ID. The schedule for a job is resolved with respect to this timezone. See [Java TimeZone](https://docs.oracle.com/javase/7/docs/api/java/util/TimeZone.html) for details. This field is required.\n\n[`pause_status`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#schedule-pause_status) string\n\nEnum: `UNPAUSED | PAUSED`\n\nDefault`\"UNPAUSED\"`\n\nIndicate whether this schedule is paused or not.\n\n[`trigger`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#trigger) object\n\nA configuration to trigger a run when certain conditions are met. The default behavior is that the job runs only when triggered by clicking “Run Now” in the Jobs UI or sending an API request to `runNow`.\n\n[`pause_status`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#trigger-pause_status) string\n\nEnum: `UNPAUSED | PAUSED`\n\nDefault`\"UNPAUSED\"`\n\nWhether this trigger is paused or not.\n\n[`file_arrival`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#trigger-file_arrival) object\n\nFile arrival trigger settings.\n\n[`periodic`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#trigger-periodic) object\n\nPeriodic trigger settings.\n\n[`continuous`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#continuous) object\n\nAn optional continuous property for this job. The continuous property will ensure that there is always one run executing. Only one of `schedule` and `continuous` can be used.\n\n[`pause_status`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#continuous-pause_status) string\n\nEnum: `UNPAUSED | PAUSED`\n\nDefault`\"UNPAUSED\"`\n\nIndicate whether the continuous execution of the job is paused or not. Defaults to UNPAUSED.\n\n[`max_concurrent_runs`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#max_concurrent_runs) int32\n\nDefault`1`\n\nExample`10`\n\nAn optional maximum allowed number of concurrent runs of the job.\nSet this value if you want to be able to execute multiple runs of the same job concurrently.\nThis is useful for example if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or if you want to trigger multiple runs which differ by their input parameters.\nThis setting affects only new runs. For example, suppose the job’s concurrency is 4 and there are 4 concurrent active runs. Then setting the concurrency to 3 won’t kill any of the active runs.\nHowever, from then on, new runs are skipped unless there are fewer than 3 active runs.\nThis value cannot exceed 1000. Setting this value to `0` causes all new runs to be skipped.\n\n[`tasks`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks) Array of object\n\n<= 100 items\n\nExample\n\nA list of task specifications to be executed by this job.\nIf more than 100 tasks are available, you can paginate through them using [jobs/get](https://docs.databricks.com/api/azure/workspace/jobs/get). Use the `next_page_token` field at the object root to determine if more results are available.\n\nArray \\[\\\n\\\n[`task_key`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-task_key) requiredstring\\\n\\\n\\[ 1 .. 100 \\] characters ^\\[\\\\w\\\\-\\\\\\_\\]+$\\\n\\\nExample`\"Task_Key\"`\\\n\\\nA unique name for the task. This field is used to refer to this task from other tasks.\\\nThis field is required and must be unique within its parent job.\\\nOn Update or Reset, this field is used to reference the tasks to be updated or reset.\\\n\\\n[`depends_on`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-depends_on) Array of object\\\n\\\nExample\\\n\\\nAn optional array of objects specifying the dependency graph of the task. All tasks specified in this field must complete before executing this task. The task will run only if the `run_if` condition is true.\\\nThe key is `task_key`, and the value is the name assigned to the dependent task.\\\n\\\n[`run_if`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-run_if) string\\\n\\\nEnum: `ALL_SUCCESS | ALL_DONE | NONE_FAILED | AT_LEAST_ONE_SUCCESS | ALL_FAILED | AT_LEAST_ONE_FAILED`\\\n\\\nDefault`\"ALL_SUCCESS\"`\\\n\\\nAn optional value specifying the condition determining whether the task is run once its dependencies have been completed.\\\n\\\n- `ALL_SUCCESS`: All dependencies have executed and succeeded\\\n- `AT_LEAST_ONE_SUCCESS`: At least one dependency has succeeded\\\n- `NONE_FAILED`: None of the dependencies have failed and at least one was executed\\\n- `ALL_DONE`: All dependencies have been completed\\\n- `AT_LEAST_ONE_FAILED`: At least one dependency failed\\\n- `ALL_FAILED`: ALl dependencies have failed\\\n\\\n[`notebook_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-notebook_task) object\\\n\\\nThe task runs a notebook when the `notebook_task` field is present.\\\n\\\n[`spark_jar_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-spark_jar_task) object\\\n\\\nThe task runs a JAR when the `spark_jar_task` field is present.\\\n\\\n[`spark_python_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-spark_python_task) object\\\n\\\nThe task runs a Python file when the `spark_python_task` field is present.\\\n\\\n[`spark_submit_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-spark_submit_task) object\\\n\\\n(Legacy) The task runs the spark-submit script when the `spark_submit_task` field is present. This task can run only on new clusters and is not compatible with serverless compute.\\\n\\\nIn the `new_cluster` specification, `libraries` and `spark_conf` are not supported. Instead, use `--jars` and `--py-files` to add Java and Python libraries and `--conf` to set the Spark configurations.\\\n\\\n`master`, `deploy-mode`, and `executor-cores` are automatically configured by Azure Databricks; you _cannot_ specify them in parameters.\\\n\\\nBy default, the Spark submit job uses all available memory (excluding reserved memory for Azure Databricks services). You can set `--driver-memory`, and `--executor-memory` to a smaller value to leave some room for off-heap usage.\\\n\\\nThe `--jars`, `--py-files`, `--files` arguments support DBFS and S3 paths.\\\n\\\n[`pipeline_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-pipeline_task) object\\\n\\\nThe task triggers a pipeline update when the `pipeline_task` field is present. Only pipelines configured to use triggered more are supported.\\\n\\\n[`python_wheel_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-python_wheel_task) object\\\n\\\nThe task runs a Python wheel when the `python_wheel_task` field is present.\\\n\\\n[`dbt_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-dbt_task) object\\\n\\\nThe task runs one or more dbt commands when the `dbt_task` field is present. The dbt task requires both Databricks SQL and the ability to use a serverless or a pro SQL warehouse.\\\n\\\n[`sql_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-sql_task) object\\\n\\\nThe task runs a SQL query or file, or it refreshes a SQL alert or a legacy SQL dashboard when the `sql_task` field is present.\\\n\\\n[`run_job_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-run_job_task) object\\\n\\\nThe task triggers another job when the `run_job_task` field is present.\\\n\\\n[`condition_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-condition_task) object\\\n\\\nThe task evaluates a condition that can be used to control the execution of other tasks when the `condition_task` field is present.\\\nThe condition task does not require a cluster to execute and does not support retries or notifications.\\\n\\\n[`for_each_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-for_each_task) object\\\n\\\nThe task executes a nested task for every input provided when the `for_each_task` field is present.\\\n\\\n[`clean_rooms_notebook_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-clean_rooms_notebook_task) object\\\n\\\nPublic preview\\\n\\\nThe task runs a [clean rooms](https://docs.databricks.com/en/clean-rooms/index.html) notebook\\\nwhen the `clean_rooms_notebook_task` field is present.\\\n\\\n[`existing_cluster_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-existing_cluster_id) string\\\n\\\nExample`\"0923-164208-meows279\"`\\\n\\\nIf existing\\_cluster\\_id, the ID of an existing cluster that is used for all runs.\\\nWhen running jobs or tasks on an existing cluster, you may need to manually restart\\\nthe cluster if it stops responding. We suggest running jobs and tasks on new clusters for\\\ngreater reliability\\\n\\\n[`new_cluster`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-new_cluster) object\\\n\\\nIf new\\_cluster, a description of a new cluster that is created for each run.\\\n\\\n[`job_cluster_key`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-job_cluster_key) string\\\n\\\n\\[ 1 .. 100 \\] characters ^\\[\\\\w\\\\-\\\\\\_\\]+$\\\n\\\nIf job\\_cluster\\_key, this task is executed reusing the cluster specified in `job.settings.job_clusters`.\\\n\\\n[`libraries`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-libraries) Array of object\\\n\\\nAn optional list of libraries to be installed on the cluster.\\\nThe default value is an empty list.\\\n\\\n[`max_retries`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-max_retries) int32\\\n\\\nDefault`0`\\\n\\\nExample`10`\\\n\\\nAn optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with the `FAILED` result\\_state or `INTERNAL_ERROR``life_cycle_state`. The value `-1` means to retry indefinitely and the value `0` means to never retry.\\\n\\\n[`min_retry_interval_millis`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-min_retry_interval_millis) int32\\\n\\\nExample`2000`\\\n\\\nAn optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\\\n\\\n[`retry_on_timeout`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-retry_on_timeout) boolean\\\n\\\nDefault`false`\\\n\\\nExample`true`\\\n\\\nAn optional policy to specify whether to retry a job when it times out. The default behavior\\\nis to not retry on timeout.\\\n\\\n[`disable_auto_optimization`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-disable_auto_optimization) boolean\\\n\\\nDefault`false`\\\n\\\nExample`true`\\\n\\\nAn option to disable auto optimization in serverless\\\n\\\n[`timeout_seconds`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-timeout_seconds) int32\\\n\\\nDefault`0`\\\n\\\nExample`86400`\\\n\\\nAn optional timeout applied to each run of this job task. A value of `0` means no timeout.\\\n\\\n[`health`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-health) object\\\n\\\nAn optional set of health rules that can be defined for this job.\\\n\\\n[`email_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-email_notifications) object\\\n\\\nDefault`{}`\\\n\\\nAn optional set of email addresses that is notified when runs of this task begin or complete as well as when this task is deleted. The default behavior is to not send any emails.\\\n\\\n[`notification_settings`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-notification_settings) object\\\n\\\nDefault`{}`\\\n\\\nOptional notification settings that are used when sending notifications to each of the `email_notifications` and `webhook_notifications` for this task.\\\n\\\n[`webhook_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-webhook_notifications) object\\\n\\\nDefault`{}`\\\n\\\nA collection of system notification IDs to notify when runs of this task begin or complete. The default behavior is to not send any system notifications.\\\n\\\n[`description`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-description) string\\\n\\\n<= 1000 characters\\\n\\\nExample`\"This is the description for this task.\"`\\\n\\\nAn optional description for this task.\\\n\\\n[`environment_key`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tasks-environment_key) string\\\n\\\n\\[ 1 .. 100 \\] characters ^\\[\\\\w\\\\-\\\\\\_\\]+$\\\n\\\nThe key that references an environment spec in a job. This field is required for Python script, Python wheel and dbt tasks when using serverless compute.\\\n\\\n\\]\n\n[`job_clusters`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#job_clusters) Array of object\n\n<= 100 items\n\nExample\n\nA list of job cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings.\nIf more than 100 job clusters are available, you can paginate through them using [jobs/get](https://docs.databricks.com/api/azure/workspace/jobs/get).\n\nArray \\[\\\n\\\n[`job_cluster_key`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#job_clusters-job_cluster_key) requiredstring\\\n\\\n\\[ 1 .. 100 \\] characters ^\\[\\\\w\\\\-\\\\\\_\\]+$\\\n\\\nExample`\"auto_scaling_cluster\"`\\\n\\\nA unique name for the job cluster. This field is required and must be unique within the job.\\\n`JobTaskSettings` may refer to this field to determine which cluster to launch for the task execution.\\\n\\\n[`new_cluster`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#job_clusters-new_cluster) object\\\n\\\nIf new\\_cluster, a description of a cluster that is created for each task.\\\n\\\n\\]\n\n[`git_source`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#git_source) object\n\nExample\n\nAn optional specification for a remote Git repository containing the source code used by tasks. Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.\n\nIf `git_source` is set, these tasks retrieve the file from the remote repository by default. However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.\n\nNote: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are used, `git_source` must be defined on the job.\n\n[`git_url`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#git_source-git_url) requiredstring\n\n<= 300 characters\n\nExample`\"https://github.com/databricks/databricks-cli\"`\n\nURL of the repository to be cloned by this job.\n\n[`git_provider`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#git_source-git_provider) requiredstring\n\nEnum: `gitHub | bitbucketCloud | azureDevOpsServices | gitHubEnterprise | bitbucketServer | gitLab | gitLabEnterpriseEdition | awsCodeCommit`\n\nUnique identifier of the service used to host the Git repository. The value is case insensitive.\n\n[`git_branch`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#git_source-git_branch) string\n\n<= 255 characters\n\nExample`\"main\"`\n\nName of the branch to be checked out and used by this job. This field cannot be specified in conjunction with git\\_tag or git\\_commit.\n\n[`git_tag`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#git_source-git_tag) string\n\n<= 255 characters\n\nExample`\"release-1.0.0\"`\n\nName of the tag to be checked out and used by this job. This field cannot be specified in conjunction with git\\_branch or git\\_commit.\n\n[`git_commit`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#git_source-git_commit) string\n\n<= 64 characters\n\nExample`\"e0056d01\"`\n\nCommit to be checked out and used by this job. This field cannot be specified in conjunction with git\\_branch or git\\_tag.\n\n[`git_snapshot`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#git_source-git_snapshot) object\n\nRead-only state of the remote repository at the time the job was run. This field is only included on job runs.\n\n[`tags`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#tags) object\n\nDefault`{}`\n\nExample\n\nA map of tags associated with the job. These are forwarded to the cluster as cluster tags for jobs clusters, and are subject to the same limitations as cluster tags. A maximum of 25 tags can be added to the job.\n\n[`format`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#format) string\n\nDeprecated\n\nEnum: `SINGLE_TASK | MULTI_TASK`\n\nExample`\"MULTI_TASK\"`\n\nUsed to tell what is the format of the job. This field is ignored in Create/Update/Reset calls. When using the Jobs API 2.1 this value is always set to `\"MULTI_TASK\"`.\n\n[`queue`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#queue) object\n\nThe queue settings of the job.\n\n[`enabled`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#queue-enabled) requiredboolean\n\nExample`true`\n\nIf true, enable queueing for the job. This is a required field.\n\n[`parameters`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#parameters) Array of object\n\nJob-level parameter definitions\n\nArray \\[\\\n\\\n[`name`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#parameters-name) requiredstring\\\n\\\n^\\[\\\\w\\\\-.\\]+$\\\n\\\nExample`\"table\"`\\\n\\\nThe name of the defined parameter. May only contain alphanumeric characters, `_`, `-`, and `.`\\\n\\\n[`default`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#parameters-default) requiredstring\\\n\\\nExample`\"users\"`\\\n\\\nDefault value of the parameter.\\\n\\\n\\]\n\n[`run_as`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#run_as) object\n\nWrite-only setting. Specifies the user or service principal that the job runs as. If not specified, the job runs as the user who created the job.\n\nEither `user_name` or `service_principal_name` should be specified. If not, an error is thrown.\n\n[`user_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#run_as-user_name) string\n\nExample`\"user@databricks.com\"`\n\nThe email of an active workspace user. Non-admin users can only set this field to their own email.\n\n[`service_principal_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#run_as-service_principal_name) string\n\nExample`\"692bc6d0-ffa3-11ed-be56-0242ac120002\"`\n\nApplication ID of an active service principal. Setting this field requires the `servicePrincipal/user` role.\n\n[`edit_mode`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#edit_mode) string\n\nEnum: `UI_LOCKED | EDITABLE`\n\nEdit mode of the job.\n\n- `UI_LOCKED`: The job is in a locked UI state and cannot be modified.\n- `EDITABLE`: The job is in an editable state and can be modified.\n\n[`deployment`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#deployment) object\n\nDeployment information for jobs managed by external sources.\n\n[`kind`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#deployment-kind) requiredstring\n\nEnum: `BUNDLE`\n\nThe kind of deployment that manages the job.\n\n- `BUNDLE`: The job is managed by Databricks Asset Bundle.\n\n[`metadata_file_path`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#deployment-metadata_file_path) string\n\nPath of the file that contains deployment metadata.\n\n[`environments`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#environments) Array of object\n\n<= 10 items\n\nA list of task execution environment specifications that can be referenced by serverless tasks of this job.\nAn environment is required to be present for serverless tasks.\nFor serverless notebook tasks, the environment is accessible in the notebook environment panel.\nFor other serverless tasks, the task environment is required to be specified using environment\\_key in the task settings.\n\nArray \\[\\\n\\\n[`environment_key`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#environments-environment_key) requiredstring\\\n\\\nThe key of an environment. It has to be unique within a job.\\\n\\\n[`spec`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#environments-spec) object\\\n\\\nThe environment entity used to preserve serverless environment side panel and jobs' environment for non-notebook task.\\\nIn this minimal environment spec, only pip dependencies are supported.\\\n\\\n\\]\n\n[`access_control_list`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#access_control_list) Array of object\n\nList of permissions to set on the job.\n\nArray \\[\\\n\\\n[`user_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#access_control_list-user_name) string\\\n\\\nname of the user\\\n\\\n[`group_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#access_control_list-group_name) string\\\n\\\nname of the group\\\n\\\n[`service_principal_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#access_control_list-service_principal_name) string\\\n\\\napplication ID of a service principal\\\n\\\n[`permission_level`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#access_control_list-permission_level) string\\\n\\\nEnum: `CAN_MANAGE | IS_OWNER | CAN_MANAGE_RUN | CAN_VIEW`\\\n\\\nPermission level\\\n\\\n\\]\n\n### Responses\n\n**200** Request completed successfully.\n\nRequest completed successfully.\n\n[`job_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/create#job_id) int64\n\nExample`11223344`\n\nThe canonical identifier for the newly created job.\n\nThis method might return the following HTTP codes: 400, 401, 403, 404, 429, 500\n\nError responses are returned in the following format:\n\n```\n{\n  \"error_code\": \"Error code\",\n  \"message\": \"Human-readable error message.\"\n}\n```\n\n# Possible error codes:\n\nHTTP code\n\nerror\\_code\n\nDescription\n\n400\n\nINVALID\\_PARAMETER\\_VALUE\n\nSupplied value for a parameter was invalid.\n\n401\n\nUNAUTHORIZED\n\nThe request does not have valid authentication credentials for the operation.\n\n403\n\nPERMISSION\\_DENIED\n\nCaller does not have permission to execute the specified operation.\n\n404\n\nFEATURE\\_DISABLED\n\nIf a given user/entity is trying to use a feature which has been disabled.\n\n429\n\nREQUEST\\_LIMIT\\_EXCEEDED\n\nRequest is rejected due to throttling.\n\n500\n\nINTERNAL\\_SERVER\\_ERROR\n\nInternal error.\n\n# Request samples\n\nJSON\n\nShark predictor workflows\n\n```\n{\n  \"email_notifications\": {\n    \"on_failure\": [\\\n      \"sharky@databricks.com\"\\\n    ]\n  },\n  \"format\": \"MULTI_TASK\",\n  \"max_concurrent_runs\": 1,\n  \"name\": \"Shark Predictor\",\n  \"notification_settings\": {\n    \"alert_on_last_attempt\": false,\n    \"no_alert_for_canceled_runs\": false,\n    \"no_alert_for_skipped_runs\": false\n  },\n  \"run_as\": {\n    \"user_name\": \"sharky@databricks.com\"\n  },\n  \"tasks\": [\\\n    {\\\n      \"existing_cluster_id\": \"0914-084715-44dhyjfb\",\\\n      \"notebook_task\": {\\\n        \"notebook_path\": \"/Users/sharky@databricks.com/weather_ingest\",\\\n        \"source\": \"WORKSPACE\"\\\n      },\\\n      \"run_if\": \"ALL_SUCCESS\",\\\n      \"task_key\": \"weather_ocean_data\"\\\n    },\\\n    {\\\n      \"existing_cluster_id\": \"0914-084715-44dhyjfb\",\\\n      \"notebook_task\": {\\\n        \"notebook_path\": \"/Users/sharky@databricks.com/shark_sightings_scraper\",\\\n        \"source\": \"WORKSPACE\"\\\n      },\\\n      \"run_if\": \"ALL_SUCCESS\",\\\n      \"task_key\": \"shark_sightings\"\\\n    },\\\n    {\\\n      \"existing_cluster_id\": \"0914-084715-44dhyjfb\",\\\n      \"notebook_task\": {\\\n        \"notebook_path\": \"/Users/sharky@databricks.com/reef_data\",\\\n        \"source\": \"WORKSPACE\"\\\n      },\\\n      \"run_if\": \"ALL_SUCCESS\",\\\n      \"task_key\": \"reef_data\"\\\n    },\\\n    {\\\n      \"depends_on\": [\\\n        {\\\n          \"task_key\": \"reef_data\"\\\n        },\\\n        {\\\n          \"task_key\": \"shark_sightings\"\\\n        },\\\n        {\\\n          \"task_key\": \"weather_ocean_data\"\\\n        }\\\n      ],\\\n      \"pipeline_task\": {\\\n        \"pipeline_id\": \"1165597e-f650-4bf3-9a4f-fc2f2d40d2c3\"\\\n      },\\\n      \"run_if\": \"AT_LEAST_ONE_SUCCESS\",\\\n      \"task_key\": \"combine_shark_data\"\\\n    },\\\n    {\\\n      \"depends_on\": [\\\n        {\\\n          \"task_key\": \"combine_shark_data\"\\\n        }\\\n      ],\\\n      \"existing_cluster_id\": \"0914-084715-44dhyjfb\",\\\n      \"notebook_task\": {\\\n        \"notebook_path\": \"/Users/sharky@databricks.com/check_drift\",\\\n        \"source\": \"WORKSPACE\"\\\n      },\\\n      \"run_if\": \"ALL_SUCCESS\",\\\n      \"task_key\": \"check_drift\"\\\n    },\\\n    {\\\n      \"condition_task\": {\\\n        \"left\": \"{{tasks.check_drift.values.retrain}}\",\\\n        \"op\": \"EQUAL_TO\",\\\n        \"right\": \"true\"\\\n      },\\\n      \"depends_on\": [\\\n        {\\\n          \"task_key\": \"check_drift\"\\\n        }\\\n      ],\\\n      \"run_if\": \"ALL_SUCCESS\",\\\n      \"task_key\": \"if_drift_above_threshold\"\\\n    },\\\n    {\\\n      \"depends_on\": [\\\n        {\\\n          \"outcome\": \"true\",\\\n          \"task_key\": \"if_drift_above_threshold\"\\\n        }\\\n      ],\\\n      \"existing_cluster_id\": \"0914-084715-44dhyjfb\",\\\n      \"run_if\": \"ALL_SUCCESS\",\\\n      \"spark_python_task\": {\\\n        \"python_file\": \"/Users/sharky@databricks.com/retrain.py\"\\\n      },\\\n      \"task_key\": \"retrain_model\"\\\n    },\\\n    {\\\n      \"depends_on\": [\\\n        {\\\n          \"task_key\": \"retrain_model\"\\\n        },\\\n        {\\\n          \"outcome\": \"false\",\\\n          \"task_key\": \"if_drift_above_threshold\"\\\n        }\\\n      ],\\\n      \"run_if\": \"ALL_SUCCESS\",\\\n      \"sql_task\": {\\\n        \"dashboard\": {\\\n          \"dashboard_id\": \"0007ce2d-9d7d-48ca-b273-734c75080f58\"\\\n        },\\\n        \"warehouse_id\": \"791ba2a31c7fd70a\"\\\n      },\\\n      \"task_key\": \"refresh_shark_dashboard\"\\\n    }\\\n  ],\n  \"webhook_notifications\": {}\n}\n```\n\n# Response samples\n\n200\n\n```\n{\n  \"job_id\": 11223344\n}\n```"
  },
  {
    "markdown": "Top bar loading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading...\n\nLoading..."
  },
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## Update all job settings (reset)\n\n`\nPOST/api/2.1/jobs/reset`\n\nOverwrite all settings for the given job. Use the [_Update_ endpoint](https://docs.databricks.com/api/azure/workspace/jobs/update) to update job settings partially.\n\n### Request body\n\n[`job_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#job_id) requiredint64\n\nExample`11223344`\n\nThe canonical identifier of the job to reset. This field is required.\n\n[`new_settings`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings) requiredobject\n\nThe new settings of the job. These settings completely replace the old settings.\n\nChanges to the field `JobBaseSettings.timeout_seconds` are applied to active runs. Changes to other fields are applied to future runs only.\n\n[`name`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-name) string\n\n<= 4096 characters\n\nDefault`\"Untitled\"`\n\nExample`\"A multitask job\"`\n\nAn optional name for the job. The maximum length is 4096 bytes in UTF-8 encoding.\n\n[`description`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-description) string\n\n<= 27700 characters\n\nExample`\"This job contain multiple tasks that are required to produce the weekly shark sightings report.\"`\n\nAn optional description for the job. The maximum length is 27700 characters in UTF-8 encoding.\n\n[`email_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-email_notifications) object\n\nDefault`{}`\n\nAn optional set of email addresses that is notified when runs of this job begin or complete as well as when this job is deleted.\n\n[`webhook_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-webhook_notifications) object\n\nDefault`{}`\n\nA collection of system notification IDs to notify when runs of this job begin or complete.\n\n[`notification_settings`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-notification_settings) object\n\nDefault`{}`\n\nOptional notification settings that are used when sending notifications to each of the `email_notifications` and `webhook_notifications` for this job.\n\n[`timeout_seconds`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-timeout_seconds) int32\n\nDefault`0`\n\nExample`86400`\n\nAn optional timeout applied to each run of this job. A value of `0` means no timeout.\n\n[`health`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-health) object\n\nAn optional set of health rules that can be defined for this job.\n\n[`schedule`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-schedule) object\n\nAn optional periodic schedule for this job. The default behavior is that the job only runs when triggered by clicking “Run Now” in the Jobs UI or sending an API request to `runNow`.\n\n[`trigger`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-trigger) object\n\nA configuration to trigger a run when certain conditions are met. The default behavior is that the job runs only when triggered by clicking “Run Now” in the Jobs UI or sending an API request to `runNow`.\n\n[`continuous`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-continuous) object\n\nAn optional continuous property for this job. The continuous property will ensure that there is always one run executing. Only one of `schedule` and `continuous` can be used.\n\n[`max_concurrent_runs`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-max_concurrent_runs) int32\n\nDefault`1`\n\nExample`10`\n\nAn optional maximum allowed number of concurrent runs of the job.\nSet this value if you want to be able to execute multiple runs of the same job concurrently.\nThis is useful for example if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or if you want to trigger multiple runs which differ by their input parameters.\nThis setting affects only new runs. For example, suppose the job’s concurrency is 4 and there are 4 concurrent active runs. Then setting the concurrency to 3 won’t kill any of the active runs.\nHowever, from then on, new runs are skipped unless there are fewer than 3 active runs.\nThis value cannot exceed 1000. Setting this value to `0` causes all new runs to be skipped.\n\n[`tasks`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-tasks) Array of object\n\n<= 100 items\n\nExample\n\nA list of task specifications to be executed by this job.\nIf more than 100 tasks are available, you can paginate through them using [jobs/get](https://docs.databricks.com/api/azure/workspace/jobs/get). Use the `next_page_token` field at the object root to determine if more results are available.\n\n[`job_clusters`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-job_clusters) Array of object\n\n<= 100 items\n\nExample\n\nA list of job cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings.\nIf more than 100 job clusters are available, you can paginate through them using [jobs/get](https://docs.databricks.com/api/azure/workspace/jobs/get).\n\n[`git_source`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-git_source) object\n\nExample\n\nAn optional specification for a remote Git repository containing the source code used by tasks. Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.\n\nIf `git_source` is set, these tasks retrieve the file from the remote repository by default. However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.\n\nNote: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are used, `git_source` must be defined on the job.\n\n[`tags`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-tags) object\n\nDefault`{}`\n\nExample\n\nA map of tags associated with the job. These are forwarded to the cluster as cluster tags for jobs clusters, and are subject to the same limitations as cluster tags. A maximum of 25 tags can be added to the job.\n\n[`format`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-format) string\n\nDeprecated\n\nEnum: `SINGLE_TASK | MULTI_TASK`\n\nExample`\"MULTI_TASK\"`\n\nUsed to tell what is the format of the job. This field is ignored in Create/Update/Reset calls. When using the Jobs API 2.1 this value is always set to `\"MULTI_TASK\"`.\n\n[`queue`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-queue) object\n\nThe queue settings of the job.\n\n[`parameters`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-parameters) Array of object\n\nJob-level parameter definitions\n\n[`run_as`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-run_as) object\n\nWrite-only setting. Specifies the user or service principal that the job runs as. If not specified, the job runs as the user who created the job.\n\nEither `user_name` or `service_principal_name` should be specified. If not, an error is thrown.\n\n[`edit_mode`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-edit_mode) string\n\nEnum: `UI_LOCKED | EDITABLE`\n\nEdit mode of the job.\n\n- `UI_LOCKED`: The job is in a locked UI state and cannot be modified.\n- `EDITABLE`: The job is in an editable state and can be modified.\n\n[`deployment`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-deployment) object\n\nDeployment information for jobs managed by external sources.\n\n[`environments`](https://docs.databricks.com/api/azure/workspace/jobs_21/reset#new_settings-environments) Array of object\n\n<= 10 items\n\nA list of task execution environment specifications that can be referenced by serverless tasks of this job.\nAn environment is required to be present for serverless tasks.\nFor serverless notebook tasks, the environment is accessible in the notebook environment panel.\nFor other serverless tasks, the task environment is required to be specified using environment\\_key in the task settings.\n\n### Responses\n\n**200** Request completed successfully.\n\nRequest completed successfully.\n\nThis method might return the following HTTP codes: 400, 401, 403, 404, 429, 500\n\nError responses are returned in the following format:\n\n```\n{\n  \"error_code\": \"Error code\",\n  \"message\": \"Human-readable error message.\"\n}\n```\n\n# Possible error codes:\n\nHTTP code\n\nerror\\_code\n\nDescription\n\n400\n\nINVALID\\_PARAMETER\\_VALUE\n\nSupplied value for a parameter was invalid.\n\n401\n\nUNAUTHORIZED\n\nThe request does not have valid authentication credentials for the operation.\n\n403\n\nPERMISSION\\_DENIED\n\nCaller does not have permission to execute the specified operation.\n\n404\n\nFEATURE\\_DISABLED\n\nIf a given user/entity is trying to use a feature which has been disabled.\n\n429\n\nREQUEST\\_LIMIT\\_EXCEEDED\n\nRequest is rejected due to throttling.\n\n500\n\nINTERNAL\\_SERVER\\_ERROR\n\nInternal error.\n\n# Request samples\n\nJSON\n\n```\n{\n  \"job_id\": 11223344,\n  \"new_settings\": {\n    \"name\": \"A multitask job\",\n    \"description\": \"This job contain multiple tasks that are required to produce the weekly shark sightings report.\",\n    \"email_notifications\": {\n      \"on_start\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"on_success\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"on_failure\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"on_duration_warning_threshold_exceeded\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"on_streaming_backlog_exceeded\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"no_alert_for_skipped_runs\": false\n    },\n    \"webhook_notifications\": {\n      \"on_start\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ],\n      \"on_success\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ],\n      \"on_failure\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ],\n      \"on_duration_warning_threshold_exceeded\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ],\n      \"on_streaming_backlog_exceeded\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ]\n    },\n    \"notification_settings\": {\n      \"no_alert_for_skipped_runs\": false,\n      \"no_alert_for_canceled_runs\": false,\n      \"alert_on_last_attempt\": false\n    },\n    \"timeout_seconds\": 86400,\n    \"health\": {\n      \"rules\": [\\\n        {\\\n          \"metric\": \"RUN_DURATION_SECONDS\",\\\n          \"op\": \"GREATER_THAN\",\\\n          \"value\": 10\\\n        }\\\n      ]\n    },\n    \"schedule\": {\n      \"quartz_cron_expression\": \"20 30 * * * ?\",\n      \"timezone_id\": \"Europe/London\",\n      \"pause_status\": \"UNPAUSED\"\n    },\n    \"trigger\": {\n      \"pause_status\": \"UNPAUSED\",\n      \"file_arrival\": {\n        \"url\": \"string\",\n        \"min_time_between_triggers_seconds\": 0,\n        \"wait_after_last_change_seconds\": 0\n      },\n      \"periodic\": {\n        \"interval\": 0,\n        \"unit\": \"HOURS\"\n      }\n    },\n    \"continuous\": {\n      \"pause_status\": \"UNPAUSED\"\n    },\n    \"max_concurrent_runs\": 10,\n    \"tasks\": [\\\n      {\\\n        \"max_retries\": 3,\\\n        \"task_key\": \"Sessionize\",\\\n        \"description\": \"Extracts session data from events\",\\\n        \"min_retry_interval_millis\": 2000,\\\n        \"depends_on\": [],\\\n        \"timeout_seconds\": 86400,\\\n        \"spark_jar_task\": {\\\n          \"main_class_name\": \"com.databricks.Sessionize\",\\\n          \"parameters\": [\\\n            \"--data\",\\\n            \"dbfs:/path/to/data.json\"\\\n          ]\\\n        },\\\n        \"libraries\": [\\\n          {\\\n            \"jar\": \"dbfs:/mnt/databricks/Sessionize.jar\"\\\n          }\\\n        ],\\\n        \"retry_on_timeout\": false,\\\n        \"existing_cluster_id\": \"0923-164208-meows279\"\\\n      },\\\n      {\\\n        \"max_retries\": 3,\\\n        \"task_key\": \"Orders_Ingest\",\\\n        \"description\": \"Ingests order data\",\\\n        \"job_cluster_key\": \"auto_scaling_cluster\",\\\n        \"min_retry_interval_millis\": 2000,\\\n        \"depends_on\": [],\\\n        \"timeout_seconds\": 86400,\\\n        \"spark_jar_task\": {\\\n          \"main_class_name\": \"com.databricks.OrdersIngest\",\\\n          \"parameters\": [\\\n            \"--data\",\\\n            \"dbfs:/path/to/order-data.json\"\\\n          ]\\\n        },\\\n        \"libraries\": [\\\n          {\\\n            \"jar\": \"dbfs:/mnt/databricks/OrderIngest.jar\"\\\n          }\\\n        ],\\\n        \"retry_on_timeout\": false\\\n      },\\\n      {\\\n        \"max_retries\": 3,\\\n        \"task_key\": \"Match\",\\\n        \"description\": \"Matches orders with user sessions\",\\\n        \"notebook_task\": {\\\n          \"base_parameters\": {\\\n            \"age\": \"35\",\\\n            \"name\": \"John Doe\"\\\n          },\\\n          \"notebook_path\": \"/Users/user.name@databricks.com/Match\"\\\n        },\\\n        \"min_retry_interval_millis\": 2000,\\\n        \"depends_on\": [\\\n          {\\\n            \"task_key\": \"Orders_Ingest\"\\\n          },\\\n          {\\\n            \"task_key\": \"Sessionize\"\\\n          }\\\n        ],\\\n        \"new_cluster\": {\\\n          \"autoscale\": {\\\n            \"max_workers\": 16,\\\n            \"min_workers\": 2\\\n          },\\\n          \"node_type_id\": null,\\\n          \"spark_conf\": {\\\n            \"spark.speculation\": true\\\n          },\\\n          \"spark_version\": \"7.3.x-scala2.12\"\\\n        },\\\n        \"timeout_seconds\": 86400,\\\n        \"retry_on_timeout\": false,\\\n        \"run_if\": \"ALL_SUCCESS\"\\\n      }\\\n    ],\n    \"job_clusters\": [\\\n      {\\\n        \"job_cluster_key\": \"auto_scaling_cluster\",\\\n        \"new_cluster\": {\\\n          \"autoscale\": {\\\n            \"max_workers\": 16,\\\n            \"min_workers\": 2\\\n          },\\\n          \"node_type_id\": null,\\\n          \"spark_conf\": {\\\n            \"spark.speculation\": true\\\n          },\\\n          \"spark_version\": \"7.3.x-scala2.12\"\\\n        }\\\n      }\\\n    ],\n    \"git_source\": {\n      \"git_branch\": \"main\",\n      \"git_provider\": \"gitHub\",\n      \"git_url\": \"https://github.com/databricks/databricks-cli\"\n    },\n    \"tags\": {\n      \"cost-center\": \"engineering\",\n      \"team\": \"jobs\"\n    },\n    \"format\": \"SINGLE_TASK\",\n    \"queue\": {\n      \"enabled\": true\n    },\n    \"parameters\": [\\\n      {\\\n        \"default\": \"users\",\\\n        \"name\": \"table\"\\\n      }\\\n    ],\n    \"run_as\": {\n      \"user_name\": \"user@databricks.com\",\n      \"service_principal_name\": \"692bc6d0-ffa3-11ed-be56-0242ac120002\"\n    },\n    \"edit_mode\": \"UI_LOCKED\",\n    \"deployment\": {\n      \"kind\": \"BUNDLE\",\n      \"metadata_file_path\": \"string\"\n    },\n    \"environments\": [\\\n      {\\\n        \"environment_key\": \"string\",\\\n        \"spec\": {\\\n          \"client\": \"1\",\\\n          \"dependencies\": [\\\n            \"string\"\\\n          ]\\\n        }\\\n      }\\\n    ]\n  }\n}\n```\n\n# Response samples\n\n200\n\n```\n{}\n```"
  },
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## Get a single job\n\n`\nGET/api/2.1/jobs/get`\n\nRetrieves the details for a single job.\n\n### Query parameters\n\n[`job_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#job_id) requiredint64\n\nExample`job_id=11223344`\n\nThe canonical identifier of the job to retrieve information about. This field is required.\n\n### Responses\n\n**200** Request completed successfully.\n\nRequest completed successfully.\n\n[`job_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#job_id) int64\n\nExample`11223344`\n\nThe canonical identifier for this job.\n\n[`creator_user_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#creator_user_name) string\n\nExample`\"user.name@databricks.com\"`\n\nThe creator user name. This field won’t be included in the response if the user has already been deleted.\n\n[`run_as_user_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#run_as_user_name) string\n\nExample`\"user.name@databricks.com\"`\n\nThe email of an active workspace user or the application ID of a service principal that the job runs as. This value can be changed by setting the `run_as` field when creating or updating a job.\n\nBy default, `run_as_user_name` is based on the current job settings and is set to the creator of the job if job access control is disabled or to the user with the `is_owner` permission if job access control is enabled.\n\n[`settings`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings) object\n\nSettings for this job and all of its runs. These settings can be updated using the `resetJob` method.\n\n[`name`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-name) string\n\n<= 4096 characters\n\nDefault`\"Untitled\"`\n\nExample`\"A multitask job\"`\n\nAn optional name for the job. The maximum length is 4096 bytes in UTF-8 encoding.\n\n[`description`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-description) string\n\n<= 27700 characters\n\nExample`\"This job contain multiple tasks that are required to produce the weekly shark sightings report.\"`\n\nAn optional description for the job. The maximum length is 27700 characters in UTF-8 encoding.\n\n[`email_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-email_notifications) object\n\nDefault`{}`\n\nAn optional set of email addresses that is notified when runs of this job begin or complete as well as when this job is deleted.\n\n[`webhook_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-webhook_notifications) object\n\nDefault`{}`\n\nA collection of system notification IDs to notify when runs of this job begin or complete.\n\n[`notification_settings`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-notification_settings) object\n\nDefault`{}`\n\nOptional notification settings that are used when sending notifications to each of the `email_notifications` and `webhook_notifications` for this job.\n\n[`timeout_seconds`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-timeout_seconds) int32\n\nDefault`0`\n\nExample`86400`\n\nAn optional timeout applied to each run of this job. A value of `0` means no timeout.\n\n[`health`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-health) object\n\nAn optional set of health rules that can be defined for this job.\n\n[`schedule`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-schedule) object\n\nAn optional periodic schedule for this job. The default behavior is that the job only runs when triggered by clicking “Run Now” in the Jobs UI or sending an API request to `runNow`.\n\n[`trigger`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-trigger) object\n\nA configuration to trigger a run when certain conditions are met. The default behavior is that the job runs only when triggered by clicking “Run Now” in the Jobs UI or sending an API request to `runNow`.\n\n[`continuous`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-continuous) object\n\nAn optional continuous property for this job. The continuous property will ensure that there is always one run executing. Only one of `schedule` and `continuous` can be used.\n\n[`max_concurrent_runs`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-max_concurrent_runs) int32\n\nDefault`1`\n\nExample`10`\n\nAn optional maximum allowed number of concurrent runs of the job.\nSet this value if you want to be able to execute multiple runs of the same job concurrently.\nThis is useful for example if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or if you want to trigger multiple runs which differ by their input parameters.\nThis setting affects only new runs. For example, suppose the job’s concurrency is 4 and there are 4 concurrent active runs. Then setting the concurrency to 3 won’t kill any of the active runs.\nHowever, from then on, new runs are skipped unless there are fewer than 3 active runs.\nThis value cannot exceed 1000. Setting this value to `0` causes all new runs to be skipped.\n\n[`tasks`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-tasks) Array of object\n\n<= 100 items\n\nExample\n\nA list of task specifications to be executed by this job.\nIf more than 100 tasks are available, you can paginate through them using [jobs/get](https://docs.databricks.com/api/azure/workspace/jobs/get). Use the `next_page_token` field at the object root to determine if more results are available.\n\n[`job_clusters`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-job_clusters) Array of object\n\n<= 100 items\n\nExample\n\nA list of job cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings.\nIf more than 100 job clusters are available, you can paginate through them using [jobs/get](https://docs.databricks.com/api/azure/workspace/jobs/get).\n\n[`git_source`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-git_source) object\n\nExample\n\nAn optional specification for a remote Git repository containing the source code used by tasks. Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.\n\nIf `git_source` is set, these tasks retrieve the file from the remote repository by default. However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.\n\nNote: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are used, `git_source` must be defined on the job.\n\n[`tags`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-tags) object\n\nDefault`{}`\n\nExample\n\nA map of tags associated with the job. These are forwarded to the cluster as cluster tags for jobs clusters, and are subject to the same limitations as cluster tags. A maximum of 25 tags can be added to the job.\n\n[`format`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-format) string\n\nDeprecated\n\nEnum: `SINGLE_TASK | MULTI_TASK`\n\nExample`\"MULTI_TASK\"`\n\nUsed to tell what is the format of the job. This field is ignored in Create/Update/Reset calls. When using the Jobs API 2.1 this value is always set to `\"MULTI_TASK\"`.\n\n[`queue`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-queue) object\n\nThe queue settings of the job.\n\n[`parameters`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-parameters) Array of object\n\nJob-level parameter definitions\n\n[`edit_mode`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-edit_mode) string\n\nEnum: `UI_LOCKED | EDITABLE`\n\nEdit mode of the job.\n\n- `UI_LOCKED`: The job is in a locked UI state and cannot be modified.\n- `EDITABLE`: The job is in an editable state and can be modified.\n\n[`deployment`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-deployment) object\n\nDeployment information for jobs managed by external sources.\n\n[`environments`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#settings-environments) Array of object\n\n<= 10 items\n\nA list of task execution environment specifications that can be referenced by serverless tasks of this job.\nAn environment is required to be present for serverless tasks.\nFor serverless notebook tasks, the environment is accessible in the notebook environment panel.\nFor other serverless tasks, the task environment is required to be specified using environment\\_key in the task settings.\n\n[`created_time`](https://docs.databricks.com/api/azure/workspace/jobs_21/get#created_time) int64\n\nExample`1601370337343`\n\nThe time at which this job was created in epoch milliseconds (milliseconds since 1/1/1970 UTC).\n\nThis method might return the following HTTP codes: 400, 401, 403, 429, 500\n\nError responses are returned in the following format:\n\n```\n{\n  \"error_code\": \"Error code\",\n  \"message\": \"Human-readable error message.\"\n}\n```\n\n# Possible error codes:\n\nHTTP code\n\nerror\\_code\n\nDescription\n\n400\n\nINVALID\\_PARAMETER\\_VALUE\n\nSupplied value for a parameter was invalid.\n\n401\n\nUNAUTHORIZED\n\nThe request does not have valid authentication credentials for the operation.\n\n403\n\nPERMISSION\\_DENIED\n\nCaller does not have permission to execute the specified operation.\n\n429\n\nREQUEST\\_LIMIT\\_EXCEEDED\n\nRequest is rejected due to throttling.\n\n500\n\nINTERNAL\\_SERVER\\_ERROR\n\nInternal error.\n\n# Response samples\n\n200\n\n```\n{\n  \"job_id\": 11223344,\n  \"creator_user_name\": \"user.name@databricks.com\",\n  \"run_as_user_name\": \"user.name@databricks.com\",\n  \"settings\": {\n    \"name\": \"A multitask job\",\n    \"description\": \"This job contain multiple tasks that are required to produce the weekly shark sightings report.\",\n    \"email_notifications\": {\n      \"on_start\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"on_success\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"on_failure\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"on_duration_warning_threshold_exceeded\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"on_streaming_backlog_exceeded\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"no_alert_for_skipped_runs\": false\n    },\n    \"webhook_notifications\": {\n      \"on_start\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ],\n      \"on_success\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ],\n      \"on_failure\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ],\n      \"on_duration_warning_threshold_exceeded\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ],\n      \"on_streaming_backlog_exceeded\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ]\n    },\n    \"notification_settings\": {\n      \"no_alert_for_skipped_runs\": false,\n      \"no_alert_for_canceled_runs\": false,\n      \"alert_on_last_attempt\": false\n    },\n    \"timeout_seconds\": 86400,\n    \"health\": {\n      \"rules\": [\\\n        {\\\n          \"metric\": \"RUN_DURATION_SECONDS\",\\\n          \"op\": \"GREATER_THAN\",\\\n          \"value\": 10\\\n        }\\\n      ]\n    },\n    \"schedule\": {\n      \"quartz_cron_expression\": \"20 30 * * * ?\",\n      \"timezone_id\": \"Europe/London\",\n      \"pause_status\": \"UNPAUSED\"\n    },\n    \"trigger\": {\n      \"pause_status\": \"UNPAUSED\",\n      \"file_arrival\": {\n        \"url\": \"string\",\n        \"min_time_between_triggers_seconds\": 0,\n        \"wait_after_last_change_seconds\": 0\n      },\n      \"periodic\": {\n        \"interval\": 0,\n        \"unit\": \"HOURS\"\n      }\n    },\n    \"continuous\": {\n      \"pause_status\": \"UNPAUSED\"\n    },\n    \"max_concurrent_runs\": 10,\n    \"tasks\": [\\\n      {\\\n        \"max_retries\": 3,\\\n        \"task_key\": \"Sessionize\",\\\n        \"description\": \"Extracts session data from events\",\\\n        \"min_retry_interval_millis\": 2000,\\\n        \"depends_on\": [],\\\n        \"timeout_seconds\": 86400,\\\n        \"spark_jar_task\": {\\\n          \"main_class_name\": \"com.databricks.Sessionize\",\\\n          \"parameters\": [\\\n            \"--data\",\\\n            \"dbfs:/path/to/data.json\"\\\n          ]\\\n        },\\\n        \"libraries\": [\\\n          {\\\n            \"jar\": \"dbfs:/mnt/databricks/Sessionize.jar\"\\\n          }\\\n        ],\\\n        \"retry_on_timeout\": false,\\\n        \"existing_cluster_id\": \"0923-164208-meows279\"\\\n      },\\\n      {\\\n        \"max_retries\": 3,\\\n        \"task_key\": \"Orders_Ingest\",\\\n        \"description\": \"Ingests order data\",\\\n        \"job_cluster_key\": \"auto_scaling_cluster\",\\\n        \"min_retry_interval_millis\": 2000,\\\n        \"depends_on\": [],\\\n        \"timeout_seconds\": 86400,\\\n        \"spark_jar_task\": {\\\n          \"main_class_name\": \"com.databricks.OrdersIngest\",\\\n          \"parameters\": [\\\n            \"--data\",\\\n            \"dbfs:/path/to/order-data.json\"\\\n          ]\\\n        },\\\n        \"libraries\": [\\\n          {\\\n            \"jar\": \"dbfs:/mnt/databricks/OrderIngest.jar\"\\\n          }\\\n        ],\\\n        \"retry_on_timeout\": false\\\n      },\\\n      {\\\n        \"max_retries\": 3,\\\n        \"task_key\": \"Match\",\\\n        \"description\": \"Matches orders with user sessions\",\\\n        \"notebook_task\": {\\\n          \"base_parameters\": {\\\n            \"age\": \"35\",\\\n            \"name\": \"John Doe\"\\\n          },\\\n          \"notebook_path\": \"/Users/user.name@databricks.com/Match\"\\\n        },\\\n        \"min_retry_interval_millis\": 2000,\\\n        \"depends_on\": [\\\n          {\\\n            \"task_key\": \"Orders_Ingest\"\\\n          },\\\n          {\\\n            \"task_key\": \"Sessionize\"\\\n          }\\\n        ],\\\n        \"new_cluster\": {\\\n          \"autoscale\": {\\\n            \"max_workers\": 16,\\\n            \"min_workers\": 2\\\n          },\\\n          \"node_type_id\": null,\\\n          \"spark_conf\": {\\\n            \"spark.speculation\": true\\\n          },\\\n          \"spark_version\": \"7.3.x-scala2.12\"\\\n        },\\\n        \"timeout_seconds\": 86400,\\\n        \"retry_on_timeout\": false,\\\n        \"run_if\": \"ALL_SUCCESS\"\\\n      }\\\n    ],\n    \"job_clusters\": [\\\n      {\\\n        \"job_cluster_key\": \"auto_scaling_cluster\",\\\n        \"new_cluster\": {\\\n          \"autoscale\": {\\\n            \"max_workers\": 16,\\\n            \"min_workers\": 2\\\n          },\\\n          \"node_type_id\": null,\\\n          \"spark_conf\": {\\\n            \"spark.speculation\": true\\\n          },\\\n          \"spark_version\": \"7.3.x-scala2.12\"\\\n        }\\\n      }\\\n    ],\n    \"git_source\": {\n      \"git_branch\": \"main\",\n      \"git_provider\": \"gitHub\",\n      \"git_url\": \"https://github.com/databricks/databricks-cli\"\n    },\n    \"tags\": {\n      \"cost-center\": \"engineering\",\n      \"team\": \"jobs\"\n    },\n    \"format\": \"SINGLE_TASK\",\n    \"queue\": {\n      \"enabled\": true\n    },\n    \"parameters\": [\\\n      {\\\n        \"default\": \"users\",\\\n        \"name\": \"table\"\\\n      }\\\n    ],\n    \"run_as\": {\n      \"user_name\": \"user@databricks.com\",\n      \"service_principal_name\": \"692bc6d0-ffa3-11ed-be56-0242ac120002\"\n    },\n    \"edit_mode\": \"UI_LOCKED\",\n    \"deployment\": {\n      \"kind\": \"BUNDLE\",\n      \"metadata_file_path\": \"string\"\n    },\n    \"environments\": [\\\n      {\\\n        \"environment_key\": \"string\",\\\n        \"spec\": {\\\n          \"client\": \"1\",\\\n          \"dependencies\": [\\\n            \"string\"\\\n          ]\\\n        }\\\n      }\\\n    ]\n  },\n  \"created_time\": 1601370337343\n}\n```"
  },
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## Delete a job run\n\n`\nPOST/api/2.1/jobs/runs/delete`\n\nDeletes a non-active run. Returns an error if the run is active.\n\n### Request body\n\n[`run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun#run_id) requiredint64\n\nExample`455644833`\n\nID of the run to delete.\n\n### Responses\n\n**200** Request completed successfully.\n\nRequest completed successfully.\n\nThis method might return the following HTTP codes: 400, 401, 403, 429, 500\n\nError responses are returned in the following format:\n\n```\n{\n  \"error_code\": \"Error code\",\n  \"message\": \"Human-readable error message.\"\n}\n```\n\n# Possible error codes:\n\nHTTP code\n\nerror\\_code\n\nDescription\n\n400\n\nINVALID\\_PARAMETER\\_VALUE\n\nSupplied value for a parameter was invalid.\n\n401\n\nUNAUTHORIZED\n\nThe request does not have valid authentication credentials for the operation.\n\n403\n\nPERMISSION\\_DENIED\n\nCaller does not have permission to execute the specified operation.\n\n429\n\nREQUEST\\_LIMIT\\_EXCEEDED\n\nRequest is rejected due to throttling.\n\n500\n\nINTERNAL\\_SERVER\\_ERROR\n\nInternal error.\n\n# Request samples\n\nJSON\n\n```\n{\n  \"run_id\": 455644833\n}\n```\n\n# Response samples\n\n200\n\n```\n{}\n```"
  },
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## Cancel all runs of a job\n\n`\nPOST/api/2.1/jobs/runs/cancel-all`\n\nCancels all active runs of a job. The runs are canceled asynchronously, so it doesn't\nprevent new runs from being started.\n\n### Request body\n\n[`job_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns#job_id) int64\n\nExample`11223344`\n\nThe canonical identifier of the job to cancel all runs of.\n\n[`all_queued_runs`](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns#all_queued_runs) boolean\n\nDefault`false`\n\nOptional boolean parameter to cancel all queued runs. If no job\\_id is provided, all queued runs in the workspace are canceled.\n\n### Responses\n\n**200** Request completed successfully.\n\nRequest completed successfully.\n\nThis method might return the following HTTP codes: 400, 401, 403, 429, 500\n\nError responses are returned in the following format:\n\n```\n{\n  \"error_code\": \"Error code\",\n  \"message\": \"Human-readable error message.\"\n}\n```\n\n# Possible error codes:\n\nHTTP code\n\nerror\\_code\n\nDescription\n\n400\n\nINVALID\\_PARAMETER\\_VALUE\n\nSupplied value for a parameter was invalid.\n\n401\n\nUNAUTHORIZED\n\nThe request does not have valid authentication credentials for the operation.\n\n403\n\nPERMISSION\\_DENIED\n\nCaller does not have permission to execute the specified operation.\n\n429\n\nREQUEST\\_LIMIT\\_EXCEEDED\n\nRequest is rejected due to throttling.\n\n500\n\nINTERNAL\\_SERVER\\_ERROR\n\nInternal error.\n\n# Request samples\n\nJSON\n\n```\n{\n  \"job_id\": 11223344,\n  \"all_queued_runs\": false\n}\n```\n\n# Response samples\n\n200\n\n```\n{}\n```\n\n[iframe](https://insight.adsrvr.org/track/up?adv=vg37y0i&ref=https%3A%2F%2Fdocs.databricks.com%2Fapi%2Fazure%2Fworkspace%2Fjobs_21%2Fcancelallruns&upid=u69d5s0&upv=1.1.0&paapi=1)"
  },
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## Get a single job run\n\n`\nGET/api/2.1/jobs/runs/get`\n\nRetrieve the metadata of a run.\n\n### Query parameters\n\n[`run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#run_id) requiredint64\n\nExample`run_id=455644833`\n\nThe canonical identifier of the run for which to retrieve the metadata.\nThis field is required.\n\n[`include_history`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#include_history) boolean\n\nDefault`false`\n\nExample`include_history=true`\n\nWhether to include the repair history in the response.\n\n[`include_resolved_values`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#include_resolved_values) boolean\n\nDefault`false`\n\nWhether to include resolved parameter values in the response.\n\n### Responses\n\n**200** Request completed successfully.\n\nRequest completed successfully.\n\n[`job_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#job_id) int64\n\nExample`11223344`\n\nThe canonical identifier of the job that contains this run.\n\n[`run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#run_id) int64\n\nExample`455644833`\n\nThe canonical identifier of the run. This ID is unique across all runs of all jobs.\n\n[`creator_user_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#creator_user_name) string\n\nExample`\"user.name@databricks.com\"`\n\nThe creator user name. This field won’t be included in the response if the user has already been deleted.\n\n[`number_in_job`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#number_in_job) int64\n\nDeprecated\n\nExample`455644833`\n\nA unique identifier for this job run. This is set to the same value as `run_id`.\n\n[`original_attempt_run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#original_attempt_run_id) int64\n\nExample`455644833`\n\nIf this run is a retry of a prior run attempt, this field contains the run\\_id of the original attempt; otherwise, it is the same as the run\\_id.\n\n[`state`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#state) object\n\nDeprecated\n\nDeprecated. Please use the `status` field instead.\n\n[`life_cycle_state`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#state-life_cycle_state) string\n\nEnum: `PENDING | RUNNING | TERMINATING | TERMINATED | SKIPPED | INTERNAL_ERROR | BLOCKED | WAITING_FOR_RETRY | QUEUED`\n\nA value indicating the run's current lifecycle state. This field is always available in the response.\n\n[`result_state`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#state-result_state) string\n\nEnum: `SUCCESS | FAILED | TIMEDOUT | CANCELED | MAXIMUM_CONCURRENT_RUNS_REACHED | UPSTREAM_CANCELED | UPSTREAM_FAILED | EXCLUDED | SUCCESS_WITH_FAILURES | DISABLED`\n\nA value indicating the run's result. This field is only available for terminal lifecycle states.\n\n[`state_message`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#state-state_message) string\n\nA descriptive message for the current state. This field is unstructured, and its exact format is subject to change.\n\n[`user_cancelled_or_timedout`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#state-user_cancelled_or_timedout) boolean\n\nDefault`false`\n\nA value indicating whether a run was canceled manually by a user or by the scheduler because the run timed out.\n\n[`queue_reason`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#state-queue_reason) string\n\nExample`\"Queued due to reaching maximum concurrent runs of 1.\"`\n\nThe reason indicating why the run was queued.\n\n[`schedule`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#schedule) object\n\nThe cron schedule that triggered this run if it was triggered by the periodic scheduler.\n\n[`quartz_cron_expression`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#schedule-quartz_cron_expression) requiredstring\n\nExample`\"20 30 * * * ?\"`\n\nA Cron expression using Quartz syntax that describes the schedule for a job. See [Cron Trigger](http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html) for details. This field is required.\n\n[`timezone_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#schedule-timezone_id) requiredstring\n\nExample`\"Europe/London\"`\n\nA Java timezone ID. The schedule for a job is resolved with respect to this timezone. See [Java TimeZone](https://docs.oracle.com/javase/7/docs/api/java/util/TimeZone.html) for details. This field is required.\n\n[`pause_status`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#schedule-pause_status) string\n\nEnum: `UNPAUSED | PAUSED`\n\nDefault`\"UNPAUSED\"`\n\nIndicate whether this schedule is paused or not.\n\n[`cluster_spec`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#cluster_spec) object\n\nA snapshot of the job’s cluster specification when this run was created.\n\n[`existing_cluster_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#cluster_spec-existing_cluster_id) string\n\nExample`\"0923-164208-meows279\"`\n\nIf existing\\_cluster\\_id, the ID of an existing cluster that is used for all runs.\nWhen running jobs or tasks on an existing cluster, you may need to manually restart\nthe cluster if it stops responding. We suggest running jobs and tasks on new clusters for\ngreater reliability\n\n[`new_cluster`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#cluster_spec-new_cluster) object\n\nIf new\\_cluster, a description of a new cluster that is created for each run.\n\n[`job_cluster_key`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#cluster_spec-job_cluster_key) string\n\n\\[ 1 .. 100 \\] characters ^\\[\\\\w\\\\-\\\\\\_\\]+$\n\nIf job\\_cluster\\_key, this task is executed reusing the cluster specified in `job.settings.job_clusters`.\n\n[`libraries`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#cluster_spec-libraries) Array of object\n\nAn optional list of libraries to be installed on the cluster.\nThe default value is an empty list.\n\n[`cluster_instance`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#cluster_instance) object\n\nThe cluster used for this run. If the run is specified to use a new cluster, this field is set once the Jobs service has requested a cluster for the run.\n\n[`cluster_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#cluster_instance-cluster_id) string\n\nExample`\"0923-164208-meows279\"`\n\nThe canonical identifier for the cluster used by a run. This field is always available for runs on existing clusters. For runs on new clusters, it becomes available once the cluster is created. This value can be used to view logs by browsing to `/#setting/sparkui/$cluster_id/driver-logs`. The logs continue to be available after the run completes.\n\nThe response won’t include this field if the identifier is not available yet.\n\n[`spark_context_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#cluster_instance-spark_context_id) string\n\nThe canonical identifier for the Spark context used by a run. This field is filled in once the run begins execution. This value can be used to view the Spark UI by browsing to `/#setting/sparkui/$cluster_id/$spark_context_id`. The Spark UI continues to be available after the run has completed.\n\nThe response won’t include this field if the identifier is not available yet.\n\n[`job_parameters`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#job_parameters) Array of object\n\nJob-level parameters used in the run\n\nArray \\[\\\n\\\n[`name`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#job_parameters-name) string\\\n\\\nExample`\"table\"`\\\n\\\nThe name of the parameter\\\n\\\n[`default`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#job_parameters-default) string\\\n\\\nExample`\"users\"`\\\n\\\nThe optional default value of the parameter\\\n\\\n[`value`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#job_parameters-value) string\\\n\\\nExample`\"customers\"`\\\n\\\nThe value used in the run\\\n\\\n\\]\n\n[`overriding_parameters`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#overriding_parameters) object\n\nThe parameters used for this run.\n\n[`pipeline_params`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#overriding_parameters-pipeline_params) object\n\nControls whether the pipeline should perform a full refresh\n\n[`jar_params`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#overriding_parameters-jar_params) Array of string\n\nDeprecated\n\nExample\n\nA list of parameters for jobs with Spark JAR tasks, for example `\"jar_params\": [\"john doe\", \"35\"]`.\nThe parameters are used to invoke the main function of the main class specified in the Spark JAR task.\nIf not specified upon `run-now`, it defaults to an empty list.\njar\\_params cannot be specified in conjunction with notebook\\_params.\nThe JSON representation of this field (for example `{\"jar_params\":[\"john doe\",\"35\"]}`) cannot exceed 10,000 bytes.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n\n[`notebook_params`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#overriding_parameters-notebook_params) object\n\nDeprecated\n\nExample\n\nA map from keys to values for jobs with notebook task, for example `\"notebook_params\": {\"name\": \"john doe\", \"age\": \"35\"}`.\nThe map is passed to the notebook and is accessible through the [dbutils.widgets.get](https://docs.databricks.com/dev-tools/databricks-utils.html) function.\n\nIf not specified upon `run-now`, the triggered run uses the job’s base parameters.\n\nnotebook\\_params cannot be specified in conjunction with jar\\_params.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n\nThe JSON representation of this field (for example `{\"notebook_params\":{\"name\":\"john doe\",\"age\":\"35\"}}`) cannot exceed 10,000 bytes.\n\n[`python_params`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#overriding_parameters-python_params) Array of string\n\nDeprecated\n\nExample\n\nA list of parameters for jobs with Python tasks, for example `\"python_params\": [\"john doe\", \"35\"]`.\nThe parameters are passed to Python file as command-line parameters. If specified upon `run-now`, it would overwrite\nthe parameters specified in job setting. The JSON representation of this field (for example `{\"python_params\":[\"john doe\",\"35\"]}`)\ncannot exceed 10,000 bytes.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n\nImportant\n\nThese parameters accept only Latin characters (ASCII character set). Using non-ASCII characters returns an error.\nExamples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis.\n\n[`spark_submit_params`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#overriding_parameters-spark_submit_params) Array of string\n\nDeprecated\n\nExample\n\nA list of parameters for jobs with spark submit task, for example `\"spark_submit_params\": [\"--class\", \"org.apache.spark.examples.SparkPi\"]`.\nThe parameters are passed to spark-submit script as command-line parameters. If specified upon `run-now`, it would overwrite the\nparameters specified in job setting. The JSON representation of this field (for example `{\"python_params\":[\"john doe\",\"35\"]}`)\ncannot exceed 10,000 bytes.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs\n\nImportant\n\nThese parameters accept only Latin characters (ASCII character set). Using non-ASCII characters returns an error.\nExamples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis.\n\n[`python_named_params`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#overriding_parameters-python_named_params) object\n\nDeprecated\n\nExample\n\n[`sql_params`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#overriding_parameters-sql_params) object\n\nDeprecated\n\nExample\n\nA map from keys to values for jobs with SQL task, for example `\"sql_params\": {\"name\": \"john doe\", \"age\": \"35\"}`. The SQL alert task does not support custom parameters.\n\n[`dbt_commands`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#overriding_parameters-dbt_commands) Array of string\n\nDeprecated\n\nExample\n\nAn array of commands to execute for jobs with the dbt task, for example `\"dbt_commands\": [\"dbt deps\", \"dbt seed\", \"dbt deps\", \"dbt seed\", \"dbt run\"]`\n\n[`start_time`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#start_time) int64\n\nExample`1625060460483`\n\nThe time at which this run was started in epoch milliseconds (milliseconds since 1/1/1970 UTC). This may not be the time when the job task starts executing, for example, if the job is scheduled to run on a new cluster, this is the time the cluster creation call is issued.\n\n[`setup_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#setup_duration) int64\n\nExample`0`\n\nThe time in milliseconds it took to set up the cluster. For runs that run on new clusters this is the cluster creation time, for runs that run on existing clusters this time should be very short. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `setup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.\n\n[`execution_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#execution_duration) int64\n\nExample`0`\n\nThe time in milliseconds it took to execute the commands in the JAR or notebook until they completed, failed, timed out, were cancelled, or encountered an unexpected error. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `execution_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.\n\n[`cleanup_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#cleanup_duration) int64\n\nExample`0`\n\nThe time in milliseconds it took to terminate the cluster and clean up any associated artifacts. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `cleanup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.\n\n[`end_time`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#end_time) int64\n\nExample`1625060863413`\n\nThe time at which this run ended in epoch milliseconds (milliseconds since 1/1/1970 UTC). This field is set to 0 if the job is still running.\n\n[`run_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#run_duration) int64\n\nExample`110183`\n\nThe time in milliseconds it took the job run and all of its repairs to finish.\n\n[`queue_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#queue_duration) int64\n\nExample`1625060863413`\n\nThe time in milliseconds that the run has spent in the queue.\n\n[`trigger`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#trigger) string\n\nEnum: `PERIODIC | ONE_TIME | RETRY | RUN_JOB_TASK | FILE_ARRIVAL | TABLE`\n\nThe type of trigger that fired this run.\n\n- `PERIODIC`: Schedules that periodically trigger runs, such as a cron scheduler.\n- `ONE_TIME`: One time triggers that fire a single run. This occurs you triggered a single run on demand through the UI or the API.\n- `RETRY`: Indicates a run that is triggered as a retry of a previously failed run. This occurs when you request to re-run the job in case of failures.\n- `RUN_JOB_TASK`: Indicates a run that is triggered using a Run Job task.\n- `FILE_ARRIVAL`: Indicates a run that is triggered by a file arrival.\n- `TABLE`: Indicates a run that is triggered by a table update.\n- `CONTINUOUS_RESTART`: Indicates a run created by user to manually restart a continuous job run.\n\n[`trigger_info`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#trigger_info) object\n\nAdditional details about what triggered the run\n\n[`run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#trigger_info-run_id) int64\n\nThe run id of the Run Job task run\n\n[`run_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#run_name) string\n\n<= 4096 characters\n\nDefault`\"Untitled\"`\n\nExample`\"A multitask job run\"`\n\nAn optional name for the run. The maximum length is 4096 bytes in UTF-8 encoding.\n\n[`run_page_url`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#run_page_url) string\n\nExample`\"https://my-workspace.cloud.databricks.com/#job/11223344/run/123\"`\n\nThe URL to the detail page of the run.\n\n[`run_type`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#run_type) string\n\nEnum: `JOB_RUN | WORKFLOW_RUN | SUBMIT_RUN`\n\nThe type of a run.\n\n- `JOB_RUN`: Normal job run. A run created with [jobs/runnow](https://docs.databricks.com/api/azure/workspace/jobs/runnow).\n- `WORKFLOW_RUN`: Workflow run. A run created with [dbutils.notebook.run](https://docs.databricks.com/dev-tools/databricks-utils.html#dbutils-workflow).\n- `SUBMIT_RUN`: Submit run. A run created with [jobs/submit](https://docs.databricks.com/api/azure/workspace/jobs/submit).\n\n[`tasks`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks) Array of object\n\n<= 100 items\n\nExample\n\nThe list of tasks performed by the run. Each task has its own `run_id` which you can use to call `JobsGetOutput` to retrieve the run resutls.\nIf more than 100 tasks are available, you can paginate through them using [jobs/getrun](https://docs.databricks.com/api/azure/workspace/jobs/getrun). Use the `next_page_token` field at the object root to determine if more results are available.\n\nArray \\[\\\n\\\n[`run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-run_id) int64\\\n\\\nExample`99887766`\\\n\\\nThe ID of the task run.\\\n\\\n[`task_key`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-task_key) requiredstring\\\n\\\n\\[ 1 .. 100 \\] characters ^\\[\\\\w\\\\-\\\\\\_\\]+$\\\n\\\nExample`\"Task_Key\"`\\\n\\\nA unique name for the task. This field is used to refer to this task from other tasks.\\\nThis field is required and must be unique within its parent job.\\\nOn Update or Reset, this field is used to reference the tasks to be updated or reset.\\\n\\\n[`description`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-description) string\\\n\\\n<= 1000 characters\\\n\\\nExample`\"This is the description for this task.\"`\\\n\\\nAn optional description for this task.\\\n\\\n[`depends_on`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-depends_on) Array of object\\\n\\\nAn optional array of objects specifying the dependency graph of the task. All tasks specified in this field must complete successfully before executing this task.\\\nThe key is `task_key`, and the value is the name assigned to the dependent task.\\\n\\\n[`run_if`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-run_if) string\\\n\\\nEnum: `ALL_SUCCESS | ALL_DONE | NONE_FAILED | AT_LEAST_ONE_SUCCESS | ALL_FAILED | AT_LEAST_ONE_FAILED`\\\n\\\nExample`\"ALL_SUCCESS\"`\\\n\\\nAn optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. When omitted, defaults to `ALL_SUCCESS`. See [jobs/create](https://docs.databricks.com/api/azure/workspace/jobs/create) for a list of possible values.\\\n\\\n[`notebook_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-notebook_task) object\\\n\\\nThe task runs a notebook when the `notebook_task` field is present.\\\n\\\n[`spark_jar_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-spark_jar_task) object\\\n\\\nThe task runs a JAR when the `spark_jar_task` field is present.\\\n\\\n[`spark_python_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-spark_python_task) object\\\n\\\nThe task runs a Python file when the `spark_python_task` field is present.\\\n\\\n[`spark_submit_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-spark_submit_task) object\\\n\\\n(Legacy) The task runs the spark-submit script when the `spark_submit_task` field is present. This task can run only on new clusters and is not compatible with serverless compute.\\\n\\\nIn the `new_cluster` specification, `libraries` and `spark_conf` are not supported. Instead, use `--jars` and `--py-files` to add Java and Python libraries and `--conf` to set the Spark configurations.\\\n\\\n`master`, `deploy-mode`, and `executor-cores` are automatically configured by Azure Databricks; you _cannot_ specify them in parameters.\\\n\\\nBy default, the Spark submit job uses all available memory (excluding reserved memory for Azure Databricks services). You can set `--driver-memory`, and `--executor-memory` to a smaller value to leave some room for off-heap usage.\\\n\\\nThe `--jars`, `--py-files`, `--files` arguments support DBFS and S3 paths.\\\n\\\n[`pipeline_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-pipeline_task) object\\\n\\\nThe task triggers a pipeline update when the `pipeline_task` field is present. Only pipelines configured to use triggered more are supported.\\\n\\\n[`python_wheel_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-python_wheel_task) object\\\n\\\nThe task runs a Python wheel when the `python_wheel_task` field is present.\\\n\\\n[`dbt_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-dbt_task) object\\\n\\\nThe task runs one or more dbt commands when the `dbt_task` field is present. The dbt task requires both Databricks SQL and the ability to use a serverless or a pro SQL warehouse.\\\n\\\n[`sql_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-sql_task) object\\\n\\\nThe task runs a SQL query or file, or it refreshes a SQL alert or a legacy SQL dashboard when the `sql_task` field is present.\\\n\\\n[`run_job_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-run_job_task) object\\\n\\\nThe task triggers another job when the `run_job_task` field is present.\\\n\\\n[`condition_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-condition_task) object\\\n\\\nThe task evaluates a condition that can be used to control the execution of other tasks when the `condition_task` field is present.\\\nThe condition task does not require a cluster to execute and does not support retries or notifications.\\\n\\\n[`for_each_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-for_each_task) object\\\n\\\nThe task executes a nested task for every input provided when the `for_each_task` field is present.\\\n\\\n[`clean_rooms_notebook_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-clean_rooms_notebook_task) object\\\n\\\nPublic preview\\\n\\\nThe task runs a [clean rooms](https://docs.databricks.com/en/clean-rooms/index.html) notebook\\\nwhen the `clean_rooms_notebook_task` field is present.\\\n\\\n[`existing_cluster_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-existing_cluster_id) string\\\n\\\nExample`\"0923-164208-meows279\"`\\\n\\\nIf existing\\_cluster\\_id, the ID of an existing cluster that is used for all runs.\\\nWhen running jobs or tasks on an existing cluster, you may need to manually restart\\\nthe cluster if it stops responding. We suggest running jobs and tasks on new clusters for\\\ngreater reliability\\\n\\\n[`new_cluster`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-new_cluster) object\\\n\\\nIf new\\_cluster, a description of a new cluster that is created for each run.\\\n\\\n[`job_cluster_key`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-job_cluster_key) string\\\n\\\n\\[ 1 .. 100 \\] characters ^\\[\\\\w\\\\-\\\\\\_\\]+$\\\n\\\nIf job\\_cluster\\_key, this task is executed reusing the cluster specified in `job.settings.job_clusters`.\\\n\\\n[`libraries`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-libraries) Array of object\\\n\\\nAn optional list of libraries to be installed on the cluster.\\\nThe default value is an empty list.\\\n\\\n[`timeout_seconds`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-timeout_seconds) int32\\\n\\\nDefault`0`\\\n\\\nExample`86400`\\\n\\\nAn optional timeout applied to each run of this job task. A value of `0` means no timeout.\\\n\\\n[`email_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-email_notifications) object\\\n\\\nDefault`{}`\\\n\\\nAn optional set of email addresses notified when the task run begins or completes. The default behavior is to not send any emails.\\\n\\\n[`notification_settings`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-notification_settings) object\\\n\\\nDefault`{}`\\\n\\\nOptional notification settings that are used when sending notifications to each of the `email_notifications` and `webhook_notifications` for this task run.\\\n\\\n[`webhook_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-webhook_notifications) object\\\n\\\nDefault`{}`\\\n\\\nA collection of system notification IDs to notify when the run begins or completes. The default behavior is to not send any system notifications. Task webhooks respect the task notification settings.\\\n\\\n[`environment_key`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-environment_key) string\\\n\\\n\\[ 1 .. 100 \\] characters ^\\[\\\\w\\\\-\\\\\\_\\]+$\\\n\\\nThe key that references an environment spec in a job. This field is required for Python script, Python wheel and dbt tasks when using serverless compute.\\\n\\\n[`state`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-state) object\\\n\\\nDeprecated\\\n\\\nDeprecated. Please use the `status` field instead.\\\n\\\n[`run_page_url`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-run_page_url) string\\\n\\\n[`start_time`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-start_time) int64\\\n\\\nExample`1625060460483`\\\n\\\nThe time at which this run was started in epoch milliseconds (milliseconds since 1/1/1970 UTC). This may not be the time when the job task starts executing, for example, if the job is scheduled to run on a new cluster, this is the time the cluster creation call is issued.\\\n\\\n[`setup_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-setup_duration) int64\\\n\\\nExample`0`\\\n\\\nThe time in milliseconds it took to set up the cluster. For runs that run on new clusters this is the cluster creation time, for runs that run on existing clusters this time should be very short. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `setup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.\\\n\\\n[`execution_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-execution_duration) int64\\\n\\\nExample`0`\\\n\\\nThe time in milliseconds it took to execute the commands in the JAR or notebook until they completed, failed, timed out, were cancelled, or encountered an unexpected error. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `execution_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.\\\n\\\n[`cleanup_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-cleanup_duration) int64\\\n\\\nExample`0`\\\n\\\nThe time in milliseconds it took to terminate the cluster and clean up any associated artifacts. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `cleanup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.\\\n\\\n[`end_time`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-end_time) int64\\\n\\\nExample`1625060863413`\\\n\\\nThe time at which this run ended in epoch milliseconds (milliseconds since 1/1/1970 UTC). This field is set to 0 if the job is still running.\\\n\\\n[`run_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-run_duration) int64\\\n\\\nExample`110183`\\\n\\\nThe time in milliseconds it took the job run and all of its repairs to finish.\\\n\\\n[`queue_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-queue_duration) int64\\\n\\\nExample`1625060863413`\\\n\\\nThe time in milliseconds that the run has spent in the queue.\\\n\\\n[`cluster_instance`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-cluster_instance) object\\\n\\\nThe cluster used for this run. If the run is specified to use a new cluster, this field is set once the Jobs service has requested a cluster for the run.\\\n\\\n[`attempt_number`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-attempt_number) int32\\\n\\\nExample`0`\\\n\\\nThe sequence number of this run attempt for a triggered job run. The initial attempt of a run has an attempt\\_number of 0. If the initial run attempt fails, and the job has a retry policy (`max_retries` \\> 0), subsequent runs are created with an `original_attempt_run_id` of the original attempt’s ID and an incrementing `attempt_number`. Runs are retried only until they succeed, and the maximum `attempt_number` is the same as the `max_retries` value for the job.\\\n\\\n[`git_source`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-git_source) object\\\n\\\nExample\\\n\\\nAn optional specification for a remote Git repository containing the source code used by tasks. Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks. If `git_source` is set, these tasks retrieve the file from the remote repository by default. However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task. Note: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are used, `git_source` must be defined on the job.\\\n\\\n[`resolved_values`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-resolved_values) object\\\n\\\nParameter values including resolved references\\\n\\\n[`status`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#tasks-status) object\\\n\\\nThe current status of the run\\\n\\\n\\]\n\n[`description`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#description) string\n\nDescription of the run\n\n[`attempt_number`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#attempt_number) int32\n\nExample`0`\n\nThe sequence number of this run attempt for a triggered job run. The initial attempt of a run has an attempt\\_number of 0. If the initial run attempt fails, and the job has a retry policy (`max_retries` \\> 0), subsequent runs are created with an `original_attempt_run_id` of the original attempt’s ID and an incrementing `attempt_number`. Runs are retried only until they succeed, and the maximum `attempt_number` is the same as the `max_retries` value for the job.\n\n[`job_clusters`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#job_clusters) Array of object\n\n<= 100 items\n\nExample\n\nA list of job cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings.\nIf more than 100 job clusters are available, you can paginate through them using [jobs/getrun](https://docs.databricks.com/api/azure/workspace/jobs/getrun).\n\nArray \\[\\\n\\\n[`job_cluster_key`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#job_clusters-job_cluster_key) requiredstring\\\n\\\n\\[ 1 .. 100 \\] characters ^\\[\\\\w\\\\-\\\\\\_\\]+$\\\n\\\nExample`\"auto_scaling_cluster\"`\\\n\\\nA unique name for the job cluster. This field is required and must be unique within the job.\\\n`JobTaskSettings` may refer to this field to determine which cluster to launch for the task execution.\\\n\\\n[`new_cluster`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#job_clusters-new_cluster) object\\\n\\\nIf new\\_cluster, a description of a cluster that is created for each task.\\\n\\\n\\]\n\n[`git_source`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#git_source) object\n\nExample\n\nAn optional specification for a remote Git repository containing the source code used by tasks. Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.\n\nIf `git_source` is set, these tasks retrieve the file from the remote repository by default. However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.\n\nNote: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are used, `git_source` must be defined on the job.\n\n[`git_url`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#git_source-git_url) requiredstring\n\n<= 300 characters\n\nExample`\"https://github.com/databricks/databricks-cli\"`\n\nURL of the repository to be cloned by this job.\n\n[`git_provider`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#git_source-git_provider) requiredstring\n\nEnum: `gitHub | bitbucketCloud | azureDevOpsServices | gitHubEnterprise | bitbucketServer | gitLab | gitLabEnterpriseEdition | awsCodeCommit`\n\nUnique identifier of the service used to host the Git repository. The value is case insensitive.\n\n[`git_branch`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#git_source-git_branch) string\n\n<= 255 characters\n\nExample`\"main\"`\n\nName of the branch to be checked out and used by this job. This field cannot be specified in conjunction with git\\_tag or git\\_commit.\n\n[`git_tag`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#git_source-git_tag) string\n\n<= 255 characters\n\nExample`\"release-1.0.0\"`\n\nName of the tag to be checked out and used by this job. This field cannot be specified in conjunction with git\\_branch or git\\_commit.\n\n[`git_commit`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#git_source-git_commit) string\n\n<= 64 characters\n\nExample`\"e0056d01\"`\n\nCommit to be checked out and used by this job. This field cannot be specified in conjunction with git\\_branch or git\\_tag.\n\n[`git_snapshot`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#git_source-git_snapshot) object\n\nRead-only state of the remote repository at the time the job was run. This field is only included on job runs.\n\n[`repair_history`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#repair_history) Array of object\n\nThe repair history of the run.\n\nArray \\[\\\n\\\n[`type`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#repair_history-type) string\\\n\\\nEnum: `ORIGINAL | REPAIR`\\\n\\\nThe repair history item type. Indicates whether a run is the original run or a repair run.\\\n\\\n[`start_time`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#repair_history-start_time) int64\\\n\\\nExample`1625060460483`\\\n\\\nThe start time of the (repaired) run.\\\n\\\n[`end_time`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#repair_history-end_time) int64\\\n\\\nExample`1625060863413`\\\n\\\nThe end time of the (repaired) run.\\\n\\\n[`state`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#repair_history-state) object\\\n\\\nDeprecated\\\n\\\nDeprecated. Please use the `status` field instead.\\\n\\\n[`id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#repair_history-id) int64\\\n\\\nExample`734650698524280`\\\n\\\nThe ID of the repair. Only returned for the items that represent a repair in `repair_history`.\\\n\\\n[`task_run_ids`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#repair_history-task_run_ids) Array of int64\\\n\\\nExample\\\n\\\nThe run IDs of the task runs that ran as part of this repair history item.\\\n\\\n[`status`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#repair_history-status) object\\\n\\\nThe current status of the run\\\n\\\n\\]\n\n[`status`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#status) object\n\nThe current status of the run\n\n[`state`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#status-state) string\n\nEnum: `BLOCKED | PENDING | QUEUED | RUNNING | TERMINATING | TERMINATED`\n\nThe current state of the run.\n\n[`termination_details`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#status-termination_details) object\n\nIf the run is in a TERMINATING or TERMINATED state, details about the reason for terminating the run.\n\n[`queue_details`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#status-queue_details) object\n\nIf the run was queued, details about the reason for queuing the run.\n\n[`job_run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun#job_run_id) int64\n\nID of the job run that this run belongs to.\nFor legacy and single-task job runs the field is populated with the job run ID.\nFor task runs, the field is populated with the ID of the job run that the task run belongs to.\n\nThis method might return the following HTTP codes: 400, 401, 403, 429, 500\n\nError responses are returned in the following format:\n\n```\n{\n  \"error_code\": \"Error code\",\n  \"message\": \"Human-readable error message.\"\n}\n```\n\n# Possible error codes:\n\nHTTP code\n\nerror\\_code\n\nDescription\n\n400\n\nINVALID\\_PARAMETER\\_VALUE\n\nSupplied value for a parameter was invalid.\n\n401\n\nUNAUTHORIZED\n\nThe request does not have valid authentication credentials for the operation.\n\n403\n\nPERMISSION\\_DENIED\n\nCaller does not have permission to execute the specified operation.\n\n429\n\nREQUEST\\_LIMIT\\_EXCEEDED\n\nRequest is rejected due to throttling.\n\n500\n\nINTERNAL\\_SERVER\\_ERROR\n\nInternal error.\n\n# Response samples\n\n200\n\n```\n{\n  \"job_id\": 11223344,\n  \"run_id\": 455644833,\n  \"creator_user_name\": \"user.name@databricks.com\",\n  \"number_in_job\": 455644833,\n  \"original_attempt_run_id\": 455644833,\n  \"state\": {\n    \"life_cycle_state\": \"PENDING\",\n    \"result_state\": \"SUCCESS\",\n    \"state_message\": \"string\",\n    \"user_cancelled_or_timedout\": false,\n    \"queue_reason\": \"Queued due to reaching maximum concurrent runs of 1.\"\n  },\n  \"schedule\": {\n    \"quartz_cron_expression\": \"20 30 * * * ?\",\n    \"timezone_id\": \"Europe/London\",\n    \"pause_status\": \"UNPAUSED\"\n  },\n  \"cluster_spec\": {\n    \"existing_cluster_id\": \"0923-164208-meows279\",\n    \"new_cluster\": {\n      \"num_workers\": 0,\n      \"autoscale\": {\n        \"min_workers\": 0,\n        \"max_workers\": 0\n      },\n      \"kind\": \"CLASSIC_PREVIEW\",\n      \"cluster_name\": \"string\",\n      \"spark_version\": \"string\",\n      \"use_ml_runtime\": true,\n      \"is_single_node\": true,\n      \"spark_conf\": {\n        \"property1\": \"string\",\n        \"property2\": \"string\"\n      },\n      \"azure_attributes\": {\n        \"log_analytics_info\": {\n          \"log_analytics_workspace_id\": \"string\",\n          \"log_analytics_primary_key\": \"string\"\n        },\n        \"first_on_demand\": \"1\",\n        \"availability\": \"SPOT_AZURE\",\n        \"spot_bid_max_price\": \"-1.0\"\n      },\n      \"node_type_id\": \"string\",\n      \"driver_node_type_id\": \"string\",\n      \"ssh_public_keys\": [\\\n        \"string\"\\\n      ],\n      \"custom_tags\": {\n        \"property1\": \"string\",\n        \"property2\": \"string\"\n      },\n      \"cluster_log_conf\": {\n        \"dbfs\": {\n          \"destination\": \"string\"\n        }\n      },\n      \"init_scripts\": [\\\n        {\\\n          \"workspace\": {\\\n            \"destination\": \"string\"\\\n          },\\\n          \"volumes\": {\\\n            \"destination\": \"string\"\\\n          },\\\n          \"file\": {\\\n            \"destination\": \"string\"\\\n          },\\\n          \"dbfs\": {\\\n            \"destination\": \"string\"\\\n          },\\\n          \"abfss\": {\\\n            \"destination\": \"string\"\\\n          },\\\n          \"gcs\": {\\\n            \"destination\": \"string\"\\\n          }\\\n        }\\\n      ],\n      \"spark_env_vars\": {\n        \"property1\": \"string\",\n        \"property2\": \"string\"\n      },\n      \"autotermination_minutes\": 0,\n      \"enable_elastic_disk\": true,\n      \"instance_pool_id\": \"string\",\n      \"policy_id\": \"string\",\n      \"enable_local_disk_encryption\": true,\n      \"driver_instance_pool_id\": \"string\",\n      \"workload_type\": {\n        \"clients\": {\n          \"notebooks\": \"true\",\n          \"jobs\": \"true\"\n        }\n      },\n      \"runtime_engine\": \"NULL\",\n      \"docker_image\": {\n        \"url\": \"string\",\n        \"basic_auth\": {\n          \"username\": \"string\",\n          \"password\": \"string\"\n        }\n      },\n      \"data_security_mode\": \"DATA_SECURITY_MODE_AUTO\",\n      \"single_user_name\": \"string\",\n      \"apply_policy_default_values\": false\n    },\n    \"job_cluster_key\": \"string\",\n    \"libraries\": [\\\n      {\\\n        \"jar\": \"string\",\\\n        \"egg\": \"string\",\\\n        \"pypi\": {\\\n          \"package\": \"string\",\\\n          \"repo\": \"string\"\\\n        },\\\n        \"maven\": {\\\n          \"coordinates\": \"string\",\\\n          \"repo\": \"string\",\\\n          \"exclusions\": [\\\n            \"string\"\\\n          ]\\\n        },\\\n        \"cran\": {\\\n          \"package\": \"string\",\\\n          \"repo\": \"string\"\\\n        },\\\n        \"whl\": \"string\",\\\n        \"requirements\": \"string\"\\\n      }\\\n    ]\n  },\n  \"cluster_instance\": {\n    \"cluster_id\": \"0923-164208-meows279\",\n    \"spark_context_id\": \"string\"\n  },\n  \"job_parameters\": [\\\n    {\\\n      \"default\": \"users\",\\\n      \"name\": \"table\",\\\n      \"value\": \"customers\"\\\n    }\\\n  ],\n  \"overriding_parameters\": {\n    \"pipeline_params\": {\n      \"full_refresh\": false\n    },\n    \"jar_params\": [\\\n      \"john\",\\\n      \"doe\",\\\n      \"35\"\\\n    ],\n    \"notebook_params\": {\n      \"age\": \"35\",\n      \"name\": \"john doe\"\n    },\n    \"python_params\": [\\\n      \"john doe\",\\\n      \"35\"\\\n    ],\n    \"spark_submit_params\": [\\\n      \"--class\",\\\n      \"org.apache.spark.examples.SparkPi\"\\\n    ],\n    \"python_named_params\": {\n      \"data\": \"dbfs:/path/to/data.json\",\n      \"name\": \"task\"\n    },\n    \"sql_params\": {\n      \"age\": \"35\",\n      \"name\": \"john doe\"\n    },\n    \"dbt_commands\": [\\\n      \"dbt deps\",\\\n      \"dbt seed\",\\\n      \"dbt run\"\\\n    ]\n  },\n  \"start_time\": 1625060460483,\n  \"setup_duration\": 0,\n  \"execution_duration\": 0,\n  \"cleanup_duration\": 0,\n  \"end_time\": 1625060863413,\n  \"run_duration\": 110183,\n  \"queue_duration\": 1625060863413,\n  \"trigger\": \"PERIODIC\",\n  \"trigger_info\": {\n    \"run_id\": 0\n  },\n  \"run_name\": \"A multitask job run\",\n  \"run_page_url\": \"https://my-workspace.cloud.databricks.com/#job/11223344/run/123\",\n  \"run_type\": \"JOB_RUN\",\n  \"tasks\": [\\\n    {\\\n      \"setup_duration\": 0,\\\n      \"start_time\": 1629989929660,\\\n      \"task_key\": \"Orders_Ingest\",\\\n      \"state\": {\\\n        \"life_cycle_state\": \"INTERNAL_ERROR\",\\\n        \"result_state\": \"FAILED\",\\\n        \"state_message\": \"Library installation failed for library due to user error. Error messages:\\n'Manage' permissions are required to install libraries on a cluster\",\\\n        \"user_cancelled_or_timedout\": false\\\n      },\\\n      \"description\": \"Ingests order data\",\\\n      \"job_cluster_key\": \"auto_scaling_cluster\",\\\n      \"end_time\": 1629989930171,\\\n      \"run_page_url\": \"https://my-workspace.cloud.databricks.com/#job/39832/run/20\",\\\n      \"run_id\": 2112892,\\\n      \"cluster_instance\": {\\\n        \"cluster_id\": \"0923-164208-meows279\",\\\n        \"spark_context_id\": \"4348585301701786933\"\\\n      },\\\n      \"spark_jar_task\": {\\\n        \"main_class_name\": \"com.databricks.OrdersIngest\"\\\n      },\\\n      \"libraries\": [\\\n        {\\\n          \"jar\": \"dbfs:/mnt/databricks/OrderIngest.jar\"\\\n        }\\\n      ],\\\n      \"attempt_number\": 0,\\\n      \"cleanup_duration\": 0,\\\n      \"execution_duration\": 0,\\\n      \"run_if\": \"ALL_SUCCESS\"\\\n    },\\\n    {\\\n      \"setup_duration\": 0,\\\n      \"start_time\": 0,\\\n      \"task_key\": \"Match\",\\\n      \"state\": {\\\n        \"life_cycle_state\": \"SKIPPED\",\\\n        \"state_message\": \"An upstream task failed.\",\\\n        \"user_cancelled_or_timedout\": false\\\n      },\\\n      \"description\": \"Matches orders with user sessions\",\\\n      \"notebook_task\": {\\\n        \"notebook_path\": \"/Users/user.name@databricks.com/Match\",\\\n        \"source\": \"WORKSPACE\"\\\n      },\\\n      \"end_time\": 1629989930238,\\\n      \"depends_on\": [\\\n        {\\\n          \"task_key\": \"Orders_Ingest\"\\\n        },\\\n        {\\\n          \"task_key\": \"Sessionize\"\\\n        }\\\n      ],\\\n      \"run_page_url\": \"https://my-workspace.cloud.databricks.com/#job/39832/run/21\",\\\n      \"new_cluster\": {\\\n        \"autoscale\": {\\\n          \"max_workers\": 16,\\\n          \"min_workers\": 2\\\n        },\\\n        \"node_type_id\": null,\\\n        \"spark_conf\": {\\\n          \"spark.speculation\": true\\\n        },\\\n        \"spark_version\": \"7.3.x-scala2.12\"\\\n      },\\\n      \"run_id\": 2112897,\\\n      \"cluster_instance\": {\\\n        \"cluster_id\": \"0923-164208-meows279\"\\\n      },\\\n      \"attempt_number\": 0,\\\n      \"cleanup_duration\": 0,\\\n      \"execution_duration\": 0,\\\n      \"run_if\": \"ALL_SUCCESS\"\\\n    },\\\n    {\\\n      \"setup_duration\": 0,\\\n      \"start_time\": 1629989929668,\\\n      \"task_key\": \"Sessionize\",\\\n      \"state\": {\\\n        \"life_cycle_state\": \"INTERNAL_ERROR\",\\\n        \"result_state\": \"FAILED\",\\\n        \"state_message\": \"Library installation failed for library due to user error. Error messages:\\n'Manage' permissions are required to install libraries on a cluster\",\\\n        \"user_cancelled_or_timedout\": false\\\n      },\\\n      \"description\": \"Extracts session data from events\",\\\n      \"end_time\": 1629989930144,\\\n      \"run_page_url\": \"https://my-workspace.cloud.databricks.com/#job/39832/run/22\",\\\n      \"run_id\": 2112902,\\\n      \"cluster_instance\": {\\\n        \"cluster_id\": \"0923-164208-meows279\",\\\n        \"spark_context_id\": \"4348585301701786933\"\\\n      },\\\n      \"spark_jar_task\": {\\\n        \"main_class_name\": \"com.databricks.Sessionize\"\\\n      },\\\n      \"libraries\": [\\\n        {\\\n          \"jar\": \"dbfs:/mnt/databricks/Sessionize.jar\"\\\n        }\\\n      ],\\\n      \"attempt_number\": 0,\\\n      \"existing_cluster_id\": \"0923-164208-meows279\",\\\n      \"cleanup_duration\": 0,\\\n      \"execution_duration\": 0,\\\n      \"run_if\": \"ALL_SUCCESS\"\\\n    }\\\n  ],\n  \"description\": \"string\",\n  \"attempt_number\": 0,\n  \"job_clusters\": [\\\n    {\\\n      \"job_cluster_key\": \"auto_scaling_cluster\",\\\n      \"new_cluster\": {\\\n        \"autoscale\": {\\\n          \"max_workers\": 16,\\\n          \"min_workers\": 2\\\n        },\\\n        \"node_type_id\": null,\\\n        \"spark_conf\": {\\\n          \"spark.speculation\": true\\\n        },\\\n        \"spark_version\": \"7.3.x-scala2.12\"\\\n      }\\\n    }\\\n  ],\n  \"git_source\": {\n    \"git_branch\": \"main\",\n    \"git_provider\": \"gitHub\",\n    \"git_url\": \"https://github.com/databricks/databricks-cli\"\n  },\n  \"repair_history\": [\\\n    {\\\n      \"type\": \"ORIGINAL\",\\\n      \"start_time\": 1625060460483,\\\n      \"end_time\": 1625060863413,\\\n      \"state\": {\\\n        \"life_cycle_state\": \"PENDING\",\\\n        \"result_state\": \"SUCCESS\",\\\n        \"state_message\": \"string\",\\\n        \"user_cancelled_or_timedout\": false,\\\n        \"queue_reason\": \"Queued due to reaching maximum concurrent runs of 1.\"\\\n      },\\\n      \"id\": 734650698524280,\\\n      \"task_run_ids\": [\\\n        1106460542112844,\\\n        988297789683452\\\n      ],\\\n      \"status\": {\\\n        \"state\": \"BLOCKED\",\\\n        \"termination_details\": {\\\n          \"code\": \"SUCCESS\",\\\n          \"type\": \"SUCCESS\",\\\n          \"message\": \"string\"\\\n        },\\\n        \"queue_details\": {\\\n          \"code\": \"ACTIVE_RUNS_LIMIT_REACHED\",\\\n          \"message\": \"string\"\\\n        }\\\n      }\\\n    }\\\n  ],\n  \"status\": {\n    \"state\": \"BLOCKED\",\n    \"termination_details\": {\n      \"code\": \"SUCCESS\",\n      \"type\": \"SUCCESS\",\n      \"message\": \"string\"\n    },\n    \"queue_details\": {\n      \"code\": \"ACTIVE_RUNS_LIMIT_REACHED\",\n      \"message\": \"string\"\n    }\n  },\n  \"job_run_id\": 0\n}\n```"
  },
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## Jobs (2.1)\n\nThe Jobs API allows you to create, edit, and delete jobs.\n\nYou can use an Azure Databricks job to run a data processing or data analysis task in an Azure Databricks\ncluster with scalable resources. Your job can consist of a single task or can be a large,\nmulti-task workflow with complex dependencies. Azure Databricks manages the task orchestration,\ncluster management, monitoring, and error reporting for all of your jobs. You can run your jobs\nimmediately or periodically through an easy-to-use scheduling system. You can implement\njob tasks using notebooks, JARS, Delta Live Tables pipelines, or Python, Scala, Spark\nsubmit, and Java applications.\n\nYou should never hard code secrets or store them in plain text. Use the [Secrets CLI](https://docs.databricks.com/dev-tools/cli/secrets-cli.html)\nto manage secrets in the [Databricks CLI](https://docs.databricks.com/dev-tools/cli/index.html).\nUse the [Secrets utility](https://docs.databricks.com/dev-tools/databricks-utils.html#dbutils-secrets) to reference secrets in notebooks and jobs."
  },
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## List jobs\n\n`\nGET/api/2.1/jobs/list`\n\nRetrieves a list of jobs.\n\n### Query parameters\n\n[`offset`](https://docs.databricks.com/api/azure/workspace/jobs_21/list#offset) int32\n\n<= 1000\n\nDeprecated\n\nDefault`0`\n\nThe offset of the first job to return, relative to the most recently created job.\nDeprecated since June 2023. Use `page_token` to iterate through the pages instead.\n\n[`limit`](https://docs.databricks.com/api/azure/workspace/jobs_21/list#limit) int32\n\n\\[ 1 .. 100 \\]\n\nDefault`20`\n\nExample`limit=25`\n\nThe number of jobs to return. This value must be greater than 0 and less or equal to 100. The default value is 20.\n\n[`expand_tasks`](https://docs.databricks.com/api/azure/workspace/jobs_21/list#expand_tasks) boolean\n\nDefault`false`\n\nWhether to include task and cluster details in the response.\n\n[`name`](https://docs.databricks.com/api/azure/workspace/jobs_21/list#name) string\n\nExample`name=A%20multitask%20job`\n\nA filter on the list based on the exact (case insensitive) job name.\n\n[`page_token`](https://docs.databricks.com/api/azure/workspace/jobs_21/list#page_token) string\n\nExample`page_token=CAEomPSriYcxMPWM_IiIxvEB`\n\nUse `next_page_token` or `prev_page_token` returned from the previous request to list the next or previous page of jobs respectively.\n\n### Responses\n\n**200** Request completed successfully.\n\nRequest completed successfully.\n\n[`jobs`](https://docs.databricks.com/api/azure/workspace/jobs_21/list#jobs) Array of object\n\nThe list of jobs. Only included in the response if there are jobs to list.\n\nArray \\[\\\n\\\n[`job_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/list#jobs-job_id) int64\\\n\\\nExample`11223344`\\\n\\\nThe canonical identifier for this job.\\\n\\\n[`creator_user_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/list#jobs-creator_user_name) string\\\n\\\nExample`\"user.name@databricks.com\"`\\\n\\\nThe creator user name. This field won’t be included in the response if the user has already been deleted.\\\n\\\n[`settings`](https://docs.databricks.com/api/azure/workspace/jobs_21/list#jobs-settings) object\\\n\\\nSettings for this job and all of its runs. These settings can be updated using the `resetJob` method.\\\n\\\n[`created_time`](https://docs.databricks.com/api/azure/workspace/jobs_21/list#jobs-created_time) int64\\\n\\\nExample`1601370337343`\\\n\\\nThe time at which this job was created in epoch milliseconds (milliseconds since 1/1/1970 UTC).\\\n\\\n\\]\n\n[`has_more`](https://docs.databricks.com/api/azure/workspace/jobs_21/list#has_more) boolean\n\nExample`false`\n\nIf true, additional jobs matching the provided filter are available for listing.\n\n[`next_page_token`](https://docs.databricks.com/api/azure/workspace/jobs_21/list#next_page_token) string\n\nExample`\"CAEomPuciYcxMKbM9JvMlwU=\"`\n\nA token that can be used to list the next page of jobs (if applicable).\n\n[`prev_page_token`](https://docs.databricks.com/api/azure/workspace/jobs_21/list#prev_page_token) string\n\nExample`\"CAAos-uriYcxMN7_rt_v7B4=\"`\n\nA token that can be used to list the previous page of jobs (if applicable).\n\nThis method might return the following HTTP codes: 400, 401, 429, 500\n\nError responses are returned in the following format:\n\n```\n{\n  \"error_code\": \"Error code\",\n  \"message\": \"Human-readable error message.\"\n}\n```\n\n# Possible error codes:\n\nHTTP code\n\nerror\\_code\n\nDescription\n\n400\n\nQUOTA\\_EXCEEDED\n\n401\n\nUNAUTHORIZED\n\nThe request does not have valid authentication credentials for the operation.\n\n429\n\nREQUEST\\_LIMIT\\_EXCEEDED\n\nRequest is rejected due to throttling.\n\n500\n\nINTERNAL\\_SERVER\\_ERROR\n\nInternal error.\n\n# Response samples\n\n200\n\n```\n{\n  \"jobs\": [\\\n    {\\\n      \"job_id\": 11223344,\\\n      \"creator_user_name\": \"user.name@databricks.com\",\\\n      \"settings\": {\\\n        \"name\": \"A multitask job\",\\\n        \"description\": \"This job contain multiple tasks that are required to produce the weekly shark sightings report.\",\\\n        \"email_notifications\": {\\\n          \"on_start\": [\\\n            \"user.name@databricks.com\"\\\n          ],\\\n          \"on_success\": [\\\n            \"user.name@databricks.com\"\\\n          ],\\\n          \"on_failure\": [\\\n            \"user.name@databricks.com\"\\\n          ],\\\n          \"on_duration_warning_threshold_exceeded\": [\\\n            \"user.name@databricks.com\"\\\n          ],\\\n          \"on_streaming_backlog_exceeded\": [\\\n            \"user.name@databricks.com\"\\\n          ],\\\n          \"no_alert_for_skipped_runs\": false\\\n        },\\\n        \"webhook_notifications\": {\\\n          \"on_start\": [\\\n            [\\\n              {\\\n                \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n              }\\\n            ]\\\n          ],\\\n          \"on_success\": [\\\n            [\\\n              {\\\n                \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n              }\\\n            ]\\\n          ],\\\n          \"on_failure\": [\\\n            [\\\n              {\\\n                \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n              }\\\n            ]\\\n          ],\\\n          \"on_duration_warning_threshold_exceeded\": [\\\n            [\\\n              {\\\n                \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n              }\\\n            ]\\\n          ],\\\n          \"on_streaming_backlog_exceeded\": [\\\n            [\\\n              {\\\n                \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n              }\\\n            ]\\\n          ]\\\n        },\\\n        \"notification_settings\": {\\\n          \"no_alert_for_skipped_runs\": false,\\\n          \"no_alert_for_canceled_runs\": false,\\\n          \"alert_on_last_attempt\": false\\\n        },\\\n        \"timeout_seconds\": 86400,\\\n        \"health\": {\\\n          \"rules\": [\\\n            {\\\n              \"metric\": \"RUN_DURATION_SECONDS\",\\\n              \"op\": \"GREATER_THAN\",\\\n              \"value\": 10\\\n            }\\\n          ]\\\n        },\\\n        \"schedule\": {\\\n          \"quartz_cron_expression\": \"20 30 * * * ?\",\\\n          \"timezone_id\": \"Europe/London\",\\\n          \"pause_status\": \"UNPAUSED\"\\\n        },\\\n        \"trigger\": {\\\n          \"pause_status\": \"UNPAUSED\",\\\n          \"file_arrival\": {\\\n            \"url\": \"string\",\\\n            \"min_time_between_triggers_seconds\": 0,\\\n            \"wait_after_last_change_seconds\": 0\\\n          },\\\n          \"periodic\": {\\\n            \"interval\": 0,\\\n            \"unit\": \"HOURS\"\\\n          }\\\n        },\\\n        \"continuous\": {\\\n          \"pause_status\": \"UNPAUSED\"\\\n        },\\\n        \"max_concurrent_runs\": 10,\\\n        \"tasks\": [\\\n          {\\\n            \"max_retries\": 3,\\\n            \"task_key\": \"Sessionize\",\\\n            \"description\": \"Extracts session data from events\",\\\n            \"min_retry_interval_millis\": 2000,\\\n            \"depends_on\": [],\\\n            \"timeout_seconds\": 86400,\\\n            \"spark_jar_task\": {\\\n              \"main_class_name\": \"com.databricks.Sessionize\",\\\n              \"parameters\": [\\\n                \"--data\",\\\n                \"dbfs:/path/to/data.json\"\\\n              ]\\\n            },\\\n            \"libraries\": [\\\n              {\\\n                \"jar\": \"dbfs:/mnt/databricks/Sessionize.jar\"\\\n              }\\\n            ],\\\n            \"retry_on_timeout\": false,\\\n            \"existing_cluster_id\": \"0923-164208-meows279\"\\\n          },\\\n          {\\\n            \"max_retries\": 3,\\\n            \"task_key\": \"Orders_Ingest\",\\\n            \"description\": \"Ingests order data\",\\\n            \"job_cluster_key\": \"auto_scaling_cluster\",\\\n            \"min_retry_interval_millis\": 2000,\\\n            \"depends_on\": [],\\\n            \"timeout_seconds\": 86400,\\\n            \"spark_jar_task\": {\\\n              \"main_class_name\": \"com.databricks.OrdersIngest\",\\\n              \"parameters\": [\\\n                \"--data\",\\\n                \"dbfs:/path/to/order-data.json\"\\\n              ]\\\n            },\\\n            \"libraries\": [\\\n              {\\\n                \"jar\": \"dbfs:/mnt/databricks/OrderIngest.jar\"\\\n              }\\\n            ],\\\n            \"retry_on_timeout\": false\\\n          },\\\n          {\\\n            \"max_retries\": 3,\\\n            \"task_key\": \"Match\",\\\n            \"description\": \"Matches orders with user sessions\",\\\n            \"notebook_task\": {\\\n              \"base_parameters\": {\\\n                \"age\": \"35\",\\\n                \"name\": \"John Doe\"\\\n              },\\\n              \"notebook_path\": \"/Users/user.name@databricks.com/Match\"\\\n            },\\\n            \"min_retry_interval_millis\": 2000,\\\n            \"depends_on\": [\\\n              {\\\n                \"task_key\": \"Orders_Ingest\"\\\n              },\\\n              {\\\n                \"task_key\": \"Sessionize\"\\\n              }\\\n            ],\\\n            \"new_cluster\": {\\\n              \"autoscale\": {\\\n                \"max_workers\": 16,\\\n                \"min_workers\": 2\\\n              },\\\n              \"node_type_id\": null,\\\n              \"spark_conf\": {\\\n                \"spark.speculation\": true\\\n              },\\\n              \"spark_version\": \"7.3.x-scala2.12\"\\\n            },\\\n            \"timeout_seconds\": 86400,\\\n            \"retry_on_timeout\": false,\\\n            \"run_if\": \"ALL_SUCCESS\"\\\n          }\\\n        ],\\\n        \"job_clusters\": [\\\n          {\\\n            \"job_cluster_key\": \"auto_scaling_cluster\",\\\n            \"new_cluster\": {\\\n              \"autoscale\": {\\\n                \"max_workers\": 16,\\\n                \"min_workers\": 2\\\n              },\\\n              \"node_type_id\": null,\\\n              \"spark_conf\": {\\\n                \"spark.speculation\": true\\\n              },\\\n              \"spark_version\": \"7.3.x-scala2.12\"\\\n            }\\\n          }\\\n        ],\\\n        \"git_source\": {\\\n          \"git_branch\": \"main\",\\\n          \"git_provider\": \"gitHub\",\\\n          \"git_url\": \"https://github.com/databricks/databricks-cli\"\\\n        },\\\n        \"tags\": {\\\n          \"cost-center\": \"engineering\",\\\n          \"team\": \"jobs\"\\\n        },\\\n        \"format\": \"SINGLE_TASK\",\\\n        \"queue\": {\\\n          \"enabled\": true\\\n        },\\\n        \"parameters\": [\\\n          {\\\n            \"default\": \"users\",\\\n            \"name\": \"table\"\\\n          }\\\n        ],\\\n        \"run_as\": {\\\n          \"user_name\": \"user@databricks.com\",\\\n          \"service_principal_name\": \"692bc6d0-ffa3-11ed-be56-0242ac120002\"\\\n        },\\\n        \"edit_mode\": \"UI_LOCKED\",\\\n        \"deployment\": {\\\n          \"kind\": \"BUNDLE\",\\\n          \"metadata_file_path\": \"string\"\\\n        },\\\n        \"environments\": [\\\n          {\\\n            \"environment_key\": \"string\",\\\n            \"spec\": {\\\n              \"client\": \"1\",\\\n              \"dependencies\": [\\\n                \"string\"\\\n              ]\\\n            }\\\n          }\\\n        ]\\\n      },\\\n      \"created_time\": 1601370337343\\\n    }\\\n  ],\n  \"has_more\": false,\n  \"next_page_token\": \"CAEomPuciYcxMKbM9JvMlwU=\",\n  \"prev_page_token\": \"CAAos-uriYcxMN7_rt_v7B4=\"\n}\n```"
  },
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## Create and trigger a one-time run\n\n`\nPOST/api/2.1/jobs/runs/submit`\n\nSubmit a one-time run. This endpoint allows you to submit a workload directly\nwithout creating a job. Runs submitted using this endpoint don’t display in\nthe UI. Use the `jobs/runs/get` API to check the run state after the job is\nsubmitted.\n\n### Request body\n\n[`run_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#run_name) string\n\nDefault`\"Untitled\"`\n\nExample`\"A multitask job run\"`\n\nAn optional name for the run. The default value is `Untitled`.\n\n[`timeout_seconds`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#timeout_seconds) int32\n\nDefault`0`\n\nExample`86400`\n\nAn optional timeout applied to each run of this job. A value of `0` means no timeout.\n\n[`health`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#health) object\n\nAn optional set of health rules that can be defined for this job.\n\n[`rules`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#health-rules) Array of object\n\n[`idempotency_token`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#idempotency_token) string\n\nExample`\"8f018174-4792-40d5-bcbc-3e6a527352c8\"`\n\nAn optional token that can be used to guarantee the idempotency of job run requests. If a run with the provided token already exists,\nthe request does not create a new run but returns the ID of the existing run instead. If a run with the provided token is deleted,\nan error is returned.\n\nIf you specify the idempotency token, upon failure you can retry until the request succeeds. Azure Databricks guarantees that exactly\none run is launched with that idempotency token.\n\nThis token must have at most 64 characters.\n\nFor more information, see [How to ensure idempotency for jobs](https://kb.databricks.com/jobs/jobs-idempotency.html).\n\n[`tasks`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks) Array of object\n\n<= 100 items\n\nExample\n\nArray \\[\\\n\\\n[`task_key`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-task_key) requiredstring\\\n\\\n\\[ 1 .. 100 \\] characters ^\\[\\\\w\\\\-\\\\\\_\\]+$\\\n\\\nExample`\"Task_Key\"`\\\n\\\nA unique name for the task. This field is used to refer to this task from other tasks.\\\nThis field is required and must be unique within its parent job.\\\nOn Update or Reset, this field is used to reference the tasks to be updated or reset.\\\n\\\n[`description`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-description) string\\\n\\\n<= 1000 characters\\\n\\\nExample`\"This is the description for this task.\"`\\\n\\\nAn optional description for this task.\\\n\\\n[`depends_on`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-depends_on) Array of object\\\n\\\nExample\\\n\\\nAn optional array of objects specifying the dependency graph of the task. All tasks specified in this field must complete successfully before executing this task.\\\nThe key is `task_key`, and the value is the name assigned to the dependent task.\\\n\\\n[`run_if`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-run_if) string\\\n\\\nEnum: `ALL_SUCCESS | ALL_DONE | NONE_FAILED | AT_LEAST_ONE_SUCCESS | ALL_FAILED | AT_LEAST_ONE_FAILED`\\\n\\\nExample`\"ALL_SUCCESS\"`\\\n\\\nAn optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. When omitted, defaults to `ALL_SUCCESS`. See [jobs/create](https://docs.databricks.com/api/azure/workspace/jobs/create) for a list of possible values.\\\n\\\n[`notebook_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-notebook_task) object\\\n\\\nThe task runs a notebook when the `notebook_task` field is present.\\\n\\\n[`spark_jar_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-spark_jar_task) object\\\n\\\nThe task runs a JAR when the `spark_jar_task` field is present.\\\n\\\n[`spark_python_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-spark_python_task) object\\\n\\\nThe task runs a Python file when the `spark_python_task` field is present.\\\n\\\n[`spark_submit_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-spark_submit_task) object\\\n\\\n(Legacy) The task runs the spark-submit script when the `spark_submit_task` field is present. This task can run only on new clusters and is not compatible with serverless compute.\\\n\\\nIn the `new_cluster` specification, `libraries` and `spark_conf` are not supported. Instead, use `--jars` and `--py-files` to add Java and Python libraries and `--conf` to set the Spark configurations.\\\n\\\n`master`, `deploy-mode`, and `executor-cores` are automatically configured by Azure Databricks; you _cannot_ specify them in parameters.\\\n\\\nBy default, the Spark submit job uses all available memory (excluding reserved memory for Azure Databricks services). You can set `--driver-memory`, and `--executor-memory` to a smaller value to leave some room for off-heap usage.\\\n\\\nThe `--jars`, `--py-files`, `--files` arguments support DBFS and S3 paths.\\\n\\\n[`pipeline_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-pipeline_task) object\\\n\\\nThe task triggers a pipeline update when the `pipeline_task` field is present. Only pipelines configured to use triggered more are supported.\\\n\\\n[`python_wheel_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-python_wheel_task) object\\\n\\\nThe task runs a Python wheel when the `python_wheel_task` field is present.\\\n\\\n[`dbt_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-dbt_task) object\\\n\\\nThe task runs one or more dbt commands when the `dbt_task` field is present. The dbt task requires both Databricks SQL and the ability to use a serverless or a pro SQL warehouse.\\\n\\\n[`sql_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-sql_task) object\\\n\\\nThe task runs a SQL query or file, or it refreshes a SQL alert or a legacy SQL dashboard when the `sql_task` field is present.\\\n\\\n[`run_job_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-run_job_task) object\\\n\\\nThe task triggers another job when the `run_job_task` field is present.\\\n\\\n[`condition_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-condition_task) object\\\n\\\nThe task evaluates a condition that can be used to control the execution of other tasks when the `condition_task` field is present.\\\nThe condition task does not require a cluster to execute and does not support retries or notifications.\\\n\\\n[`for_each_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-for_each_task) object\\\n\\\nThe task executes a nested task for every input provided when the `for_each_task` field is present.\\\n\\\n[`clean_rooms_notebook_task`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-clean_rooms_notebook_task) object\\\n\\\nPublic preview\\\n\\\nThe task runs a [clean rooms](https://docs.databricks.com/en/clean-rooms/index.html) notebook\\\nwhen the `clean_rooms_notebook_task` field is present.\\\n\\\n[`existing_cluster_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-existing_cluster_id) string\\\n\\\nExample`\"0923-164208-meows279\"`\\\n\\\nIf existing\\_cluster\\_id, the ID of an existing cluster that is used for all runs.\\\nWhen running jobs or tasks on an existing cluster, you may need to manually restart\\\nthe cluster if it stops responding. We suggest running jobs and tasks on new clusters for\\\ngreater reliability\\\n\\\n[`new_cluster`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-new_cluster) object\\\n\\\nIf new\\_cluster, a description of a new cluster that is created for each run.\\\n\\\n[`job_cluster_key`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-job_cluster_key) string\\\n\\\n\\[ 1 .. 100 \\] characters ^\\[\\\\w\\\\-\\\\\\_\\]+$\\\n\\\nIf job\\_cluster\\_key, this task is executed reusing the cluster specified in `job.settings.job_clusters`.\\\n\\\n[`libraries`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-libraries) Array of object\\\n\\\nAn optional list of libraries to be installed on the cluster.\\\nThe default value is an empty list.\\\n\\\n[`timeout_seconds`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-timeout_seconds) int32\\\n\\\nDefault`0`\\\n\\\nExample`86400`\\\n\\\nAn optional timeout applied to each run of this job task. A value of `0` means no timeout.\\\n\\\n[`email_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-email_notifications) object\\\n\\\nDefault`{}`\\\n\\\nAn optional set of email addresses notified when the task run begins or completes. The default behavior is to not send any emails.\\\n\\\n[`health`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-health) object\\\n\\\nAn optional set of health rules that can be defined for this job.\\\n\\\n[`notification_settings`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-notification_settings) object\\\n\\\nDefault`{}`\\\n\\\nOptional notification settings that are used when sending notifications to each of the `email_notifications` and `webhook_notifications` for this task run.\\\n\\\n[`webhook_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-webhook_notifications) object\\\n\\\nDefault`{}`\\\n\\\nA collection of system notification IDs to notify when the run begins or completes. The default behavior is to not send any system notifications. Task webhooks respect the task notification settings.\\\n\\\n[`environment_key`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#tasks-environment_key) string\\\n\\\n\\[ 1 .. 100 \\] characters ^\\[\\\\w\\\\-\\\\\\_\\]+$\\\n\\\nThe key that references an environment spec in a job. This field is required for Python script, Python wheel and dbt tasks when using serverless compute.\\\n\\\n\\]\n\n[`git_source`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#git_source) object\n\nExample\n\nAn optional specification for a remote Git repository containing the source code used by tasks. Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.\n\nIf `git_source` is set, these tasks retrieve the file from the remote repository by default. However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.\n\nNote: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are used, `git_source` must be defined on the job.\n\n[`git_url`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#git_source-git_url) requiredstring\n\n<= 300 characters\n\nExample`\"https://github.com/databricks/databricks-cli\"`\n\nURL of the repository to be cloned by this job.\n\n[`git_provider`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#git_source-git_provider) requiredstring\n\nEnum: `gitHub | bitbucketCloud | azureDevOpsServices | gitHubEnterprise | bitbucketServer | gitLab | gitLabEnterpriseEdition | awsCodeCommit`\n\nUnique identifier of the service used to host the Git repository. The value is case insensitive.\n\n[`git_branch`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#git_source-git_branch) string\n\n<= 255 characters\n\nExample`\"main\"`\n\nName of the branch to be checked out and used by this job. This field cannot be specified in conjunction with git\\_tag or git\\_commit.\n\n[`git_tag`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#git_source-git_tag) string\n\n<= 255 characters\n\nExample`\"release-1.0.0\"`\n\nName of the tag to be checked out and used by this job. This field cannot be specified in conjunction with git\\_branch or git\\_commit.\n\n[`git_commit`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#git_source-git_commit) string\n\n<= 64 characters\n\nExample`\"e0056d01\"`\n\nCommit to be checked out and used by this job. This field cannot be specified in conjunction with git\\_branch or git\\_tag.\n\n[`git_snapshot`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#git_source-git_snapshot) object\n\nRead-only state of the remote repository at the time the job was run. This field is only included on job runs.\n\n[`webhook_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#webhook_notifications) object\n\nDefault`{}`\n\nA collection of system notification IDs to notify when the run begins or completes.\n\n[`on_start`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#webhook_notifications-on_start) Array of object\n\nAn optional list of system notification IDs to call when the run starts. A maximum of 3 destinations can be specified for the `on_start` property.\n\n[`on_success`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#webhook_notifications-on_success) Array of object\n\nAn optional list of system notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified for the `on_success` property.\n\n[`on_failure`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#webhook_notifications-on_failure) Array of object\n\nAn optional list of system notification IDs to call when the run fails. A maximum of 3 destinations can be specified for the `on_failure` property.\n\n[`on_duration_warning_threshold_exceeded`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#webhook_notifications-on_duration_warning_threshold_exceeded) Array of object\n\nAn optional list of system notification IDs to call when the duration of a run exceeds the threshold specified for the `RUN_DURATION_SECONDS` metric in the `health` field. A maximum of 3 destinations can be specified for the `on_duration_warning_threshold_exceeded` property.\n\n[`on_streaming_backlog_exceeded`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#webhook_notifications-on_streaming_backlog_exceeded) Array of object\n\nPublic preview\n\nAn optional list of system notification IDs to call when any streaming backlog thresholds are exceeded for any stream.\nStreaming backlog thresholds can be set in the `health` field using the following metrics: `STREAMING_BACKLOG_BYTES`, `STREAMING_BACKLOG_RECORDS`, `STREAMING_BACKLOG_SECONDS`, or `STREAMING_BACKLOG_FILES`.\nAlerting is based on the 10-minute average of these metrics. If the issue persists, notifications are resent every 30 minutes.\nA maximum of 3 destinations can be specified for the `on_streaming_backlog_exceeded` property.\n\n[`email_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#email_notifications) object\n\nDefault`{}`\n\nAn optional set of email addresses notified when the run begins or completes.\n\n[`on_start`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#email_notifications-on_start) Array of string\n\nA list of email addresses to be notified when a run begins. If not specified on job creation, reset, or update, the list is empty, and notifications are not sent.\n\n[`on_success`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#email_notifications-on_success) Array of string\n\nA list of email addresses to be notified when a run successfully completes. A run is considered to have completed successfully if it ends with a `TERMINATED``life_cycle_state` and a `SUCCESS` result\\_state. If not specified on job creation, reset, or update, the list is empty, and notifications are not sent.\n\n[`on_failure`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#email_notifications-on_failure) Array of string\n\nA list of email addresses to be notified when a run unsuccessfully completes. A run is considered to have completed unsuccessfully if it ends with an `INTERNAL_ERROR``life_cycle_state` or a `FAILED`, or `TIMED_OUT` result\\_state. If this is not specified on job creation, reset, or update the list is empty, and notifications are not sent.\n\n[`on_duration_warning_threshold_exceeded`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#email_notifications-on_duration_warning_threshold_exceeded) Array of string\n\nA list of email addresses to be notified when the duration of a run exceeds the threshold specified for the `RUN_DURATION_SECONDS` metric in the `health` field. If no rule for the `RUN_DURATION_SECONDS` metric is specified in the `health` field for the job, notifications are not sent.\n\n[`on_streaming_backlog_exceeded`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#email_notifications-on_streaming_backlog_exceeded) Array of string\n\nPublic preview\n\nA list of email addresses to notify when any streaming backlog thresholds are exceeded for any stream.\nStreaming backlog thresholds can be set in the `health` field using the following metrics: `STREAMING_BACKLOG_BYTES`, `STREAMING_BACKLOG_RECORDS`, `STREAMING_BACKLOG_SECONDS`, or `STREAMING_BACKLOG_FILES`.\nAlerting is based on the 10-minute average of these metrics. If the issue persists, notifications are resent every 30 minutes.\n\n[`no_alert_for_skipped_runs`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#email_notifications-no_alert_for_skipped_runs) boolean\n\nDeprecated\n\nDefault`false`\n\nIf true, do not send email to recipients specified in `on_failure` if the run is skipped.\nThis field is `deprecated`. Please use the `notification_settings.no_alert_for_skipped_runs` field.\n\n[`notification_settings`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#notification_settings) object\n\nDefault`{}`\n\nOptional notification settings that are used when sending notifications to each of the `email_notifications` and `webhook_notifications` for this run.\n\n[`no_alert_for_skipped_runs`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#notification_settings-no_alert_for_skipped_runs) boolean\n\nDefault`false`\n\nIf true, do not send notifications to recipients specified in `on_failure` if the run is skipped.\n\n[`no_alert_for_canceled_runs`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#notification_settings-no_alert_for_canceled_runs) boolean\n\nDefault`false`\n\nIf true, do not send notifications to recipients specified in `on_failure` if the run is canceled.\n\n[`alert_on_last_attempt`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#notification_settings-alert_on_last_attempt) boolean\n\nDefault`false`\n\nIf true, do not send notifications to recipients specified in `on_start` for the retried runs and do not send notifications to recipients specified in `on_failure` until the last retry of the run.\n\n[`environments`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#environments) Array of object\n\n<= 10 items\n\nA list of task execution environment specifications that can be referenced by tasks of this run.\n\nArray \\[\\\n\\\n[`environment_key`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#environments-environment_key) requiredstring\\\n\\\nThe key of an environment. It has to be unique within a job.\\\n\\\n[`spec`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#environments-spec) object\\\n\\\nThe environment entity used to preserve serverless environment side panel and jobs' environment for non-notebook task.\\\nIn this minimal environment spec, only pip dependencies are supported.\\\n\\\n\\]\n\n[`access_control_list`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#access_control_list) Array of object\n\nList of permissions to set on the job.\n\nArray \\[\\\n\\\n[`user_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#access_control_list-user_name) string\\\n\\\nname of the user\\\n\\\n[`group_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#access_control_list-group_name) string\\\n\\\nname of the group\\\n\\\n[`service_principal_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#access_control_list-service_principal_name) string\\\n\\\napplication ID of a service principal\\\n\\\n[`permission_level`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#access_control_list-permission_level) string\\\n\\\nEnum: `CAN_MANAGE | IS_OWNER | CAN_MANAGE_RUN | CAN_VIEW`\\\n\\\nPermission level\\\n\\\n\\]\n\n[`queue`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#queue) object\n\nThe queue settings of the one-time run.\n\n[`enabled`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#queue-enabled) requiredboolean\n\nExample`true`\n\nIf true, enable queueing for the job. This is a required field.\n\n[`run_as`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#run_as) object\n\nSpecifies the user or service principal that the job runs as. If not specified, the job runs as the user who submits the request.\n\n[`user_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#run_as-user_name) string\n\nExample`\"user@databricks.com\"`\n\nThe email of an active workspace user. Non-admin users can only set this field to their own email.\n\n[`service_principal_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#run_as-service_principal_name) string\n\nExample`\"692bc6d0-ffa3-11ed-be56-0242ac120002\"`\n\nApplication ID of an active service principal. Setting this field requires the `servicePrincipal/user` role.\n\n### Responses\n\n**200** Request completed successfully.\n\nRequest completed successfully.\n\n[`run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/submit#run_id) int64\n\nExample`455644833`\n\nThe canonical identifier for the newly submitted run.\n\nThis method might return the following HTTP codes: 400, 401, 403, 404, 429, 500\n\nError responses are returned in the following format:\n\n```\n{\n  \"error_code\": \"Error code\",\n  \"message\": \"Human-readable error message.\"\n}\n```\n\n# Possible error codes:\n\nHTTP code\n\nerror\\_code\n\nDescription\n\n400\n\nINVALID\\_PARAMETER\\_VALUE\n\nSupplied value for a parameter was invalid.\n\n401\n\nUNAUTHORIZED\n\nThe request does not have valid authentication credentials for the operation.\n\n403\n\nPERMISSION\\_DENIED\n\nCaller does not have permission to execute the specified operation.\n\n404\n\nFEATURE\\_DISABLED\n\nIf a given user/entity is trying to use a feature which has been disabled.\n\n429\n\nREQUEST\\_LIMIT\\_EXCEEDED\n\nRequest is rejected due to throttling.\n\n500\n\nINTERNAL\\_SERVER\\_ERROR\n\nInternal error.\n\n# Request samples\n\nJSON\n\n```\n{\n  \"run_name\": \"A multitask job run\",\n  \"timeout_seconds\": 86400,\n  \"health\": {\n    \"rules\": [\\\n      {\\\n        \"metric\": \"RUN_DURATION_SECONDS\",\\\n        \"op\": \"GREATER_THAN\",\\\n        \"value\": 10\\\n      }\\\n    ]\n  },\n  \"idempotency_token\": \"8f018174-4792-40d5-bcbc-3e6a527352c8\",\n  \"tasks\": [\\\n    {\\\n      \"task_key\": \"Sessionize\",\\\n      \"description\": \"Extracts session data from events\",\\\n      \"min_retry_interval_millis\": 2000,\\\n      \"depends_on\": [],\\\n      \"timeout_seconds\": 86400,\\\n      \"spark_jar_task\": {\\\n        \"main_class_name\": \"com.databricks.Sessionize\",\\\n        \"parameters\": [\\\n          \"--data\",\\\n          \"dbfs:/path/to/data.json\"\\\n        ]\\\n      },\\\n      \"libraries\": [\\\n        {\\\n          \"jar\": \"dbfs:/mnt/databricks/Sessionize.jar\"\\\n        }\\\n      ],\\\n      \"retry_on_timeout\": false,\\\n      \"existing_cluster_id\": \"0923-164208-meows279\"\\\n    },\\\n    {\\\n      \"task_key\": \"Orders_Ingest\",\\\n      \"description\": \"Ingests order data\",\\\n      \"depends_on\": [],\\\n      \"timeout_seconds\": 86400,\\\n      \"spark_jar_task\": {\\\n        \"main_class_name\": \"com.databricks.OrdersIngest\",\\\n        \"parameters\": [\\\n          \"--data\",\\\n          \"dbfs:/path/to/order-data.json\"\\\n        ]\\\n      },\\\n      \"libraries\": [\\\n        {\\\n          \"jar\": \"dbfs:/mnt/databricks/OrderIngest.jar\"\\\n        }\\\n      ],\\\n      \"existing_cluster_id\": \"0923-164208-meows279\"\\\n    },\\\n    {\\\n      \"task_key\": \"Match\",\\\n      \"description\": \"Matches orders with user sessions\",\\\n      \"notebook_task\": {\\\n        \"base_parameters\": {\\\n          \"age\": \"35\",\\\n          \"name\": \"John Doe\"\\\n        },\\\n        \"notebook_path\": \"/Users/user.name@databricks.com/Match\"\\\n      },\\\n      \"depends_on\": [\\\n        {\\\n          \"task_key\": \"Orders_Ingest\"\\\n        },\\\n        {\\\n          \"task_key\": \"Sessionize\"\\\n        }\\\n      ],\\\n      \"new_cluster\": {\\\n        \"autoscale\": {\\\n          \"max_workers\": 16,\\\n          \"min_workers\": 2\\\n        },\\\n        \"node_type_id\": null,\\\n        \"spark_conf\": {\\\n          \"spark.speculation\": true\\\n        },\\\n        \"spark_version\": \"7.3.x-scala2.12\"\\\n      },\\\n      \"timeout_seconds\": 86400,\\\n      \"retry_on_timeout\": false,\\\n      \"run_if\": \"ALL_SUCCESS\"\\\n    }\\\n  ],\n  \"git_source\": {\n    \"git_branch\": \"main\",\n    \"git_provider\": \"gitHub\",\n    \"git_url\": \"https://github.com/databricks/databricks-cli\"\n  },\n  \"webhook_notifications\": {\n    \"on_start\": [\\\n      [\\\n        {\\\n          \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n        }\\\n      ]\\\n    ],\n    \"on_success\": [\\\n      [\\\n        {\\\n          \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n        }\\\n      ]\\\n    ],\n    \"on_failure\": [\\\n      [\\\n        {\\\n          \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n        }\\\n      ]\\\n    ],\n    \"on_duration_warning_threshold_exceeded\": [\\\n      [\\\n        {\\\n          \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n        }\\\n      ]\\\n    ],\n    \"on_streaming_backlog_exceeded\": [\\\n      [\\\n        {\\\n          \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n        }\\\n      ]\\\n    ]\n  },\n  \"email_notifications\": {\n    \"on_start\": [\\\n      \"user.name@databricks.com\"\\\n    ],\n    \"on_success\": [\\\n      \"user.name@databricks.com\"\\\n    ],\n    \"on_failure\": [\\\n      \"user.name@databricks.com\"\\\n    ],\n    \"on_duration_warning_threshold_exceeded\": [\\\n      \"user.name@databricks.com\"\\\n    ],\n    \"on_streaming_backlog_exceeded\": [\\\n      \"user.name@databricks.com\"\\\n    ],\n    \"no_alert_for_skipped_runs\": false\n  },\n  \"notification_settings\": {\n    \"no_alert_for_skipped_runs\": false,\n    \"no_alert_for_canceled_runs\": false,\n    \"alert_on_last_attempt\": false\n  },\n  \"environments\": [\\\n    {\\\n      \"environment_key\": \"string\",\\\n      \"spec\": {\\\n        \"client\": \"1\",\\\n        \"dependencies\": [\\\n          \"string\"\\\n        ]\\\n      }\\\n    }\\\n  ],\n  \"access_control_list\": [\\\n    {\\\n      \"user_name\": \"string\",\\\n      \"group_name\": \"string\",\\\n      \"service_principal_name\": \"string\",\\\n      \"permission_level\": \"CAN_MANAGE\"\\\n    }\\\n  ],\n  \"queue\": {\n    \"enabled\": true\n  },\n  \"run_as\": {\n    \"user_name\": \"user@databricks.com\",\n    \"service_principal_name\": \"692bc6d0-ffa3-11ed-be56-0242ac120002\"\n  }\n}\n```\n\n# Response samples\n\n200\n\n```\n{\n  \"run_id\": 455644833\n}\n```"
  },
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## Update job settings partially\n\n`\nPOST/api/2.1/jobs/update`\n\nAdd, update, or remove specific settings of an existing job. Use the [_Reset_ endpoint](https://docs.databricks.com/api/azure/workspace/jobs/reset) to overwrite all job settings.\n\n### Request body\n\n[`job_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#job_id) requiredint64\n\nExample`11223344`\n\nThe canonical identifier of the job to update. This field is required.\n\n[`new_settings`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings) object\n\nThe new settings for the job.\n\nTop-level fields specified in `new_settings` are completely replaced, except for arrays which are merged. That is, new and existing entries are completely replaced based on the respective key fields, i.e. `task_key` or `job_cluster_key`, while previous entries are kept.\n\nPartially updating nested fields is not supported.\n\nChanges to the field `JobSettings.timeout_seconds` are applied to active runs. Changes to other fields are applied to future runs only.\n\n[`name`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-name) string\n\n<= 4096 characters\n\nDefault`\"Untitled\"`\n\nExample`\"A multitask job\"`\n\nAn optional name for the job. The maximum length is 4096 bytes in UTF-8 encoding.\n\n[`description`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-description) string\n\n<= 27700 characters\n\nExample`\"This job contain multiple tasks that are required to produce the weekly shark sightings report.\"`\n\nAn optional description for the job. The maximum length is 27700 characters in UTF-8 encoding.\n\n[`email_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-email_notifications) object\n\nDefault`{}`\n\nAn optional set of email addresses that is notified when runs of this job begin or complete as well as when this job is deleted.\n\n[`webhook_notifications`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-webhook_notifications) object\n\nDefault`{}`\n\nA collection of system notification IDs to notify when runs of this job begin or complete.\n\n[`notification_settings`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-notification_settings) object\n\nDefault`{}`\n\nOptional notification settings that are used when sending notifications to each of the `email_notifications` and `webhook_notifications` for this job.\n\n[`timeout_seconds`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-timeout_seconds) int32\n\nDefault`0`\n\nExample`86400`\n\nAn optional timeout applied to each run of this job. A value of `0` means no timeout.\n\n[`health`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-health) object\n\nAn optional set of health rules that can be defined for this job.\n\n[`schedule`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-schedule) object\n\nAn optional periodic schedule for this job. The default behavior is that the job only runs when triggered by clicking “Run Now” in the Jobs UI or sending an API request to `runNow`.\n\n[`trigger`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-trigger) object\n\nA configuration to trigger a run when certain conditions are met. The default behavior is that the job runs only when triggered by clicking “Run Now” in the Jobs UI or sending an API request to `runNow`.\n\n[`continuous`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-continuous) object\n\nAn optional continuous property for this job. The continuous property will ensure that there is always one run executing. Only one of `schedule` and `continuous` can be used.\n\n[`max_concurrent_runs`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-max_concurrent_runs) int32\n\nDefault`1`\n\nExample`10`\n\nAn optional maximum allowed number of concurrent runs of the job.\nSet this value if you want to be able to execute multiple runs of the same job concurrently.\nThis is useful for example if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or if you want to trigger multiple runs which differ by their input parameters.\nThis setting affects only new runs. For example, suppose the job’s concurrency is 4 and there are 4 concurrent active runs. Then setting the concurrency to 3 won’t kill any of the active runs.\nHowever, from then on, new runs are skipped unless there are fewer than 3 active runs.\nThis value cannot exceed 1000. Setting this value to `0` causes all new runs to be skipped.\n\n[`tasks`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-tasks) Array of object\n\n<= 100 items\n\nExample\n\nA list of task specifications to be executed by this job.\nIf more than 100 tasks are available, you can paginate through them using [jobs/get](https://docs.databricks.com/api/azure/workspace/jobs/get). Use the `next_page_token` field at the object root to determine if more results are available.\n\n[`job_clusters`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-job_clusters) Array of object\n\n<= 100 items\n\nExample\n\nA list of job cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings.\nIf more than 100 job clusters are available, you can paginate through them using [jobs/get](https://docs.databricks.com/api/azure/workspace/jobs/get).\n\n[`git_source`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-git_source) object\n\nExample\n\nAn optional specification for a remote Git repository containing the source code used by tasks. Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.\n\nIf `git_source` is set, these tasks retrieve the file from the remote repository by default. However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.\n\nNote: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are used, `git_source` must be defined on the job.\n\n[`tags`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-tags) object\n\nDefault`{}`\n\nExample\n\nA map of tags associated with the job. These are forwarded to the cluster as cluster tags for jobs clusters, and are subject to the same limitations as cluster tags. A maximum of 25 tags can be added to the job.\n\n[`format`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-format) string\n\nDeprecated\n\nEnum: `SINGLE_TASK | MULTI_TASK`\n\nExample`\"MULTI_TASK\"`\n\nUsed to tell what is the format of the job. This field is ignored in Create/Update/Reset calls. When using the Jobs API 2.1 this value is always set to `\"MULTI_TASK\"`.\n\n[`queue`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-queue) object\n\nThe queue settings of the job.\n\n[`parameters`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-parameters) Array of object\n\nJob-level parameter definitions\n\n[`run_as`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-run_as) object\n\nWrite-only setting. Specifies the user or service principal that the job runs as. If not specified, the job runs as the user who created the job.\n\nEither `user_name` or `service_principal_name` should be specified. If not, an error is thrown.\n\n[`edit_mode`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-edit_mode) string\n\nEnum: `UI_LOCKED | EDITABLE`\n\nEdit mode of the job.\n\n- `UI_LOCKED`: The job is in a locked UI state and cannot be modified.\n- `EDITABLE`: The job is in an editable state and can be modified.\n\n[`deployment`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-deployment) object\n\nDeployment information for jobs managed by external sources.\n\n[`environments`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#new_settings-environments) Array of object\n\n<= 10 items\n\nA list of task execution environment specifications that can be referenced by serverless tasks of this job.\nAn environment is required to be present for serverless tasks.\nFor serverless notebook tasks, the environment is accessible in the notebook environment panel.\nFor other serverless tasks, the task environment is required to be specified using environment\\_key in the task settings.\n\n[`fields_to_remove`](https://docs.databricks.com/api/azure/workspace/jobs_21/update#fields_to_remove) Array of string\n\nExample\n\nRemove top-level fields in the job settings. Removing nested fields is not supported, except for tasks and job clusters (`tasks/task_1`). This field is optional.\n\n### Responses\n\n**200** Request completed successfully.\n\nRequest completed successfully.\n\nThis method might return the following HTTP codes: 400, 401, 403, 404, 429, 500\n\nError responses are returned in the following format:\n\n```\n{\n  \"error_code\": \"Error code\",\n  \"message\": \"Human-readable error message.\"\n}\n```\n\n# Possible error codes:\n\nHTTP code\n\nerror\\_code\n\nDescription\n\n400\n\nINVALID\\_PARAMETER\\_VALUE\n\nSupplied value for a parameter was invalid.\n\n401\n\nUNAUTHORIZED\n\nThe request does not have valid authentication credentials for the operation.\n\n403\n\nPERMISSION\\_DENIED\n\nCaller does not have permission to execute the specified operation.\n\n404\n\nFEATURE\\_DISABLED\n\nIf a given user/entity is trying to use a feature which has been disabled.\n\n429\n\nREQUEST\\_LIMIT\\_EXCEEDED\n\nRequest is rejected due to throttling.\n\n500\n\nINTERNAL\\_SERVER\\_ERROR\n\nInternal error.\n\n# Request samples\n\nJSON\n\n```\n{\n  \"job_id\": 11223344,\n  \"new_settings\": {\n    \"name\": \"A multitask job\",\n    \"description\": \"This job contain multiple tasks that are required to produce the weekly shark sightings report.\",\n    \"email_notifications\": {\n      \"on_start\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"on_success\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"on_failure\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"on_duration_warning_threshold_exceeded\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"on_streaming_backlog_exceeded\": [\\\n        \"user.name@databricks.com\"\\\n      ],\n      \"no_alert_for_skipped_runs\": false\n    },\n    \"webhook_notifications\": {\n      \"on_start\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ],\n      \"on_success\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ],\n      \"on_failure\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ],\n      \"on_duration_warning_threshold_exceeded\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ],\n      \"on_streaming_backlog_exceeded\": [\\\n        [\\\n          {\\\n            \"id\": \"0481e838-0a59-4eff-9541-a4ca6f149574\"\\\n          }\\\n        ]\\\n      ]\n    },\n    \"notification_settings\": {\n      \"no_alert_for_skipped_runs\": false,\n      \"no_alert_for_canceled_runs\": false,\n      \"alert_on_last_attempt\": false\n    },\n    \"timeout_seconds\": 86400,\n    \"health\": {\n      \"rules\": [\\\n        {\\\n          \"metric\": \"RUN_DURATION_SECONDS\",\\\n          \"op\": \"GREATER_THAN\",\\\n          \"value\": 10\\\n        }\\\n      ]\n    },\n    \"schedule\": {\n      \"quartz_cron_expression\": \"20 30 * * * ?\",\n      \"timezone_id\": \"Europe/London\",\n      \"pause_status\": \"UNPAUSED\"\n    },\n    \"trigger\": {\n      \"pause_status\": \"UNPAUSED\",\n      \"file_arrival\": {\n        \"url\": \"string\",\n        \"min_time_between_triggers_seconds\": 0,\n        \"wait_after_last_change_seconds\": 0\n      },\n      \"periodic\": {\n        \"interval\": 0,\n        \"unit\": \"HOURS\"\n      }\n    },\n    \"continuous\": {\n      \"pause_status\": \"UNPAUSED\"\n    },\n    \"max_concurrent_runs\": 10,\n    \"tasks\": [\\\n      {\\\n        \"max_retries\": 3,\\\n        \"task_key\": \"Sessionize\",\\\n        \"description\": \"Extracts session data from events\",\\\n        \"min_retry_interval_millis\": 2000,\\\n        \"depends_on\": [],\\\n        \"timeout_seconds\": 86400,\\\n        \"spark_jar_task\": {\\\n          \"main_class_name\": \"com.databricks.Sessionize\",\\\n          \"parameters\": [\\\n            \"--data\",\\\n            \"dbfs:/path/to/data.json\"\\\n          ]\\\n        },\\\n        \"libraries\": [\\\n          {\\\n            \"jar\": \"dbfs:/mnt/databricks/Sessionize.jar\"\\\n          }\\\n        ],\\\n        \"retry_on_timeout\": false,\\\n        \"existing_cluster_id\": \"0923-164208-meows279\"\\\n      },\\\n      {\\\n        \"max_retries\": 3,\\\n        \"task_key\": \"Orders_Ingest\",\\\n        \"description\": \"Ingests order data\",\\\n        \"job_cluster_key\": \"auto_scaling_cluster\",\\\n        \"min_retry_interval_millis\": 2000,\\\n        \"depends_on\": [],\\\n        \"timeout_seconds\": 86400,\\\n        \"spark_jar_task\": {\\\n          \"main_class_name\": \"com.databricks.OrdersIngest\",\\\n          \"parameters\": [\\\n            \"--data\",\\\n            \"dbfs:/path/to/order-data.json\"\\\n          ]\\\n        },\\\n        \"libraries\": [\\\n          {\\\n            \"jar\": \"dbfs:/mnt/databricks/OrderIngest.jar\"\\\n          }\\\n        ],\\\n        \"retry_on_timeout\": false\\\n      },\\\n      {\\\n        \"max_retries\": 3,\\\n        \"task_key\": \"Match\",\\\n        \"description\": \"Matches orders with user sessions\",\\\n        \"notebook_task\": {\\\n          \"base_parameters\": {\\\n            \"age\": \"35\",\\\n            \"name\": \"John Doe\"\\\n          },\\\n          \"notebook_path\": \"/Users/user.name@databricks.com/Match\"\\\n        },\\\n        \"min_retry_interval_millis\": 2000,\\\n        \"depends_on\": [\\\n          {\\\n            \"task_key\": \"Orders_Ingest\"\\\n          },\\\n          {\\\n            \"task_key\": \"Sessionize\"\\\n          }\\\n        ],\\\n        \"new_cluster\": {\\\n          \"autoscale\": {\\\n            \"max_workers\": 16,\\\n            \"min_workers\": 2\\\n          },\\\n          \"node_type_id\": null,\\\n          \"spark_conf\": {\\\n            \"spark.speculation\": true\\\n          },\\\n          \"spark_version\": \"7.3.x-scala2.12\"\\\n        },\\\n        \"timeout_seconds\": 86400,\\\n        \"retry_on_timeout\": false,\\\n        \"run_if\": \"ALL_SUCCESS\"\\\n      }\\\n    ],\n    \"job_clusters\": [\\\n      {\\\n        \"job_cluster_key\": \"auto_scaling_cluster\",\\\n        \"new_cluster\": {\\\n          \"autoscale\": {\\\n            \"max_workers\": 16,\\\n            \"min_workers\": 2\\\n          },\\\n          \"node_type_id\": null,\\\n          \"spark_conf\": {\\\n            \"spark.speculation\": true\\\n          },\\\n          \"spark_version\": \"7.3.x-scala2.12\"\\\n        }\\\n      }\\\n    ],\n    \"git_source\": {\n      \"git_branch\": \"main\",\n      \"git_provider\": \"gitHub\",\n      \"git_url\": \"https://github.com/databricks/databricks-cli\"\n    },\n    \"tags\": {\n      \"cost-center\": \"engineering\",\n      \"team\": \"jobs\"\n    },\n    \"format\": \"SINGLE_TASK\",\n    \"queue\": {\n      \"enabled\": true\n    },\n    \"parameters\": [\\\n      {\\\n        \"default\": \"users\",\\\n        \"name\": \"table\"\\\n      }\\\n    ],\n    \"run_as\": {\n      \"user_name\": \"user@databricks.com\",\n      \"service_principal_name\": \"692bc6d0-ffa3-11ed-be56-0242ac120002\"\n    },\n    \"edit_mode\": \"UI_LOCKED\",\n    \"deployment\": {\n      \"kind\": \"BUNDLE\",\n      \"metadata_file_path\": \"string\"\n    },\n    \"environments\": [\\\n      {\\\n        \"environment_key\": \"string\",\\\n        \"spec\": {\\\n          \"client\": \"1\",\\\n          \"dependencies\": [\\\n            \"string\"\\\n          ]\\\n        }\\\n      }\\\n    ]\n  },\n  \"fields_to_remove\": [\\\n    \"libraries\",\\\n    \"schedule\",\\\n    \"tasks/task_1\",\\\n    \"job_clusters/Default\"\\\n  ]\n}\n```\n\n# Response samples\n\n200\n\n```\n{}\n```"
  },
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## Cancel a run\n\n`\nPOST/api/2.1/jobs/runs/cancel`\n\nCancels a job run or a task run. The run is canceled asynchronously, so it may still be running when\nthis request completes.\n\n### Request body\n\n[`run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun#run_id) requiredint64\n\nExample`455644833`\n\nThis field is required.\n\n### Responses\n\n**200** Request completed successfully.\n\nRequest completed successfully.\n\nThis method might return the following HTTP codes: 400, 401, 403, 429, 500\n\nError responses are returned in the following format:\n\n```\n{\n  \"error_code\": \"Error code\",\n  \"message\": \"Human-readable error message.\"\n}\n```\n\n# Possible error codes:\n\nHTTP code\n\nerror\\_code\n\nDescription\n\n400\n\nINVALID\\_PARAMETER\\_VALUE\n\nSupplied value for a parameter was invalid.\n\n401\n\nUNAUTHORIZED\n\nThe request does not have valid authentication credentials for the operation.\n\n403\n\nPERMISSION\\_DENIED\n\nCaller does not have permission to execute the specified operation.\n\n429\n\nREQUEST\\_LIMIT\\_EXCEEDED\n\nRequest is rejected due to throttling.\n\n500\n\nINTERNAL\\_SERVER\\_ERROR\n\nInternal error.\n\n# Request samples\n\nJSON\n\n```\n{\n  \"run_id\": 455644833\n}\n```\n\n# Response samples\n\n200\n\n```\n{}\n```"
  },
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## Repair a job run\n\n`\nPOST/api/2.1/jobs/runs/repair`\n\nRe-run one or more tasks. Tasks are re-run as part of the original job run.\nThey use the current job and task settings, and can be viewed in the history for the\noriginal job run.\n\n### Request body\n\n[`run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#run_id) requiredint64\n\nExample`455644833`\n\nThe job run ID of the run to repair. The run must not be in progress.\n\n[`latest_repair_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#latest_repair_id) int64\n\nExample`734650698524280`\n\nThe ID of the latest repair. This parameter is not required when repairing a run for the first time, but must be provided on subsequent requests to repair the same run.\n\n[`rerun_tasks`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#rerun_tasks) Array of string\n\nExample\n\nThe task keys of the task runs to repair.\n\n[`job_parameters`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#job_parameters) object\n\nJob-level parameters used in the run. for example `\"param\": \"overriding_val\"`\n\n[`pipeline_params`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#pipeline_params) object\n\nControls whether the pipeline should perform a full refresh\n\n[`full_refresh`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#pipeline_params-full_refresh) boolean\n\nDefault`false`\n\nIf true, triggers a full refresh on the delta live table.\n\n[`jar_params`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#jar_params) Array of string\n\nDeprecated\n\nExample\n\nA list of parameters for jobs with Spark JAR tasks, for example `\"jar_params\": [\"john doe\", \"35\"]`.\nThe parameters are used to invoke the main function of the main class specified in the Spark JAR task.\nIf not specified upon `run-now`, it defaults to an empty list.\njar\\_params cannot be specified in conjunction with notebook\\_params.\nThe JSON representation of this field (for example `{\"jar_params\":[\"john doe\",\"35\"]}`) cannot exceed 10,000 bytes.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n\n[`notebook_params`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#notebook_params) object\n\nDeprecated\n\nExample\n\nA map from keys to values for jobs with notebook task, for example `\"notebook_params\": {\"name\": \"john doe\", \"age\": \"35\"}`.\nThe map is passed to the notebook and is accessible through the [dbutils.widgets.get](https://docs.databricks.com/dev-tools/databricks-utils.html) function.\n\nIf not specified upon `run-now`, the triggered run uses the job’s base parameters.\n\nnotebook\\_params cannot be specified in conjunction with jar\\_params.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n\nThe JSON representation of this field (for example `{\"notebook_params\":{\"name\":\"john doe\",\"age\":\"35\"}}`) cannot exceed 10,000 bytes.\n\n[`python_params`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#python_params) Array of string\n\nDeprecated\n\nExample\n\nA list of parameters for jobs with Python tasks, for example `\"python_params\": [\"john doe\", \"35\"]`.\nThe parameters are passed to Python file as command-line parameters. If specified upon `run-now`, it would overwrite\nthe parameters specified in job setting. The JSON representation of this field (for example `{\"python_params\":[\"john doe\",\"35\"]}`)\ncannot exceed 10,000 bytes.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n\nImportant\n\nThese parameters accept only Latin characters (ASCII character set). Using non-ASCII characters returns an error.\nExamples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis.\n\n[`spark_submit_params`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#spark_submit_params) Array of string\n\nDeprecated\n\nExample\n\nA list of parameters for jobs with spark submit task, for example `\"spark_submit_params\": [\"--class\", \"org.apache.spark.examples.SparkPi\"]`.\nThe parameters are passed to spark-submit script as command-line parameters. If specified upon `run-now`, it would overwrite the\nparameters specified in job setting. The JSON representation of this field (for example `{\"python_params\":[\"john doe\",\"35\"]}`)\ncannot exceed 10,000 bytes.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs\n\nImportant\n\nThese parameters accept only Latin characters (ASCII character set). Using non-ASCII characters returns an error.\nExamples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis.\n\n[`python_named_params`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#python_named_params) object\n\nDeprecated\n\nExample\n\n[`sql_params`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#sql_params) object\n\nDeprecated\n\nExample\n\nA map from keys to values for jobs with SQL task, for example `\"sql_params\": {\"name\": \"john doe\", \"age\": \"35\"}`. The SQL alert task does not support custom parameters.\n\n[`dbt_commands`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#dbt_commands) Array of string\n\nDeprecated\n\nExample\n\nAn array of commands to execute for jobs with the dbt task, for example `\"dbt_commands\": [\"dbt deps\", \"dbt seed\", \"dbt deps\", \"dbt seed\", \"dbt run\"]`\n\n[`rerun_all_failed_tasks`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#rerun_all_failed_tasks) boolean\n\nDefault`false`\n\nIf true, repair all failed tasks. Only one of `rerun_tasks` or `rerun_all_failed_tasks` can be used.\n\n[`rerun_dependent_tasks`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#rerun_dependent_tasks) boolean\n\nDefault`false`\n\nIf true, repair all tasks that depend on the tasks in `rerun_tasks`, even if they were previously successful. Can be also used in combination with `rerun_all_failed_tasks`.\n\n### Responses\n\n**200** Request completed successfully.\n\nRequest completed successfully.\n\n[`repair_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun#repair_id) int64\n\nExample`734650698524280`\n\nThe ID of the repair. Must be provided in subsequent repairs using the `latest_repair_id` field to ensure sequential repairs.\n\nThis method might return the following HTTP codes: 400, 401, 403, 500\n\nError responses are returned in the following format:\n\n```\n{\n  \"error_code\": \"Error code\",\n  \"message\": \"Human-readable error message.\"\n}\n```\n\n# Possible error codes:\n\nHTTP code\n\nerror\\_code\n\nDescription\n\n400\n\nINVALID\\_PARAMETER\\_VALUE\n\nSupplied value for a parameter was invalid.\n\n401\n\nUNAUTHORIZED\n\nThe request does not have valid authentication credentials for the operation.\n\n403\n\nPERMISSION\\_DENIED\n\nCaller does not have permission to execute the specified operation.\n\n500\n\nINTERNAL\\_SERVER\\_ERROR\n\nInternal error.\n\n# Request samples\n\nJSON\n\n```\n{\n  \"run_id\": 455644833,\n  \"latest_repair_id\": 734650698524280,\n  \"rerun_tasks\": [\\\n    \"task0\",\\\n    \"task1\"\\\n  ],\n  \"job_parameters\": {\n    \"property1\": \"string\",\n    \"property2\": \"string\"\n  },\n  \"pipeline_params\": {\n    \"full_refresh\": false\n  },\n  \"jar_params\": [\\\n    \"john\",\\\n    \"doe\",\\\n    \"35\"\\\n  ],\n  \"notebook_params\": {\n    \"age\": \"35\",\n    \"name\": \"john doe\"\n  },\n  \"python_params\": [\\\n    \"john doe\",\\\n    \"35\"\\\n  ],\n  \"spark_submit_params\": [\\\n    \"--class\",\\\n    \"org.apache.spark.examples.SparkPi\"\\\n  ],\n  \"python_named_params\": {\n    \"data\": \"dbfs:/path/to/data.json\",\n    \"name\": \"task\"\n  },\n  \"sql_params\": {\n    \"age\": \"35\",\n    \"name\": \"john doe\"\n  },\n  \"dbt_commands\": [\\\n    \"dbt deps\",\\\n    \"dbt seed\",\\\n    \"dbt run\"\\\n  ],\n  \"rerun_all_failed_tasks\": false,\n  \"rerun_dependent_tasks\": false\n}\n```\n\n# Response samples\n\n200\n\n```\n{\n  \"repair_id\": 734650698524280\n}\n```"
  },
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## List job runs\n\n`\nGET/api/2.1/jobs/runs/list`\n\nList runs in descending order by start time.\n\n### Query parameters\n\n[`job_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#job_id) int64\n\nExample`job_id=11223344`\n\nThe job for which to list runs. If omitted, the Jobs service lists runs from all jobs.\n\n[`active_only`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#active_only) boolean\n\nExample`active_only=false`\n\nIf active\\_only is `true`, only active runs are included in the results; otherwise,\nlists both active and completed runs. An active run is a run in the `QUEUED`, `PENDING`,\n`RUNNING`, or `TERMINATING`. This field cannot be `true` when completed\\_only is `true`.\n\n[`completed_only`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#completed_only) boolean\n\nExample`completed_only=false`\n\nIf completed\\_only is `true`, only completed runs are included in the results;\notherwise, lists both active and completed runs. This field cannot be `true` when\nactive\\_only is `true`.\n\n[`offset`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#offset) int32\n\n<= 10000\n\nDeprecated\n\nExample`offset=0`\n\nThe offset of the first run to return, relative to the most recent run.\nDeprecated since June 2023. Use `page_token` to iterate through the pages instead.\n\n[`limit`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#limit) int32\n\n\\[ 1 .. 25 \\]\n\nDefault`20`\n\nThe number of runs to return. This value must be greater than 0 and less than 25.\nThe default value is 20. If a request specifies a limit of 0, the service instead\nuses the maximum limit.\n\n[`run_type`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#run_type) string\n\nEnum: `JOB_RUN | WORKFLOW_RUN | SUBMIT_RUN`\n\nExample`run_type=JOB_RUN`\n\nThe type of runs to return. For a description of run types, see [jobs/getrun](https://docs.databricks.com/api/azure/workspace/jobs/getrun).\n\n[`expand_tasks`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#expand_tasks) boolean\n\nDefault`false`\n\nWhether to include task and cluster details in the response.\n\n[`start_time_from`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#start_time_from) int64\n\nExample`start_time_from=1642521600000`\n\nShow runs that started _at or after_ this value. The value must be a UTC timestamp\nin milliseconds. Can be combined with _start\\_time\\_to_ to filter by a time range.\n\n[`start_time_to`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#start_time_to) int64\n\nExample`start_time_to=1642608000000`\n\nShow runs that started _at or before_ this value. The value must be a UTC timestamp\nin milliseconds. Can be combined with _start\\_time\\_from_ to filter by a time range.\n\n[`page_token`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#page_token) string\n\nExample`page_token=CAEomPSriYcxMPWM_IiIxvEB`\n\nUse `next_page_token` or `prev_page_token` returned from the previous request to list the next or previous page of runs respectively.\n\n### Responses\n\n**200** Request completed successfully.\n\nRequest completed successfully.\n\n[`runs`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs) Array of object\n\nA list of runs, from most recently started to least. Only included in the response if there are runs to list.\n\nArray \\[\\\n\\\n[`job_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-job_id) int64\\\n\\\nExample`11223344`\\\n\\\nThe canonical identifier of the job that contains this run.\\\n\\\n[`run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-run_id) int64\\\n\\\nExample`455644833`\\\n\\\nThe canonical identifier of the run. This ID is unique across all runs of all jobs.\\\n\\\n[`creator_user_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-creator_user_name) string\\\n\\\nExample`\"user.name@databricks.com\"`\\\n\\\nThe creator user name. This field won’t be included in the response if the user has already been deleted.\\\n\\\n[`number_in_job`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-number_in_job) int64\\\n\\\nDeprecated\\\n\\\nExample`455644833`\\\n\\\nA unique identifier for this job run. This is set to the same value as `run_id`.\\\n\\\n[`original_attempt_run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-original_attempt_run_id) int64\\\n\\\nExample`455644833`\\\n\\\nIf this run is a retry of a prior run attempt, this field contains the run\\_id of the original attempt; otherwise, it is the same as the run\\_id.\\\n\\\n[`state`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-state) object\\\n\\\nDeprecated\\\n\\\nDeprecated. Please use the `status` field instead.\\\n\\\n[`schedule`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-schedule) object\\\n\\\nThe cron schedule that triggered this run if it was triggered by the periodic scheduler.\\\n\\\n[`cluster_spec`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-cluster_spec) object\\\n\\\nA snapshot of the job’s cluster specification when this run was created.\\\n\\\n[`cluster_instance`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-cluster_instance) object\\\n\\\nThe cluster used for this run. If the run is specified to use a new cluster, this field is set once the Jobs service has requested a cluster for the run.\\\n\\\n[`job_parameters`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-job_parameters) Array of object\\\n\\\nJob-level parameters used in the run\\\n\\\n[`overriding_parameters`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-overriding_parameters) object\\\n\\\nThe parameters used for this run.\\\n\\\n[`start_time`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-start_time) int64\\\n\\\nExample`1625060460483`\\\n\\\nThe time at which this run was started in epoch milliseconds (milliseconds since 1/1/1970 UTC). This may not be the time when the job task starts executing, for example, if the job is scheduled to run on a new cluster, this is the time the cluster creation call is issued.\\\n\\\n[`setup_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-setup_duration) int64\\\n\\\nExample`0`\\\n\\\nThe time in milliseconds it took to set up the cluster. For runs that run on new clusters this is the cluster creation time, for runs that run on existing clusters this time should be very short. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `setup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.\\\n\\\n[`execution_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-execution_duration) int64\\\n\\\nExample`0`\\\n\\\nThe time in milliseconds it took to execute the commands in the JAR or notebook until they completed, failed, timed out, were cancelled, or encountered an unexpected error. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `execution_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.\\\n\\\n[`cleanup_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-cleanup_duration) int64\\\n\\\nExample`0`\\\n\\\nThe time in milliseconds it took to terminate the cluster and clean up any associated artifacts. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `cleanup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.\\\n\\\n[`end_time`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-end_time) int64\\\n\\\nExample`1625060863413`\\\n\\\nThe time at which this run ended in epoch milliseconds (milliseconds since 1/1/1970 UTC). This field is set to 0 if the job is still running.\\\n\\\n[`run_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-run_duration) int64\\\n\\\nExample`110183`\\\n\\\nThe time in milliseconds it took the job run and all of its repairs to finish.\\\n\\\n[`queue_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-queue_duration) int64\\\n\\\nExample`1625060863413`\\\n\\\nThe time in milliseconds that the run has spent in the queue.\\\n\\\n[`trigger`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-trigger) string\\\n\\\nEnum: `PERIODIC | ONE_TIME | RETRY | RUN_JOB_TASK | FILE_ARRIVAL | TABLE`\\\n\\\nThe type of trigger that fired this run.\\\n\\\n- `PERIODIC`: Schedules that periodically trigger runs, such as a cron scheduler.\\\n- `ONE_TIME`: One time triggers that fire a single run. This occurs you triggered a single run on demand through the UI or the API.\\\n- `RETRY`: Indicates a run that is triggered as a retry of a previously failed run. This occurs when you request to re-run the job in case of failures.\\\n- `RUN_JOB_TASK`: Indicates a run that is triggered using a Run Job task.\\\n- `FILE_ARRIVAL`: Indicates a run that is triggered by a file arrival.\\\n- `TABLE`: Indicates a run that is triggered by a table update.\\\n- `CONTINUOUS_RESTART`: Indicates a run created by user to manually restart a continuous job run.\\\n\\\n[`trigger_info`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-trigger_info) object\\\n\\\nAdditional details about what triggered the run\\\n\\\n[`run_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-run_name) string\\\n\\\n<= 4096 characters\\\n\\\nDefault`\"Untitled\"`\\\n\\\nExample`\"A multitask job run\"`\\\n\\\nAn optional name for the run. The maximum length is 4096 bytes in UTF-8 encoding.\\\n\\\n[`run_page_url`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-run_page_url) string\\\n\\\nExample`\"https://my-workspace.cloud.databricks.com/#job/11223344/run/123\"`\\\n\\\nThe URL to the detail page of the run.\\\n\\\n[`run_type`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-run_type) string\\\n\\\nEnum: `JOB_RUN | WORKFLOW_RUN | SUBMIT_RUN`\\\n\\\nThe type of a run.\\\n\\\n- `JOB_RUN`: Normal job run. A run created with [jobs/runnow](https://docs.databricks.com/api/azure/workspace/jobs/runnow).\\\n- `WORKFLOW_RUN`: Workflow run. A run created with [dbutils.notebook.run](https://docs.databricks.com/dev-tools/databricks-utils.html#dbutils-workflow).\\\n- `SUBMIT_RUN`: Submit run. A run created with [jobs/submit](https://docs.databricks.com/api/azure/workspace/jobs/submit).\\\n\\\n[`tasks`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-tasks) Array of object\\\n\\\n<= 100 items\\\n\\\nExample\\\n\\\nThe list of tasks performed by the run. Each task has its own `run_id` which you can use to call `JobsGetOutput` to retrieve the run resutls.\\\nIf more than 100 tasks are available, you can paginate through them using [jobs/getrun](https://docs.databricks.com/api/azure/workspace/jobs/getrun). Use the `next_page_token` field at the object root to determine if more results are available.\\\n\\\n[`description`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-description) string\\\n\\\nDescription of the run\\\n\\\n[`attempt_number`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-attempt_number) int32\\\n\\\nExample`0`\\\n\\\nThe sequence number of this run attempt for a triggered job run. The initial attempt of a run has an attempt\\_number of 0. If the initial run attempt fails, and the job has a retry policy (`max_retries` \\> 0), subsequent runs are created with an `original_attempt_run_id` of the original attempt’s ID and an incrementing `attempt_number`. Runs are retried only until they succeed, and the maximum `attempt_number` is the same as the `max_retries` value for the job.\\\n\\\n[`job_clusters`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-job_clusters) Array of object\\\n\\\n<= 100 items\\\n\\\nExample\\\n\\\nA list of job cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings.\\\nIf more than 100 job clusters are available, you can paginate through them using [jobs/getrun](https://docs.databricks.com/api/azure/workspace/jobs/getrun).\\\n\\\n[`git_source`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-git_source) object\\\n\\\nExample\\\n\\\nAn optional specification for a remote Git repository containing the source code used by tasks. Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.\\\n\\\nIf `git_source` is set, these tasks retrieve the file from the remote repository by default. However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.\\\n\\\nNote: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are used, `git_source` must be defined on the job.\\\n\\\n[`repair_history`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-repair_history) Array of object\\\n\\\nThe repair history of the run.\\\n\\\n[`status`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-status) object\\\n\\\nThe current status of the run\\\n\\\n[`job_run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#runs-job_run_id) int64\\\n\\\nID of the job run that this run belongs to.\\\nFor legacy and single-task job runs the field is populated with the job run ID.\\\nFor task runs, the field is populated with the ID of the job run that the task run belongs to.\\\n\\\n\\]\n\n[`has_more`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#has_more) boolean\n\nIf true, additional runs matching the provided filter are available for listing.\n\n[`next_page_token`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#next_page_token) string\n\nExample`\"CAEomPuciYcxMKbM9JvMlwU=\"`\n\nA token that can be used to list the next page of runs (if applicable).\n\n[`prev_page_token`](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns#prev_page_token) string\n\nExample`\"CAAos-uriYcxMN7_rt_v7B4=\"`\n\nA token that can be used to list the previous page of runs (if applicable).\n\nThis method might return the following HTTP codes: 400, 401, 403, 429, 500\n\nError responses are returned in the following format:\n\n```\n{\n  \"error_code\": \"Error code\",\n  \"message\": \"Human-readable error message.\"\n}\n```\n\n# Possible error codes:\n\nHTTP code\n\nerror\\_code\n\nDescription\n\n400\n\nINVALID\\_PARAMETER\\_VALUE\n\nSupplied value for a parameter was invalid.\n\n401\n\nUNAUTHORIZED\n\nThe request does not have valid authentication credentials for the operation.\n\n403\n\nPERMISSION\\_DENIED\n\nCaller does not have permission to execute the specified operation.\n\n429\n\nREQUEST\\_LIMIT\\_EXCEEDED\n\nRequest is rejected due to throttling.\n\n500\n\nINTERNAL\\_SERVER\\_ERROR\n\nInternal error.\n\n# Response samples\n\n200\n\n```\n{\n  \"runs\": [\\\n    {\\\n      \"job_id\": 11223344,\\\n      \"run_id\": 455644833,\\\n      \"creator_user_name\": \"user.name@databricks.com\",\\\n      \"number_in_job\": 455644833,\\\n      \"original_attempt_run_id\": 455644833,\\\n      \"state\": {\\\n        \"life_cycle_state\": \"PENDING\",\\\n        \"result_state\": \"SUCCESS\",\\\n        \"state_message\": \"string\",\\\n        \"user_cancelled_or_timedout\": false,\\\n        \"queue_reason\": \"Queued due to reaching maximum concurrent runs of 1.\"\\\n      },\\\n      \"schedule\": {\\\n        \"quartz_cron_expression\": \"20 30 * * * ?\",\\\n        \"timezone_id\": \"Europe/London\",\\\n        \"pause_status\": \"UNPAUSED\"\\\n      },\\\n      \"cluster_spec\": {\\\n        \"existing_cluster_id\": \"0923-164208-meows279\",\\\n        \"new_cluster\": {\\\n          \"num_workers\": 0,\\\n          \"autoscale\": {\\\n            \"min_workers\": 0,\\\n            \"max_workers\": 0\\\n          },\\\n          \"kind\": \"CLASSIC_PREVIEW\",\\\n          \"cluster_name\": \"string\",\\\n          \"spark_version\": \"string\",\\\n          \"use_ml_runtime\": true,\\\n          \"is_single_node\": true,\\\n          \"spark_conf\": {\\\n            \"property1\": \"string\",\\\n            \"property2\": \"string\"\\\n          },\\\n          \"azure_attributes\": {\\\n            \"log_analytics_info\": {\\\n              \"log_analytics_workspace_id\": \"string\",\\\n              \"log_analytics_primary_key\": \"string\"\\\n            },\\\n            \"first_on_demand\": \"1\",\\\n            \"availability\": \"SPOT_AZURE\",\\\n            \"spot_bid_max_price\": \"-1.0\"\\\n          },\\\n          \"node_type_id\": \"string\",\\\n          \"driver_node_type_id\": \"string\",\\\n          \"ssh_public_keys\": [\\\n            \"string\"\\\n          ],\\\n          \"custom_tags\": {\\\n            \"property1\": \"string\",\\\n            \"property2\": \"string\"\\\n          },\\\n          \"cluster_log_conf\": {\\\n            \"dbfs\": {\\\n              \"destination\": \"string\"\\\n            }\\\n          },\\\n          \"init_scripts\": [\\\n            {\\\n              \"workspace\": {\\\n                \"destination\": \"string\"\\\n              },\\\n              \"volumes\": {\\\n                \"destination\": \"string\"\\\n              },\\\n              \"file\": {\\\n                \"destination\": \"string\"\\\n              },\\\n              \"dbfs\": {\\\n                \"destination\": \"string\"\\\n              },\\\n              \"abfss\": {\\\n                \"destination\": \"string\"\\\n              },\\\n              \"gcs\": {\\\n                \"destination\": \"string\"\\\n              }\\\n            }\\\n          ],\\\n          \"spark_env_vars\": {\\\n            \"property1\": \"string\",\\\n            \"property2\": \"string\"\\\n          },\\\n          \"autotermination_minutes\": 0,\\\n          \"enable_elastic_disk\": true,\\\n          \"instance_pool_id\": \"string\",\\\n          \"policy_id\": \"string\",\\\n          \"enable_local_disk_encryption\": true,\\\n          \"driver_instance_pool_id\": \"string\",\\\n          \"workload_type\": {\\\n            \"clients\": {\\\n              \"notebooks\": \"true\",\\\n              \"jobs\": \"true\"\\\n            }\\\n          },\\\n          \"runtime_engine\": \"NULL\",\\\n          \"docker_image\": {\\\n            \"url\": \"string\",\\\n            \"basic_auth\": {\\\n              \"username\": \"string\",\\\n              \"password\": \"string\"\\\n            }\\\n          },\\\n          \"data_security_mode\": \"DATA_SECURITY_MODE_AUTO\",\\\n          \"single_user_name\": \"string\",\\\n          \"apply_policy_default_values\": false\\\n        },\\\n        \"job_cluster_key\": \"string\",\\\n        \"libraries\": [\\\n          {\\\n            \"jar\": \"string\",\\\n            \"egg\": \"string\",\\\n            \"pypi\": {\\\n              \"package\": \"string\",\\\n              \"repo\": \"string\"\\\n            },\\\n            \"maven\": {\\\n              \"coordinates\": \"string\",\\\n              \"repo\": \"string\",\\\n              \"exclusions\": [\\\n                \"string\"\\\n              ]\\\n            },\\\n            \"cran\": {\\\n              \"package\": \"string\",\\\n              \"repo\": \"string\"\\\n            },\\\n            \"whl\": \"string\",\\\n            \"requirements\": \"string\"\\\n          }\\\n        ]\\\n      },\\\n      \"cluster_instance\": {\\\n        \"cluster_id\": \"0923-164208-meows279\",\\\n        \"spark_context_id\": \"string\"\\\n      },\\\n      \"job_parameters\": [\\\n        {\\\n          \"default\": \"users\",\\\n          \"name\": \"table\",\\\n          \"value\": \"customers\"\\\n        }\\\n      ],\\\n      \"overriding_parameters\": {\\\n        \"pipeline_params\": {\\\n          \"full_refresh\": false\\\n        },\\\n        \"jar_params\": [\\\n          \"john\",\\\n          \"doe\",\\\n          \"35\"\\\n        ],\\\n        \"notebook_params\": {\\\n          \"age\": \"35\",\\\n          \"name\": \"john doe\"\\\n        },\\\n        \"python_params\": [\\\n          \"john doe\",\\\n          \"35\"\\\n        ],\\\n        \"spark_submit_params\": [\\\n          \"--class\",\\\n          \"org.apache.spark.examples.SparkPi\"\\\n        ],\\\n        \"python_named_params\": {\\\n          \"data\": \"dbfs:/path/to/data.json\",\\\n          \"name\": \"task\"\\\n        },\\\n        \"sql_params\": {\\\n          \"age\": \"35\",\\\n          \"name\": \"john doe\"\\\n        },\\\n        \"dbt_commands\": [\\\n          \"dbt deps\",\\\n          \"dbt seed\",\\\n          \"dbt run\"\\\n        ]\\\n      },\\\n      \"start_time\": 1625060460483,\\\n      \"setup_duration\": 0,\\\n      \"execution_duration\": 0,\\\n      \"cleanup_duration\": 0,\\\n      \"end_time\": 1625060863413,\\\n      \"run_duration\": 110183,\\\n      \"queue_duration\": 1625060863413,\\\n      \"trigger\": \"PERIODIC\",\\\n      \"trigger_info\": {\\\n        \"run_id\": 0\\\n      },\\\n      \"run_name\": \"A multitask job run\",\\\n      \"run_page_url\": \"https://my-workspace.cloud.databricks.com/#job/11223344/run/123\",\\\n      \"run_type\": \"JOB_RUN\",\\\n      \"tasks\": [\\\n        {\\\n          \"setup_duration\": 0,\\\n          \"start_time\": 1629989929660,\\\n          \"task_key\": \"Orders_Ingest\",\\\n          \"state\": {\\\n            \"life_cycle_state\": \"INTERNAL_ERROR\",\\\n            \"result_state\": \"FAILED\",\\\n            \"state_message\": \"Library installation failed for library due to user error. Error messages:\\n'Manage' permissions are required to install libraries on a cluster\",\\\n            \"user_cancelled_or_timedout\": false\\\n          },\\\n          \"description\": \"Ingests order data\",\\\n          \"job_cluster_key\": \"auto_scaling_cluster\",\\\n          \"end_time\": 1629989930171,\\\n          \"run_page_url\": \"https://my-workspace.cloud.databricks.com/#job/39832/run/20\",\\\n          \"run_id\": 2112892,\\\n          \"cluster_instance\": {\\\n            \"cluster_id\": \"0923-164208-meows279\",\\\n            \"spark_context_id\": \"4348585301701786933\"\\\n          },\\\n          \"spark_jar_task\": {\\\n            \"main_class_name\": \"com.databricks.OrdersIngest\"\\\n          },\\\n          \"libraries\": [\\\n            {\\\n              \"jar\": \"dbfs:/mnt/databricks/OrderIngest.jar\"\\\n            }\\\n          ],\\\n          \"attempt_number\": 0,\\\n          \"cleanup_duration\": 0,\\\n          \"execution_duration\": 0,\\\n          \"run_if\": \"ALL_SUCCESS\"\\\n        },\\\n        {\\\n          \"setup_duration\": 0,\\\n          \"start_time\": 0,\\\n          \"task_key\": \"Match\",\\\n          \"state\": {\\\n            \"life_cycle_state\": \"SKIPPED\",\\\n            \"state_message\": \"An upstream task failed.\",\\\n            \"user_cancelled_or_timedout\": false\\\n          },\\\n          \"description\": \"Matches orders with user sessions\",\\\n          \"notebook_task\": {\\\n            \"notebook_path\": \"/Users/user.name@databricks.com/Match\",\\\n            \"source\": \"WORKSPACE\"\\\n          },\\\n          \"end_time\": 1629989930238,\\\n          \"depends_on\": [\\\n            {\\\n              \"task_key\": \"Orders_Ingest\"\\\n            },\\\n            {\\\n              \"task_key\": \"Sessionize\"\\\n            }\\\n          ],\\\n          \"run_page_url\": \"https://my-workspace.cloud.databricks.com/#job/39832/run/21\",\\\n          \"new_cluster\": {\\\n            \"autoscale\": {\\\n              \"max_workers\": 16,\\\n              \"min_workers\": 2\\\n            },\\\n            \"node_type_id\": null,\\\n            \"spark_conf\": {\\\n              \"spark.speculation\": true\\\n            },\\\n            \"spark_version\": \"7.3.x-scala2.12\"\\\n          },\\\n          \"run_id\": 2112897,\\\n          \"cluster_instance\": {\\\n            \"cluster_id\": \"0923-164208-meows279\"\\\n          },\\\n          \"attempt_number\": 0,\\\n          \"cleanup_duration\": 0,\\\n          \"execution_duration\": 0,\\\n          \"run_if\": \"ALL_SUCCESS\"\\\n        },\\\n        {\\\n          \"setup_duration\": 0,\\\n          \"start_time\": 1629989929668,\\\n          \"task_key\": \"Sessionize\",\\\n          \"state\": {\\\n            \"life_cycle_state\": \"INTERNAL_ERROR\",\\\n            \"result_state\": \"FAILED\",\\\n            \"state_message\": \"Library installation failed for library due to user error. Error messages:\\n'Manage' permissions are required to install libraries on a cluster\",\\\n            \"user_cancelled_or_timedout\": false\\\n          },\\\n          \"description\": \"Extracts session data from events\",\\\n          \"end_time\": 1629989930144,\\\n          \"run_page_url\": \"https://my-workspace.cloud.databricks.com/#job/39832/run/22\",\\\n          \"run_id\": 2112902,\\\n          \"cluster_instance\": {\\\n            \"cluster_id\": \"0923-164208-meows279\",\\\n            \"spark_context_id\": \"4348585301701786933\"\\\n          },\\\n          \"spark_jar_task\": {\\\n            \"main_class_name\": \"com.databricks.Sessionize\"\\\n          },\\\n          \"libraries\": [\\\n            {\\\n              \"jar\": \"dbfs:/mnt/databricks/Sessionize.jar\"\\\n            }\\\n          ],\\\n          \"attempt_number\": 0,\\\n          \"existing_cluster_id\": \"0923-164208-meows279\",\\\n          \"cleanup_duration\": 0,\\\n          \"execution_duration\": 0,\\\n          \"run_if\": \"ALL_SUCCESS\"\\\n        }\\\n      ],\\\n      \"description\": \"string\",\\\n      \"attempt_number\": 0,\\\n      \"job_clusters\": [\\\n        {\\\n          \"job_cluster_key\": \"auto_scaling_cluster\",\\\n          \"new_cluster\": {\\\n            \"autoscale\": {\\\n              \"max_workers\": 16,\\\n              \"min_workers\": 2\\\n            },\\\n            \"node_type_id\": null,\\\n            \"spark_conf\": {\\\n              \"spark.speculation\": true\\\n            },\\\n            \"spark_version\": \"7.3.x-scala2.12\"\\\n          }\\\n        }\\\n      ],\\\n      \"git_source\": {\\\n        \"git_branch\": \"main\",\\\n        \"git_provider\": \"gitHub\",\\\n        \"git_url\": \"https://github.com/databricks/databricks-cli\"\\\n      },\\\n      \"repair_history\": [\\\n        {\\\n          \"type\": \"ORIGINAL\",\\\n          \"start_time\": 1625060460483,\\\n          \"end_time\": 1625060863413,\\\n          \"state\": {\\\n            \"life_cycle_state\": \"PENDING\",\\\n            \"result_state\": \"SUCCESS\",\\\n            \"state_message\": \"string\",\\\n            \"user_cancelled_or_timedout\": false,\\\n            \"queue_reason\": \"Queued due to reaching maximum concurrent runs of 1.\"\\\n          },\\\n          \"id\": 734650698524280,\\\n          \"task_run_ids\": [\\\n            1106460542112844,\\\n            988297789683452\\\n          ],\\\n          \"status\": {\\\n            \"state\": \"BLOCKED\",\\\n            \"termination_details\": {\\\n              \"code\": \"SUCCESS\",\\\n              \"type\": \"SUCCESS\",\\\n              \"message\": \"string\"\\\n            },\\\n            \"queue_details\": {\\\n              \"code\": \"ACTIVE_RUNS_LIMIT_REACHED\",\\\n              \"message\": \"string\"\\\n            }\\\n          }\\\n        }\\\n      ],\\\n      \"status\": {\\\n        \"state\": \"BLOCKED\",\\\n        \"termination_details\": {\\\n          \"code\": \"SUCCESS\",\\\n          \"type\": \"SUCCESS\",\\\n          \"message\": \"string\"\\\n        },\\\n        \"queue_details\": {\\\n          \"code\": \"ACTIVE_RUNS_LIMIT_REACHED\",\\\n          \"message\": \"string\"\\\n        }\\\n      },\\\n      \"job_run_id\": 0\\\n    }\\\n  ],\n  \"has_more\": true,\n  \"next_page_token\": \"CAEomPuciYcxMKbM9JvMlwU=\",\n  \"prev_page_token\": \"CAAos-uriYcxMN7_rt_v7B4=\"\n}\n```"
  },
  {
    "markdown": "[Databricks REST API ReferenceAPI](https://databricks.com/)\n\nCTRL + P\n\n[Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n\n[Support](https://help.databricks.com/)\n\n[Feedback](mailto:doc-feedback@databricks.com?subject=Documentation%20Feedback)\n\nAWSGCPAzure\n\nWorkspaceAccount\n\n[Introduction](https://docs.databricks.com/api/azure/workspace/introduction)\n\nDatabricks Workspace\n\n[Git Credentials](https://docs.databricks.com/api/azure/workspace/gitcredentials)\n\n[Repos](https://docs.databricks.com/api/azure/workspace/repos)\n\n[Secret](https://docs.databricks.com/api/azure/workspace/secrets)\n\n[Workspace](https://docs.databricks.com/api/azure/workspace/workspace)\n\nCompute\n\n[Cluster Policies](https://docs.databricks.com/api/azure/workspace/clusterpolicies)\n\n[Clusters](https://docs.databricks.com/api/azure/workspace/clusters)\n\n[Command Execution](https://docs.databricks.com/api/azure/workspace/commandexecution)\n\n[Global Init Scripts](https://docs.databricks.com/api/azure/workspace/globalinitscripts)\n\n[Instance Pools](https://docs.databricks.com/api/azure/workspace/instancepools)\n\n[Managed Libraries](https://docs.databricks.com/api/azure/workspace/libraries)\n\n[Policy compliance for clusters](https://docs.databricks.com/api/azure/workspace/policycomplianceforclusters)\n\n[Policy Families](https://docs.databricks.com/api/azure/workspace/policyfamilies)\n\nWorkflows\n\n[Jobs (2.2)](https://docs.databricks.com/api/azure/workspace/jobs)\n\n[Jobs (2.1)](https://docs.databricks.com/api/azure/workspace/jobs_21)\n\n[Create a new job](https://docs.databricks.com/api/azure/workspace/jobs_21/create)\n\n[Create and trigger a one-time run](https://docs.databricks.com/api/azure/workspace/jobs_21/submit)\n\n[Update all job settings (reset)](https://docs.databricks.com/api/azure/workspace/jobs_21/reset)\n\n[Update job settings partially](https://docs.databricks.com/api/azure/workspace/jobs_21/update)\n\n[Delete a job](https://docs.databricks.com/api/azure/workspace/jobs_21/delete)\n\n[Get a single job](https://docs.databricks.com/api/azure/workspace/jobs_21/get)\n\n[List jobs](https://docs.databricks.com/api/azure/workspace/jobs_21/list)\n\n[Trigger a new job run](https://docs.databricks.com/api/azure/workspace/jobs_21/runnow)\n\n[Repair a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/repairrun)\n\n[List job runs](https://docs.databricks.com/api/azure/workspace/jobs_21/listruns)\n\n[Get a single job run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrun)\n\n[Delete a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/deleterun)\n\n[Cancel a run](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelrun)\n\n[Cancel all runs of a job](https://docs.databricks.com/api/azure/workspace/jobs_21/cancelallruns)\n\n[Get the output for a single run](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput)\n\n[Export and retrieve a job run](https://docs.databricks.com/api/azure/workspace/jobs_21/exportrun)\n\n[Policy compliance for jobs](https://docs.databricks.com/api/azure/workspace/policycomplianceforjobs)\n\nDelta Live Tables\n\n[Pipelines](https://docs.databricks.com/api/azure/workspace/pipelines)\n\nFile Management\n\n[DBFS](https://docs.databricks.com/api/azure/workspace/dbfs)\n\n[Files Public preview](https://docs.databricks.com/api/azure/workspace/files)\n\nMachine Learning\n\n[Experiments](https://docs.databricks.com/api/azure/workspace/experiments)\n\n[Model Registry](https://docs.databricks.com/api/azure/workspace/modelregistry)\n\nReal-time Serving\n\n[Serving endpoints](https://docs.databricks.com/api/azure/workspace/servingendpoints)\n\nApps\n\n[Apps Public preview](https://docs.databricks.com/api/azure/workspace/apps)\n\nVector Search\n\n[Endpoints](https://docs.databricks.com/api/azure/workspace/vectorsearchendpoints)\n\n[Indexes](https://docs.databricks.com/api/azure/workspace/vectorsearchindexes)\n\nIdentity and Access Management\n\n[Account Access Control Proxy Public preview](https://docs.databricks.com/api/azure/workspace/accountaccesscontrolproxy)\n\n[Current User Public preview](https://docs.databricks.com/api/azure/workspace/currentuser)\n\n[Groups Public preview](https://docs.databricks.com/api/azure/workspace/groups)\n\n[Permissions](https://docs.databricks.com/api/azure/workspace/permissions)\n\n[Service Principals Public preview](https://docs.databricks.com/api/azure/workspace/serviceprincipals)\n\n[Users Public preview](https://docs.databricks.com/api/azure/workspace/users)\n\nDatabricks SQL\n\n[Alerts Public preview](https://docs.databricks.com/api/azure/workspace/alerts)\n\n[Alerts (legacy) Public preview](https://docs.databricks.com/api/azure/workspace/alertslegacy)\n\n[Dashboards (legacy)](https://docs.databricks.com/api/azure/workspace/dashboards)\n\n[Data Sources (legacy)](https://docs.databricks.com/api/azure/workspace/datasources)\n\n[ACL / Permissions](https://docs.databricks.com/api/azure/workspace/dbsqlpermissions)\n\n[Queries Public preview](https://docs.databricks.com/api/azure/workspace/queries)\n\n[Queries (legacy)](https://docs.databricks.com/api/azure/workspace/querieslegacy)\n\n[Query History](https://docs.databricks.com/api/azure/workspace/queryhistory)\n\n[Statement Execution](https://docs.databricks.com/api/azure/workspace/statementexecution)\n\n[SQL Warehouses](https://docs.databricks.com/api/azure/workspace/warehouses)\n\nDashboards\n\n[Lakeview](https://docs.databricks.com/api/azure/workspace/lakeview)\n\nUnity Catalog\n\n[Artifact Allowlists Public preview](https://docs.databricks.com/api/azure/workspace/artifactallowlists)\n\n[Catalogs](https://docs.databricks.com/api/azure/workspace/catalogs)\n\n[Connections Public preview](https://docs.databricks.com/api/azure/workspace/connections)\n\n[Credentials](https://docs.databricks.com/api/azure/workspace/credentials)\n\n[External Locations](https://docs.databricks.com/api/azure/workspace/externallocations)\n\n[Functions](https://docs.databricks.com/api/azure/workspace/functions)\n\n[Grants](https://docs.databricks.com/api/azure/workspace/grants)\n\n[Metastores](https://docs.databricks.com/api/azure/workspace/metastores)\n\n[Model Versions](https://docs.databricks.com/api/azure/workspace/modelversions)\n\n[Online Tables Public preview](https://docs.databricks.com/api/azure/workspace/onlinetables)\n\n[Quality Monitors](https://docs.databricks.com/api/azure/workspace/qualitymonitors)\n\n[Registered Models](https://docs.databricks.com/api/azure/workspace/registeredmodels)\n\n[Resource Quotas](https://docs.databricks.com/api/azure/workspace/resourcequotas)\n\n[Schemas](https://docs.databricks.com/api/azure/workspace/schemas)\n\n[Storage Credentials](https://docs.databricks.com/api/azure/workspace/storagecredentials)\n\n[SystemSchemas Public preview](https://docs.databricks.com/api/azure/workspace/systemschemas)\n\n[Table Constraints](https://docs.databricks.com/api/azure/workspace/tableconstraints)\n\n[Tables](https://docs.databricks.com/api/azure/workspace/tables)\n\n[Temporary Table Credentials](https://docs.databricks.com/api/azure/workspace/temporarytablecredentials)\n\n[Volumes](https://docs.databricks.com/api/azure/workspace/volumes)\n\n[Workspace Bindings](https://docs.databricks.com/api/azure/workspace/workspacebindings)\n\nDelta Sharing\n\n[Providers](https://docs.databricks.com/api/azure/workspace/providers)\n\n[Recipient Activation](https://docs.databricks.com/api/azure/workspace/recipientactivation)\n\n[Recipients](https://docs.databricks.com/api/azure/workspace/recipients)\n\n[Shares](https://docs.databricks.com/api/azure/workspace/shares)\n\nSettings\n\nSettings\n\n[IP Access Lists](https://docs.databricks.com/api/azure/workspace/ipaccesslists)\n\n[Notification Destinations](https://docs.databricks.com/api/azure/workspace/notificationdestinations)\n\n[Token management](https://docs.databricks.com/api/azure/workspace/tokenmanagement)\n\n[Token](https://docs.databricks.com/api/azure/workspace/tokens)\n\n[Workspace Conf](https://docs.databricks.com/api/azure/workspace/workspaceconf)\n\nMarketplace\n\n[Consumer Fulfillments Public preview](https://docs.databricks.com/api/azure/workspace/consumerfulfillments)\n\n[Consumer Installations Public preview](https://docs.databricks.com/api/azure/workspace/consumerinstallations)\n\n[Consumer Listings Public preview](https://docs.databricks.com/api/azure/workspace/consumerlistings)\n\n[Consumer Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/consumerpersonalizationrequests)\n\n[Consumer Providers Public preview](https://docs.databricks.com/api/azure/workspace/consumerproviders)\n\n[Provider Exchange Filters Public preview](https://docs.databricks.com/api/azure/workspace/providerexchangefilters)\n\n[Provider Exchanges Public preview](https://docs.databricks.com/api/azure/workspace/providerexchanges)\n\n[Provider Files Public preview](https://docs.databricks.com/api/azure/workspace/providerfiles)\n\n[Provider Listings Public preview](https://docs.databricks.com/api/azure/workspace/providerlistings)\n\n[Provider Personalization Requests Public preview](https://docs.databricks.com/api/azure/workspace/providerpersonalizationrequests)\n\n[Provider Providers Analytics Dashboards Public preview](https://docs.databricks.com/api/azure/workspace/providerprovideranalyticsdashboards)\n\n[Provider Providers Public preview](https://docs.databricks.com/api/azure/workspace/providerproviders)\n\nClean Rooms\n\n[Assets](https://docs.databricks.com/api/azure/workspace/cleanroomassets)\n\n[Task Runs](https://docs.databricks.com/api/azure/workspace/cleanroomtaskruns)\n\n[Clean Rooms](https://docs.databricks.com/api/azure/workspace/cleanrooms)\n\n## Get the output for a single run\n\n`\nGET/api/2.1/jobs/runs/get-output`\n\nRetrieve the output and metadata of a single task run. When a notebook task returns\na value through the `dbutils.notebook.exit()` call, you can use this endpoint to retrieve\nthat value. Azure Databricks restricts this API to returning the first 5 MB of the output.\nTo return a larger result, you can store job results in a cloud storage service.\n\nThis endpoint validates that the run\\_id parameter is valid and returns an HTTP status\ncode 400 if the run\\_id parameter is invalid. Runs are automatically removed after\n60 days. If you to want to reference them beyond 60 days, you must save old run results\nbefore they expire.\n\n### Query parameters\n\n[`run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#run_id) requiredint64\n\nExample`run_id=455644833`\n\nThe canonical identifier for the run.\n\n### Responses\n\n**200** Request completed successfully.\n\nRequest completed successfully.\n\n[`metadata`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata) object\n\nAll details of the run except for its output.\n\n[`job_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-job_id) int64\n\nExample`11223344`\n\nThe canonical identifier of the job that contains this run.\n\n[`run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-run_id) int64\n\nExample`455644833`\n\nThe canonical identifier of the run. This ID is unique across all runs of all jobs.\n\n[`creator_user_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-creator_user_name) string\n\nExample`\"user.name@databricks.com\"`\n\nThe creator user name. This field won’t be included in the response if the user has already been deleted.\n\n[`number_in_job`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-number_in_job) int64\n\nDeprecated\n\nExample`455644833`\n\nA unique identifier for this job run. This is set to the same value as `run_id`.\n\n[`original_attempt_run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-original_attempt_run_id) int64\n\nExample`455644833`\n\nIf this run is a retry of a prior run attempt, this field contains the run\\_id of the original attempt; otherwise, it is the same as the run\\_id.\n\n[`state`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-state) object\n\nDeprecated\n\nDeprecated. Please use the `status` field instead.\n\n[`schedule`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-schedule) object\n\nThe cron schedule that triggered this run if it was triggered by the periodic scheduler.\n\n[`cluster_spec`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-cluster_spec) object\n\nA snapshot of the job’s cluster specification when this run was created.\n\n[`cluster_instance`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-cluster_instance) object\n\nThe cluster used for this run. If the run is specified to use a new cluster, this field is set once the Jobs service has requested a cluster for the run.\n\n[`job_parameters`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-job_parameters) Array of object\n\nJob-level parameters used in the run\n\n[`overriding_parameters`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-overriding_parameters) object\n\nThe parameters used for this run.\n\n[`start_time`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-start_time) int64\n\nExample`1625060460483`\n\nThe time at which this run was started in epoch milliseconds (milliseconds since 1/1/1970 UTC). This may not be the time when the job task starts executing, for example, if the job is scheduled to run on a new cluster, this is the time the cluster creation call is issued.\n\n[`setup_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-setup_duration) int64\n\nExample`0`\n\nThe time in milliseconds it took to set up the cluster. For runs that run on new clusters this is the cluster creation time, for runs that run on existing clusters this time should be very short. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `setup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.\n\n[`execution_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-execution_duration) int64\n\nExample`0`\n\nThe time in milliseconds it took to execute the commands in the JAR or notebook until they completed, failed, timed out, were cancelled, or encountered an unexpected error. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `execution_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.\n\n[`cleanup_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-cleanup_duration) int64\n\nExample`0`\n\nThe time in milliseconds it took to terminate the cluster and clean up any associated artifacts. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `cleanup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.\n\n[`end_time`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-end_time) int64\n\nExample`1625060863413`\n\nThe time at which this run ended in epoch milliseconds (milliseconds since 1/1/1970 UTC). This field is set to 0 if the job is still running.\n\n[`run_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-run_duration) int64\n\nExample`110183`\n\nThe time in milliseconds it took the job run and all of its repairs to finish.\n\n[`queue_duration`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-queue_duration) int64\n\nExample`1625060863413`\n\nThe time in milliseconds that the run has spent in the queue.\n\n[`trigger`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-trigger) string\n\nEnum: `PERIODIC | ONE_TIME | RETRY | RUN_JOB_TASK | FILE_ARRIVAL | TABLE`\n\nThe type of trigger that fired this run.\n\n- `PERIODIC`: Schedules that periodically trigger runs, such as a cron scheduler.\n- `ONE_TIME`: One time triggers that fire a single run. This occurs you triggered a single run on demand through the UI or the API.\n- `RETRY`: Indicates a run that is triggered as a retry of a previously failed run. This occurs when you request to re-run the job in case of failures.\n- `RUN_JOB_TASK`: Indicates a run that is triggered using a Run Job task.\n- `FILE_ARRIVAL`: Indicates a run that is triggered by a file arrival.\n- `TABLE`: Indicates a run that is triggered by a table update.\n- `CONTINUOUS_RESTART`: Indicates a run created by user to manually restart a continuous job run.\n\n[`trigger_info`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-trigger_info) object\n\nAdditional details about what triggered the run\n\n[`run_name`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-run_name) string\n\n<= 4096 characters\n\nDefault`\"Untitled\"`\n\nExample`\"A multitask job run\"`\n\nAn optional name for the run. The maximum length is 4096 bytes in UTF-8 encoding.\n\n[`run_page_url`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-run_page_url) string\n\nExample`\"https://my-workspace.cloud.databricks.com/#job/11223344/run/123\"`\n\nThe URL to the detail page of the run.\n\n[`run_type`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-run_type) string\n\nEnum: `JOB_RUN | WORKFLOW_RUN | SUBMIT_RUN`\n\nThe type of a run.\n\n- `JOB_RUN`: Normal job run. A run created with [jobs/runnow](https://docs.databricks.com/api/azure/workspace/jobs/runnow).\n- `WORKFLOW_RUN`: Workflow run. A run created with [dbutils.notebook.run](https://docs.databricks.com/dev-tools/databricks-utils.html#dbutils-workflow).\n- `SUBMIT_RUN`: Submit run. A run created with [jobs/submit](https://docs.databricks.com/api/azure/workspace/jobs/submit).\n\n[`tasks`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-tasks) Array of object\n\n<= 100 items\n\nExample\n\nThe list of tasks performed by the run. Each task has its own `run_id` which you can use to call `JobsGetOutput` to retrieve the run resutls.\nIf more than 100 tasks are available, you can paginate through them using [jobs/getrun](https://docs.databricks.com/api/azure/workspace/jobs/getrun). Use the `next_page_token` field at the object root to determine if more results are available.\n\n[`description`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-description) string\n\nDescription of the run\n\n[`attempt_number`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-attempt_number) int32\n\nExample`0`\n\nThe sequence number of this run attempt for a triggered job run. The initial attempt of a run has an attempt\\_number of 0. If the initial run attempt fails, and the job has a retry policy (`max_retries` \\> 0), subsequent runs are created with an `original_attempt_run_id` of the original attempt’s ID and an incrementing `attempt_number`. Runs are retried only until they succeed, and the maximum `attempt_number` is the same as the `max_retries` value for the job.\n\n[`job_clusters`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-job_clusters) Array of object\n\n<= 100 items\n\nExample\n\nA list of job cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings.\nIf more than 100 job clusters are available, you can paginate through them using [jobs/getrun](https://docs.databricks.com/api/azure/workspace/jobs/getrun).\n\n[`git_source`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-git_source) object\n\nExample\n\nAn optional specification for a remote Git repository containing the source code used by tasks. Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.\n\nIf `git_source` is set, these tasks retrieve the file from the remote repository by default. However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.\n\nNote: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are used, `git_source` must be defined on the job.\n\n[`repair_history`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-repair_history) Array of object\n\nThe repair history of the run.\n\n[`status`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-status) object\n\nThe current status of the run\n\n[`job_run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#metadata-job_run_id) int64\n\nID of the job run that this run belongs to.\nFor legacy and single-task job runs the field is populated with the job run ID.\nFor task runs, the field is populated with the ID of the job run that the task run belongs to.\n\n[`error`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#error) string\n\nExample`\"ZeroDivisionError: integer division or modulo by zero\"`\n\nAn error message indicating why a task failed or why output is not available. The message is unstructured, and its exact format is subject to change.\n\n[`logs`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#logs) string\n\nExample`\"Hello World!\"`\n\nThe output from tasks that write to standard streams (stdout/stderr) such as\nspark\\_jar\\_task, spark\\_python\\_task, python\\_wheel\\_task.\n\nIt's not supported for the notebook\\_task, pipeline\\_task or spark\\_submit\\_task.\n\nAzure Databricks restricts this API to return the last 5 MB of these logs.\n\n[`logs_truncated`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#logs_truncated) boolean\n\nExample`true`\n\nWhether the logs are truncated.\n\n[`error_trace`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#error_trace) string\n\nIf there was an error executing the run, this field contains any available stack traces.\n\n[`info`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#info) string\n\n[`notebook_output`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#notebook_output) object\n\nThe output of a notebook task, if available. A notebook task that terminates (either successfully or with a failure)\nwithout calling `dbutils.notebook.exit()` is considered to have an empty output.\nThis field is set but its result value is empty. Azure Databricks restricts this API to return the first 5 MB of the output.\nTo return a larger result, use the [ClusterLogConf](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/clusters#clusterlogconf) field to configure log storage\nfor the job cluster.\n\n[`result`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#notebook_output-result) string\n\nExample`\"An arbitrary string passed by calling dbutils.notebook.exit(...)\"`\n\nThe value passed to [dbutils.notebook.exit()](https://docs.databricks.com/notebooks/notebook-workflows.html#notebook-workflows-exit).\nAzure Databricks restricts this API to return the first 5 MB of the value. For a larger result, your job can store the results in a cloud storage service.\nThis field is absent if `dbutils.notebook.exit()` was never called.\n\n[`truncated`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#notebook_output-truncated) boolean\n\nExample`false`\n\nWhether or not the result was truncated.\n\n[`sql_output`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#sql_output) object\n\nThe output of a SQL task, if available.\n\n[`query_output`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#sql_output-query_output) object\n\nThe output of a SQL query task, if available.\n\n[`dashboard_output`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#sql_output-dashboard_output) object\n\nThe output of a SQL dashboard task, if available.\n\n[`alert_output`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#sql_output-alert_output) object\n\nThe output of a SQL alert task, if available.\n\n[`dbt_output`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#dbt_output) object\n\nThe output of a dbt task, if available.\n\n[`artifacts_link`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#dbt_output-artifacts_link) string\n\nA pre-signed URL to download the (compressed) dbt artifacts. This link is valid for a limited time (30 minutes). This information is only available after the run has finished.\n\n[`artifacts_headers`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#dbt_output-artifacts_headers) object\n\nAn optional map of headers to send when retrieving the artifact from the `artifacts_link`.\n\n[`run_job_output`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#run_job_output) object\n\nThe output of a run job task, if available\n\n[`run_id`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#run_job_output-run_id) int64\n\nThe run id of the triggered job run\n\n[`clean_rooms_notebook_output`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#clean_rooms_notebook_output) object\n\nPublic preview\n\nThe output of a clean rooms notebook task, if available\n\n[`clean_room_job_run_state`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#clean_rooms_notebook_output-clean_room_job_run_state) object\n\nThe run state of the clean rooms notebook task.\n\n[`notebook_output`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#clean_rooms_notebook_output-notebook_output) object\n\nThe notebook output for the clean room run\n\n[`output_schema_info`](https://docs.databricks.com/api/azure/workspace/jobs_21/getrunoutput#clean_rooms_notebook_output-output_schema_info) object\n\nInformation on how to access the output schema for the clean room run\n\nThis method might return the following HTTP codes: 400, 401, 403, 429, 500\n\nError responses are returned in the following format:\n\n```\n{\n  \"error_code\": \"Error code\",\n  \"message\": \"Human-readable error message.\"\n}\n```\n\n# Possible error codes:\n\nHTTP code\n\nerror\\_code\n\nDescription\n\n400\n\nINVALID\\_PARAMETER\\_VALUE\n\nSupplied value for a parameter was invalid.\n\n401\n\nUNAUTHORIZED\n\nThe request does not have valid authentication credentials for the operation.\n\n403\n\nPERMISSION\\_DENIED\n\nCaller does not have permission to execute the specified operation.\n\n429\n\nREQUEST\\_LIMIT\\_EXCEEDED\n\nRequest is rejected due to throttling.\n\n500\n\nINTERNAL\\_SERVER\\_ERROR\n\nInternal error.\n\n# Response samples\n\n200\n\n```\n{\n  \"metadata\": {\n    \"job_id\": 11223344,\n    \"run_id\": 455644833,\n    \"creator_user_name\": \"user.name@databricks.com\",\n    \"number_in_job\": 455644833,\n    \"original_attempt_run_id\": 455644833,\n    \"state\": {\n      \"life_cycle_state\": \"PENDING\",\n      \"result_state\": \"SUCCESS\",\n      \"state_message\": \"string\",\n      \"user_cancelled_or_timedout\": false,\n      \"queue_reason\": \"Queued due to reaching maximum concurrent runs of 1.\"\n    },\n    \"schedule\": {\n      \"quartz_cron_expression\": \"20 30 * * * ?\",\n      \"timezone_id\": \"Europe/London\",\n      \"pause_status\": \"UNPAUSED\"\n    },\n    \"cluster_spec\": {\n      \"existing_cluster_id\": \"0923-164208-meows279\",\n      \"new_cluster\": {\n        \"num_workers\": 0,\n        \"autoscale\": {\n          \"min_workers\": 0,\n          \"max_workers\": 0\n        },\n        \"kind\": \"CLASSIC_PREVIEW\",\n        \"cluster_name\": \"string\",\n        \"spark_version\": \"string\",\n        \"use_ml_runtime\": true,\n        \"is_single_node\": true,\n        \"spark_conf\": {\n          \"property1\": \"string\",\n          \"property2\": \"string\"\n        },\n        \"azure_attributes\": {\n          \"log_analytics_info\": {\n            \"log_analytics_workspace_id\": \"string\",\n            \"log_analytics_primary_key\": \"string\"\n          },\n          \"first_on_demand\": \"1\",\n          \"availability\": \"SPOT_AZURE\",\n          \"spot_bid_max_price\": \"-1.0\"\n        },\n        \"node_type_id\": \"string\",\n        \"driver_node_type_id\": \"string\",\n        \"ssh_public_keys\": [\\\n          \"string\"\\\n        ],\n        \"custom_tags\": {\n          \"property1\": \"string\",\n          \"property2\": \"string\"\n        },\n        \"cluster_log_conf\": {\n          \"dbfs\": {\n            \"destination\": \"string\"\n          }\n        },\n        \"init_scripts\": [\\\n          {\\\n            \"workspace\": {\\\n              \"destination\": \"string\"\\\n            },\\\n            \"volumes\": {\\\n              \"destination\": \"string\"\\\n            },\\\n            \"file\": {\\\n              \"destination\": \"string\"\\\n            },\\\n            \"dbfs\": {\\\n              \"destination\": \"string\"\\\n            },\\\n            \"abfss\": {\\\n              \"destination\": \"string\"\\\n            },\\\n            \"gcs\": {\\\n              \"destination\": \"string\"\\\n            }\\\n          }\\\n        ],\n        \"spark_env_vars\": {\n          \"property1\": \"string\",\n          \"property2\": \"string\"\n        },\n        \"autotermination_minutes\": 0,\n        \"enable_elastic_disk\": true,\n        \"instance_pool_id\": \"string\",\n        \"policy_id\": \"string\",\n        \"enable_local_disk_encryption\": true,\n        \"driver_instance_pool_id\": \"string\",\n        \"workload_type\": {\n          \"clients\": {\n            \"notebooks\": \"true\",\n            \"jobs\": \"true\"\n          }\n        },\n        \"runtime_engine\": \"NULL\",\n        \"docker_image\": {\n          \"url\": \"string\",\n          \"basic_auth\": {\n            \"username\": \"string\",\n            \"password\": \"string\"\n          }\n        },\n        \"data_security_mode\": \"DATA_SECURITY_MODE_AUTO\",\n        \"single_user_name\": \"string\",\n        \"apply_policy_default_values\": false\n      },\n      \"job_cluster_key\": \"string\",\n      \"libraries\": [\\\n        {\\\n          \"jar\": \"string\",\\\n          \"egg\": \"string\",\\\n          \"pypi\": {\\\n            \"package\": \"string\",\\\n            \"repo\": \"string\"\\\n          },\\\n          \"maven\": {\\\n            \"coordinates\": \"string\",\\\n            \"repo\": \"string\",\\\n            \"exclusions\": [\\\n              \"string\"\\\n            ]\\\n          },\\\n          \"cran\": {\\\n            \"package\": \"string\",\\\n            \"repo\": \"string\"\\\n          },\\\n          \"whl\": \"string\",\\\n          \"requirements\": \"string\"\\\n        }\\\n      ]\n    },\n    \"cluster_instance\": {\n      \"cluster_id\": \"0923-164208-meows279\",\n      \"spark_context_id\": \"string\"\n    },\n    \"job_parameters\": [\\\n      {\\\n        \"default\": \"users\",\\\n        \"name\": \"table\",\\\n        \"value\": \"customers\"\\\n      }\\\n    ],\n    \"overriding_parameters\": {\n      \"pipeline_params\": {\n        \"full_refresh\": false\n      },\n      \"jar_params\": [\\\n        \"john\",\\\n        \"doe\",\\\n        \"35\"\\\n      ],\n      \"notebook_params\": {\n        \"age\": \"35\",\n        \"name\": \"john doe\"\n      },\n      \"python_params\": [\\\n        \"john doe\",\\\n        \"35\"\\\n      ],\n      \"spark_submit_params\": [\\\n        \"--class\",\\\n        \"org.apache.spark.examples.SparkPi\"\\\n      ],\n      \"python_named_params\": {\n        \"data\": \"dbfs:/path/to/data.json\",\n        \"name\": \"task\"\n      },\n      \"sql_params\": {\n        \"age\": \"35\",\n        \"name\": \"john doe\"\n      },\n      \"dbt_commands\": [\\\n        \"dbt deps\",\\\n        \"dbt seed\",\\\n        \"dbt run\"\\\n      ]\n    },\n    \"start_time\": 1625060460483,\n    \"setup_duration\": 0,\n    \"execution_duration\": 0,\n    \"cleanup_duration\": 0,\n    \"end_time\": 1625060863413,\n    \"run_duration\": 110183,\n    \"queue_duration\": 1625060863413,\n    \"trigger\": \"PERIODIC\",\n    \"trigger_info\": {\n      \"run_id\": 0\n    },\n    \"run_name\": \"A multitask job run\",\n    \"run_page_url\": \"https://my-workspace.cloud.databricks.com/#job/11223344/run/123\",\n    \"run_type\": \"JOB_RUN\",\n    \"tasks\": [\\\n      {\\\n        \"setup_duration\": 0,\\\n        \"start_time\": 1629989929660,\\\n        \"task_key\": \"Orders_Ingest\",\\\n        \"state\": {\\\n          \"life_cycle_state\": \"INTERNAL_ERROR\",\\\n          \"result_state\": \"FAILED\",\\\n          \"state_message\": \"Library installation failed for library due to user error. Error messages:\\n'Manage' permissions are required to install libraries on a cluster\",\\\n          \"user_cancelled_or_timedout\": false\\\n        },\\\n        \"description\": \"Ingests order data\",\\\n        \"job_cluster_key\": \"auto_scaling_cluster\",\\\n        \"end_time\": 1629989930171,\\\n        \"run_page_url\": \"https://my-workspace.cloud.databricks.com/#job/39832/run/20\",\\\n        \"run_id\": 2112892,\\\n        \"cluster_instance\": {\\\n          \"cluster_id\": \"0923-164208-meows279\",\\\n          \"spark_context_id\": \"4348585301701786933\"\\\n        },\\\n        \"spark_jar_task\": {\\\n          \"main_class_name\": \"com.databricks.OrdersIngest\"\\\n        },\\\n        \"libraries\": [\\\n          {\\\n            \"jar\": \"dbfs:/mnt/databricks/OrderIngest.jar\"\\\n          }\\\n        ],\\\n        \"attempt_number\": 0,\\\n        \"cleanup_duration\": 0,\\\n        \"execution_duration\": 0,\\\n        \"run_if\": \"ALL_SUCCESS\"\\\n      },\\\n      {\\\n        \"setup_duration\": 0,\\\n        \"start_time\": 0,\\\n        \"task_key\": \"Match\",\\\n        \"state\": {\\\n          \"life_cycle_state\": \"SKIPPED\",\\\n          \"state_message\": \"An upstream task failed.\",\\\n          \"user_cancelled_or_timedout\": false\\\n        },\\\n        \"description\": \"Matches orders with user sessions\",\\\n        \"notebook_task\": {\\\n          \"notebook_path\": \"/Users/user.name@databricks.com/Match\",\\\n          \"source\": \"WORKSPACE\"\\\n        },\\\n        \"end_time\": 1629989930238,\\\n        \"depends_on\": [\\\n          {\\\n            \"task_key\": \"Orders_Ingest\"\\\n          },\\\n          {\\\n            \"task_key\": \"Sessionize\"\\\n          }\\\n        ],\\\n        \"run_page_url\": \"https://my-workspace.cloud.databricks.com/#job/39832/run/21\",\\\n        \"new_cluster\": {\\\n          \"autoscale\": {\\\n            \"max_workers\": 16,\\\n            \"min_workers\": 2\\\n          },\\\n          \"node_type_id\": null,\\\n          \"spark_conf\": {\\\n            \"spark.speculation\": true\\\n          },\\\n          \"spark_version\": \"7.3.x-scala2.12\"\\\n        },\\\n        \"run_id\": 2112897,\\\n        \"cluster_instance\": {\\\n          \"cluster_id\": \"0923-164208-meows279\"\\\n        },\\\n        \"attempt_number\": 0,\\\n        \"cleanup_duration\": 0,\\\n        \"execution_duration\": 0,\\\n        \"run_if\": \"ALL_SUCCESS\"\\\n      },\\\n      {\\\n        \"setup_duration\": 0,\\\n        \"start_time\": 1629989929668,\\\n        \"task_key\": \"Sessionize\",\\\n        \"state\": {\\\n          \"life_cycle_state\": \"INTERNAL_ERROR\",\\\n          \"result_state\": \"FAILED\",\\\n          \"state_message\": \"Library installation failed for library due to user error. Error messages:\\n'Manage' permissions are required to install libraries on a cluster\",\\\n          \"user_cancelled_or_timedout\": false\\\n        },\\\n        \"description\": \"Extracts session data from events\",\\\n        \"end_time\": 1629989930144,\\\n        \"run_page_url\": \"https://my-workspace.cloud.databricks.com/#job/39832/run/22\",\\\n        \"run_id\": 2112902,\\\n        \"cluster_instance\": {\\\n          \"cluster_id\": \"0923-164208-meows279\",\\\n          \"spark_context_id\": \"4348585301701786933\"\\\n        },\\\n        \"spark_jar_task\": {\\\n          \"main_class_name\": \"com.databricks.Sessionize\"\\\n        },\\\n        \"libraries\": [\\\n          {\\\n            \"jar\": \"dbfs:/mnt/databricks/Sessionize.jar\"\\\n          }\\\n        ],\\\n        \"attempt_number\": 0,\\\n        \"existing_cluster_id\": \"0923-164208-meows279\",\\\n        \"cleanup_duration\": 0,\\\n        \"execution_duration\": 0,\\\n        \"run_if\": \"ALL_SUCCESS\"\\\n      }\\\n    ],\n    \"description\": \"string\",\n    \"attempt_number\": 0,\n    \"job_clusters\": [\\\n      {\\\n        \"job_cluster_key\": \"auto_scaling_cluster\",\\\n        \"new_cluster\": {\\\n          \"autoscale\": {\\\n            \"max_workers\": 16,\\\n            \"min_workers\": 2\\\n          },\\\n          \"node_type_id\": null,\\\n          \"spark_conf\": {\\\n            \"spark.speculation\": true\\\n          },\\\n          \"spark_version\": \"7.3.x-scala2.12\"\\\n        }\\\n      }\\\n    ],\n    \"git_source\": {\n      \"git_branch\": \"main\",\n      \"git_provider\": \"gitHub\",\n      \"git_url\": \"https://github.com/databricks/databricks-cli\"\n    },\n    \"repair_history\": [\\\n      {\\\n        \"type\": \"ORIGINAL\",\\\n        \"start_time\": 1625060460483,\\\n        \"end_time\": 1625060863413,\\\n        \"state\": {\\\n          \"life_cycle_state\": \"PENDING\",\\\n          \"result_state\": \"SUCCESS\",\\\n          \"state_message\": \"string\",\\\n          \"user_cancelled_or_timedout\": false,\\\n          \"queue_reason\": \"Queued due to reaching maximum concurrent runs of 1.\"\\\n        },\\\n        \"id\": 734650698524280,\\\n        \"task_run_ids\": [\\\n          1106460542112844,\\\n          988297789683452\\\n        ],\\\n        \"status\": {\\\n          \"state\": \"BLOCKED\",\\\n          \"termination_details\": {\\\n            \"code\": \"SUCCESS\",\\\n            \"type\": \"SUCCESS\",\\\n            \"message\": \"string\"\\\n          },\\\n          \"queue_details\": {\\\n            \"code\": \"ACTIVE_RUNS_LIMIT_REACHED\",\\\n            \"message\": \"string\"\\\n          }\\\n        }\\\n      }\\\n    ],\n    \"status\": {\n      \"state\": \"BLOCKED\",\n      \"termination_details\": {\n        \"code\": \"SUCCESS\",\n        \"type\": \"SUCCESS\",\n        \"message\": \"string\"\n      },\n      \"queue_details\": {\n        \"code\": \"ACTIVE_RUNS_LIMIT_REACHED\",\n        \"message\": \"string\"\n      }\n    },\n    \"job_run_id\": 0\n  },\n  \"error\": \"ZeroDivisionError: integer division or modulo by zero\",\n  \"logs\": \"Hello World!\",\n  \"logs_truncated\": true,\n  \"error_trace\": \"string\",\n  \"info\": \"string\",\n  \"notebook_output\": {\n    \"result\": \"An arbitrary string passed by calling dbutils.notebook.exit(...)\",\n    \"truncated\": false\n  },\n  \"sql_output\": {\n    \"query_output\": {\n      \"query_text\": \"string\",\n      \"endpoint_id\": \"string\",\n      \"sql_statements\": [\\\n        {\\\n          \"lookup_key\": \"string\"\\\n        }\\\n      ],\n      \"output_link\": \"string\",\n      \"warehouse_id\": \"string\"\n    },\n    \"dashboard_output\": {\n      \"widgets\": [\\\n        {\\\n          \"widget_id\": \"string\",\\\n          \"widget_title\": \"string\",\\\n          \"output_link\": \"string\",\\\n          \"status\": \"PENDING\",\\\n          \"error\": {\\\n            \"message\": \"string\"\\\n          },\\\n          \"start_time\": 0,\\\n          \"end_time\": 0\\\n        }\\\n      ],\n      \"warehouse_id\": \"string\"\n    },\n    \"alert_output\": {\n      \"query_text\": \"string\",\n      \"sql_statements\": [\\\n        {\\\n          \"lookup_key\": \"string\"\\\n        }\\\n      ],\n      \"output_link\": \"string\",\n      \"warehouse_id\": \"string\",\n      \"alert_state\": \"UNKNOWN\"\n    }\n  },\n  \"dbt_output\": {\n    \"artifacts_link\": \"string\",\n    \"artifacts_headers\": {\n      \"property1\": \"string\",\n      \"property2\": \"string\"\n    }\n  },\n  \"run_job_output\": {\n    \"run_id\": 0\n  },\n  \"clean_rooms_notebook_output\": {\n    \"clean_room_job_run_state\": {\n      \"life_cycle_state\": \"RUN_LIFE_CYCLE_STATE_UNSPECIFIED\",\n      \"result_state\": \"RUN_RESULT_STATE_UNSPECIFIED\"\n    },\n    \"notebook_output\": {\n      \"result\": \"An arbitrary string passed by calling dbutils.notebook.exit(...)\",\n      \"truncated\": false\n    },\n    \"output_schema_info\": {\n      \"catalog_name\": \"string\",\n      \"schema_name\": \"string\",\n      \"expiration_time\": 0\n    }\n  }\n}\n```"
  }
]